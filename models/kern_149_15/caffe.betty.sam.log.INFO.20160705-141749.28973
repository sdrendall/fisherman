Log file created at: 2016/07/05 14:17:49
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0705 14:17:49.219887 28973 caffe.cpp:185] Using GPUs 0
I0705 14:17:49.295389 28973 caffe.cpp:190] GPU 0: GeForce GTX 760
I0705 14:17:49.476105 28973 solver.cpp:48] Initializing solver from parameters: 
test_iter: 50
test_interval: 100
base_lr: 1e-07
display: 20
max_iter: 2000
lr_policy: "fixed"
momentum: 0.75
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "fish_net_deconv_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_deconv_trainer.prototxt"
average_loss: 20
I0705 14:17:49.476373 28973 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_deconv_trainer.prototxt
I0705 14:17:49.476799 28973 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0705 14:17:49.476822 28973 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0705 14:17:49.476948 28973 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "ChunkingFishFovDataLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'seed\': 1337, \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_same_res/\', \'split\': \'train\', \'n_samples\': 20, \'chunker_params\': {\'chunk_size\': 254, \'window_size\': 1}}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip_conv1"
  type: "Convolution"
  bottom: "pool2"
  top: "ip_conv_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip_conv2"
  type: "Convolution"
  bottom: "ip_conv_1"
  top: "ip_conv2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "ip_conv2"
  top: "upsample"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: false
    kernel_size: 149
    stride: 15
  }
}
layer {
  name: "loss"
  type: "InfogainSoftmaxLoss"
  bottom: "upsample"
  bottom: "label"
  top: "loss"
  loss_param {
    normalize: true
  }
  infogain_loss_param {
    source: "/home/sam/code/fisherman/infogainH.binaryproto"
  }
  softmax_param {
    axis: 1
  }
}
I0705 14:17:49.477557 28973 layer_factory.hpp:77] Creating layer training_cells
I0705 14:17:50.262676 28973 net.cpp:91] Creating Layer training_cells
I0705 14:17:50.262707 28973 net.cpp:399] training_cells -> image
I0705 14:17:50.262725 28973 net.cpp:399] training_cells -> label
I0705 14:17:50.351843 28973 net.cpp:141] Setting up training_cells
I0705 14:17:50.351881 28973 net.cpp:148] Top shape: 20 2 254 254 (2580640)
I0705 14:17:50.351938 28973 net.cpp:148] Top shape: 20 1 254 254 (1290320)
I0705 14:17:50.351944 28973 net.cpp:156] Memory required for data: 15483840
I0705 14:17:50.351956 28973 layer_factory.hpp:77] Creating layer conv1
I0705 14:17:50.351979 28973 net.cpp:91] Creating Layer conv1
I0705 14:17:50.351986 28973 net.cpp:425] conv1 <- image
I0705 14:17:50.352000 28973 net.cpp:399] conv1 -> conv1
I0705 14:17:50.356551 28973 net.cpp:141] Setting up conv1
I0705 14:17:50.356570 28973 net.cpp:148] Top shape: 20 15 240 240 (17280000)
I0705 14:17:50.356575 28973 net.cpp:156] Memory required for data: 84603840
I0705 14:17:50.356590 28973 layer_factory.hpp:77] Creating layer pool1
I0705 14:17:50.356601 28973 net.cpp:91] Creating Layer pool1
I0705 14:17:50.356606 28973 net.cpp:425] pool1 <- conv1
I0705 14:17:50.356614 28973 net.cpp:399] pool1 -> pool1
I0705 14:17:50.356654 28973 net.cpp:141] Setting up pool1
I0705 14:17:50.356662 28973 net.cpp:148] Top shape: 20 15 48 48 (691200)
I0705 14:17:50.356667 28973 net.cpp:156] Memory required for data: 87368640
I0705 14:17:50.356672 28973 layer_factory.hpp:77] Creating layer conv2
I0705 14:17:50.356681 28973 net.cpp:91] Creating Layer conv2
I0705 14:17:50.356686 28973 net.cpp:425] conv2 <- pool1
I0705 14:17:50.356704 28973 net.cpp:399] conv2 -> conv2
I0705 14:17:50.361047 28973 net.cpp:141] Setting up conv2
I0705 14:17:50.361073 28973 net.cpp:148] Top shape: 20 5 42 42 (176400)
I0705 14:17:50.361078 28973 net.cpp:156] Memory required for data: 88074240
I0705 14:17:50.361088 28973 layer_factory.hpp:77] Creating layer pool2
I0705 14:17:50.361098 28973 net.cpp:91] Creating Layer pool2
I0705 14:17:50.361102 28973 net.cpp:425] pool2 <- conv2
I0705 14:17:50.361109 28973 net.cpp:399] pool2 -> pool2
I0705 14:17:50.361135 28973 net.cpp:141] Setting up pool2
I0705 14:17:50.361142 28973 net.cpp:148] Top shape: 20 5 14 14 (19600)
I0705 14:17:50.361147 28973 net.cpp:156] Memory required for data: 88152640
I0705 14:17:50.361151 28973 layer_factory.hpp:77] Creating layer ip_conv1
I0705 14:17:50.361161 28973 net.cpp:91] Creating Layer ip_conv1
I0705 14:17:50.361166 28973 net.cpp:425] ip_conv1 <- pool2
I0705 14:17:50.361182 28973 net.cpp:399] ip_conv1 -> ip_conv_1
I0705 14:17:50.361371 28973 net.cpp:141] Setting up ip_conv1
I0705 14:17:50.361382 28973 net.cpp:148] Top shape: 20 32 8 8 (40960)
I0705 14:17:50.361387 28973 net.cpp:156] Memory required for data: 88316480
I0705 14:17:50.361395 28973 layer_factory.hpp:77] Creating layer relu1
I0705 14:17:50.361404 28973 net.cpp:91] Creating Layer relu1
I0705 14:17:50.361408 28973 net.cpp:425] relu1 <- ip_conv_1
I0705 14:17:50.361414 28973 net.cpp:386] relu1 -> ip_conv_1 (in-place)
I0705 14:17:50.361423 28973 net.cpp:141] Setting up relu1
I0705 14:17:50.361428 28973 net.cpp:148] Top shape: 20 32 8 8 (40960)
I0705 14:17:50.361433 28973 net.cpp:156] Memory required for data: 88480320
I0705 14:17:50.361438 28973 layer_factory.hpp:77] Creating layer drop1
I0705 14:17:50.361449 28973 net.cpp:91] Creating Layer drop1
I0705 14:17:50.361454 28973 net.cpp:425] drop1 <- ip_conv_1
I0705 14:17:50.361459 28973 net.cpp:386] drop1 -> ip_conv_1 (in-place)
I0705 14:17:50.361479 28973 net.cpp:141] Setting up drop1
I0705 14:17:50.361485 28973 net.cpp:148] Top shape: 20 32 8 8 (40960)
I0705 14:17:50.361490 28973 net.cpp:156] Memory required for data: 88644160
I0705 14:17:50.361495 28973 layer_factory.hpp:77] Creating layer ip_conv2
I0705 14:17:50.361505 28973 net.cpp:91] Creating Layer ip_conv2
I0705 14:17:50.361510 28973 net.cpp:425] ip_conv2 <- ip_conv_1
I0705 14:17:50.361516 28973 net.cpp:399] ip_conv2 -> ip_conv2
I0705 14:17:50.361649 28973 net.cpp:141] Setting up ip_conv2
I0705 14:17:50.361656 28973 net.cpp:148] Top shape: 20 2 8 8 (2560)
I0705 14:17:50.361660 28973 net.cpp:156] Memory required for data: 88654400
I0705 14:17:50.361667 28973 layer_factory.hpp:77] Creating layer upsample
I0705 14:17:50.361676 28973 net.cpp:91] Creating Layer upsample
I0705 14:17:50.361681 28973 net.cpp:425] upsample <- ip_conv2
I0705 14:17:50.361688 28973 net.cpp:399] upsample -> upsample
I0705 14:17:50.362262 28973 net.cpp:141] Setting up upsample
I0705 14:17:50.362277 28973 net.cpp:148] Top shape: 20 2 254 254 (2580640)
I0705 14:17:50.362282 28973 net.cpp:156] Memory required for data: 98976960
I0705 14:17:50.362289 28973 layer_factory.hpp:77] Creating layer loss
I0705 14:17:50.362299 28973 net.cpp:91] Creating Layer loss
I0705 14:17:50.362304 28973 net.cpp:425] loss <- upsample
I0705 14:17:50.362309 28973 net.cpp:425] loss <- label
I0705 14:17:50.362318 28973 net.cpp:399] loss -> loss
I0705 14:17:50.362334 28973 layer_factory.hpp:77] Creating layer loss
I0705 14:17:50.367231 28973 net.cpp:141] Setting up loss
I0705 14:17:50.367264 28973 net.cpp:148] Top shape: (1)
I0705 14:17:50.367269 28973 net.cpp:151]     with loss weight 1
I0705 14:17:50.367290 28973 net.cpp:156] Memory required for data: 98976964
I0705 14:17:50.367296 28973 net.cpp:217] loss needs backward computation.
I0705 14:17:50.367303 28973 net.cpp:217] upsample needs backward computation.
I0705 14:17:50.367308 28973 net.cpp:219] ip_conv2 does not need backward computation.
I0705 14:17:50.367314 28973 net.cpp:219] drop1 does not need backward computation.
I0705 14:17:50.367319 28973 net.cpp:219] relu1 does not need backward computation.
I0705 14:17:50.367324 28973 net.cpp:219] ip_conv1 does not need backward computation.
I0705 14:17:50.367329 28973 net.cpp:219] pool2 does not need backward computation.
I0705 14:17:50.367334 28973 net.cpp:219] conv2 does not need backward computation.
I0705 14:17:50.367339 28973 net.cpp:219] pool1 does not need backward computation.
I0705 14:17:50.367344 28973 net.cpp:219] conv1 does not need backward computation.
I0705 14:17:50.367349 28973 net.cpp:219] training_cells does not need backward computation.
I0705 14:17:50.367354 28973 net.cpp:261] This network produces output loss
I0705 14:17:50.367365 28973 net.cpp:274] Network initialization done.
I0705 14:17:50.367735 28973 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_deconv_trainer.prototxt
I0705 14:17:50.367784 28973 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0705 14:17:50.367880 28973 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "ChunkingFishFovDataLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'seed\': 1337, \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_same_res/\', \'split\': \'test\', \'n_samples\': 5, \'chunker_params\': {\'chunk_size\': 254, \'window_size\': 1}}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip_conv1"
  type: "Convolution"
  bottom: "pool2"
  top: "ip_conv_1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip_conv2"
  type: "Convolution"
  bottom: "ip_conv_1"
  top: "ip_conv2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "ip_conv2"
  top: "upsample"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 2
    bias_term: false
    kernel_size: 149
    stride: 15
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "upsample"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "InfogainSoftmaxLoss"
  bottom: "upsample"
  bottom: "label"
  top: "loss"
  loss_param {
    normalize: true
  }
  infogain_loss_param {
    source: "/home/sam/code/fisherman/infogainH.binaryproto"
  }
  softmax_param {
    axis: 1
  }
}
I0705 14:17:50.368345 28973 layer_factory.hpp:77] Creating layer testing_cells
I0705 14:17:50.368399 28973 net.cpp:91] Creating Layer testing_cells
I0705 14:17:50.368408 28973 net.cpp:399] testing_cells -> image
I0705 14:17:50.368417 28973 net.cpp:399] testing_cells -> label
I0705 14:17:50.397079 28973 net.cpp:141] Setting up testing_cells
I0705 14:17:50.397111 28973 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0705 14:17:50.397119 28973 net.cpp:148] Top shape: 5 1 254 254 (322580)
I0705 14:17:50.397125 28973 net.cpp:156] Memory required for data: 3870960
I0705 14:17:50.397133 28973 layer_factory.hpp:77] Creating layer label_testing_cells_1_split
I0705 14:17:50.397150 28973 net.cpp:91] Creating Layer label_testing_cells_1_split
I0705 14:17:50.397156 28973 net.cpp:425] label_testing_cells_1_split <- label
I0705 14:17:50.397162 28973 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_0
I0705 14:17:50.397192 28973 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_1
I0705 14:17:50.397233 28973 net.cpp:141] Setting up label_testing_cells_1_split
I0705 14:17:50.397240 28973 net.cpp:148] Top shape: 5 1 254 254 (322580)
I0705 14:17:50.397246 28973 net.cpp:148] Top shape: 5 1 254 254 (322580)
I0705 14:17:50.397250 28973 net.cpp:156] Memory required for data: 6451600
I0705 14:17:50.397254 28973 layer_factory.hpp:77] Creating layer conv1
I0705 14:17:50.397266 28973 net.cpp:91] Creating Layer conv1
I0705 14:17:50.397271 28973 net.cpp:425] conv1 <- image
I0705 14:17:50.397277 28973 net.cpp:399] conv1 -> conv1
I0705 14:17:50.397500 28973 net.cpp:141] Setting up conv1
I0705 14:17:50.397511 28973 net.cpp:148] Top shape: 5 15 240 240 (4320000)
I0705 14:17:50.397516 28973 net.cpp:156] Memory required for data: 23731600
I0705 14:17:50.397526 28973 layer_factory.hpp:77] Creating layer pool1
I0705 14:17:50.397533 28973 net.cpp:91] Creating Layer pool1
I0705 14:17:50.397538 28973 net.cpp:425] pool1 <- conv1
I0705 14:17:50.397544 28973 net.cpp:399] pool1 -> pool1
I0705 14:17:50.397569 28973 net.cpp:141] Setting up pool1
I0705 14:17:50.397575 28973 net.cpp:148] Top shape: 5 15 48 48 (172800)
I0705 14:17:50.397580 28973 net.cpp:156] Memory required for data: 24422800
I0705 14:17:50.397584 28973 layer_factory.hpp:77] Creating layer conv2
I0705 14:17:50.397593 28973 net.cpp:91] Creating Layer conv2
I0705 14:17:50.397598 28973 net.cpp:425] conv2 <- pool1
I0705 14:17:50.397603 28973 net.cpp:399] conv2 -> conv2
I0705 14:17:50.397755 28973 net.cpp:141] Setting up conv2
I0705 14:17:50.397763 28973 net.cpp:148] Top shape: 5 5 42 42 (44100)
I0705 14:17:50.397768 28973 net.cpp:156] Memory required for data: 24599200
I0705 14:17:50.397776 28973 layer_factory.hpp:77] Creating layer pool2
I0705 14:17:50.397783 28973 net.cpp:91] Creating Layer pool2
I0705 14:17:50.397788 28973 net.cpp:425] pool2 <- conv2
I0705 14:17:50.397794 28973 net.cpp:399] pool2 -> pool2
I0705 14:17:50.397815 28973 net.cpp:141] Setting up pool2
I0705 14:17:50.397821 28973 net.cpp:148] Top shape: 5 5 14 14 (4900)
I0705 14:17:50.397878 28973 net.cpp:156] Memory required for data: 24618800
I0705 14:17:50.397883 28973 layer_factory.hpp:77] Creating layer ip_conv1
I0705 14:17:50.397892 28973 net.cpp:91] Creating Layer ip_conv1
I0705 14:17:50.397897 28973 net.cpp:425] ip_conv1 <- pool2
I0705 14:17:50.397903 28973 net.cpp:399] ip_conv1 -> ip_conv_1
I0705 14:17:50.398092 28973 net.cpp:141] Setting up ip_conv1
I0705 14:17:50.398111 28973 net.cpp:148] Top shape: 5 32 8 8 (10240)
I0705 14:17:50.398115 28973 net.cpp:156] Memory required for data: 24659760
I0705 14:17:50.398124 28973 layer_factory.hpp:77] Creating layer relu1
I0705 14:17:50.398131 28973 net.cpp:91] Creating Layer relu1
I0705 14:17:50.398135 28973 net.cpp:425] relu1 <- ip_conv_1
I0705 14:17:50.398141 28973 net.cpp:386] relu1 -> ip_conv_1 (in-place)
I0705 14:17:50.398147 28973 net.cpp:141] Setting up relu1
I0705 14:17:50.398154 28973 net.cpp:148] Top shape: 5 32 8 8 (10240)
I0705 14:17:50.398157 28973 net.cpp:156] Memory required for data: 24700720
I0705 14:17:50.398161 28973 layer_factory.hpp:77] Creating layer drop1
I0705 14:17:50.398169 28973 net.cpp:91] Creating Layer drop1
I0705 14:17:50.398174 28973 net.cpp:425] drop1 <- ip_conv_1
I0705 14:17:50.398178 28973 net.cpp:386] drop1 -> ip_conv_1 (in-place)
I0705 14:17:50.398195 28973 net.cpp:141] Setting up drop1
I0705 14:17:50.398202 28973 net.cpp:148] Top shape: 5 32 8 8 (10240)
I0705 14:17:50.398206 28973 net.cpp:156] Memory required for data: 24741680
I0705 14:17:50.398211 28973 layer_factory.hpp:77] Creating layer ip_conv2
I0705 14:17:50.398219 28973 net.cpp:91] Creating Layer ip_conv2
I0705 14:17:50.398223 28973 net.cpp:425] ip_conv2 <- ip_conv_1
I0705 14:17:50.398229 28973 net.cpp:399] ip_conv2 -> ip_conv2
I0705 14:17:50.398356 28973 net.cpp:141] Setting up ip_conv2
I0705 14:17:50.398363 28973 net.cpp:148] Top shape: 5 2 8 8 (640)
I0705 14:17:50.398367 28973 net.cpp:156] Memory required for data: 24744240
I0705 14:17:50.398375 28973 layer_factory.hpp:77] Creating layer upsample
I0705 14:17:50.398381 28973 net.cpp:91] Creating Layer upsample
I0705 14:17:50.398386 28973 net.cpp:425] upsample <- ip_conv2
I0705 14:17:50.398392 28973 net.cpp:399] upsample -> upsample
I0705 14:17:50.398557 28973 net.cpp:141] Setting up upsample
I0705 14:17:50.398578 28973 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0705 14:17:50.398583 28973 net.cpp:156] Memory required for data: 27324880
I0705 14:17:50.398600 28973 layer_factory.hpp:77] Creating layer upsample_upsample_0_split
I0705 14:17:50.398607 28973 net.cpp:91] Creating Layer upsample_upsample_0_split
I0705 14:17:50.398612 28973 net.cpp:425] upsample_upsample_0_split <- upsample
I0705 14:17:50.398617 28973 net.cpp:399] upsample_upsample_0_split -> upsample_upsample_0_split_0
I0705 14:17:50.398624 28973 net.cpp:399] upsample_upsample_0_split -> upsample_upsample_0_split_1
I0705 14:17:50.398645 28973 net.cpp:141] Setting up upsample_upsample_0_split
I0705 14:17:50.398653 28973 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0705 14:17:50.398658 28973 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0705 14:17:50.398661 28973 net.cpp:156] Memory required for data: 32486160
I0705 14:17:50.398666 28973 layer_factory.hpp:77] Creating layer accuracy
I0705 14:17:50.398675 28973 net.cpp:91] Creating Layer accuracy
I0705 14:17:50.398680 28973 net.cpp:425] accuracy <- upsample_upsample_0_split_0
I0705 14:17:50.398686 28973 net.cpp:425] accuracy <- label_testing_cells_1_split_0
I0705 14:17:50.398692 28973 net.cpp:399] accuracy -> accuracy
I0705 14:17:50.398700 28973 net.cpp:141] Setting up accuracy
I0705 14:17:50.398705 28973 net.cpp:148] Top shape: (1)
I0705 14:17:50.398710 28973 net.cpp:156] Memory required for data: 32486164
I0705 14:17:50.398715 28973 layer_factory.hpp:77] Creating layer loss
I0705 14:17:50.398722 28973 net.cpp:91] Creating Layer loss
I0705 14:17:50.398726 28973 net.cpp:425] loss <- upsample_upsample_0_split_1
I0705 14:17:50.398731 28973 net.cpp:425] loss <- label_testing_cells_1_split_1
I0705 14:17:50.398737 28973 net.cpp:399] loss -> loss
I0705 14:17:50.398775 28973 layer_factory.hpp:77] Creating layer loss
I0705 14:17:50.407271 28973 net.cpp:141] Setting up loss
I0705 14:17:50.407301 28973 net.cpp:148] Top shape: (1)
I0705 14:17:50.407306 28973 net.cpp:151]     with loss weight 1
I0705 14:17:50.407318 28973 net.cpp:156] Memory required for data: 32486168
I0705 14:17:50.407323 28973 net.cpp:217] loss needs backward computation.
I0705 14:17:50.407330 28973 net.cpp:219] accuracy does not need backward computation.
I0705 14:17:50.407335 28973 net.cpp:217] upsample_upsample_0_split needs backward computation.
I0705 14:17:50.407338 28973 net.cpp:217] upsample needs backward computation.
I0705 14:17:50.407343 28973 net.cpp:219] ip_conv2 does not need backward computation.
I0705 14:17:50.407348 28973 net.cpp:219] drop1 does not need backward computation.
I0705 14:17:50.407353 28973 net.cpp:219] relu1 does not need backward computation.
I0705 14:17:50.407358 28973 net.cpp:219] ip_conv1 does not need backward computation.
I0705 14:17:50.407362 28973 net.cpp:219] pool2 does not need backward computation.
I0705 14:17:50.407367 28973 net.cpp:219] conv2 does not need backward computation.
I0705 14:17:50.407372 28973 net.cpp:219] pool1 does not need backward computation.
I0705 14:17:50.407377 28973 net.cpp:219] conv1 does not need backward computation.
I0705 14:17:50.407382 28973 net.cpp:219] label_testing_cells_1_split does not need backward computation.
I0705 14:17:50.407387 28973 net.cpp:219] testing_cells does not need backward computation.
I0705 14:17:50.407392 28973 net.cpp:261] This network produces output accuracy
I0705 14:17:50.407397 28973 net.cpp:261] This network produces output loss
I0705 14:17:50.407407 28973 net.cpp:274] Network initialization done.
I0705 14:17:50.407470 28973 solver.cpp:60] Solver scaffolding done.
I0705 14:17:50.407667 28973 caffe.cpp:129] Finetuning from fish_net_pretrain_conv.caffemodel
I0705 14:17:50.408381 28973 net.cpp:752] Ignoring source layer testing_cells
I0705 14:17:50.408392 28973 net.cpp:752] Ignoring source layer label_testing_cells_1_split
I0705 14:17:50.408483 28973 net.cpp:752] Ignoring source layer upsample_upsample_0_split
I0705 14:17:50.408489 28973 net.cpp:752] Ignoring source layer accuracy
I0705 14:17:50.409116 28973 caffe.cpp:219] Starting Optimization
I0705 14:17:50.409135 28973 solver.cpp:279] Solving fish_filter
I0705 14:17:50.409144 28973 solver.cpp:280] Learning Rate Policy: fixed
I0705 14:17:50.409595 28973 solver.cpp:337] Iteration 0, Testing net (#0)
I0705 14:17:50.409618 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:17:57.052629 28973 solver.cpp:404]     Test net output #0: accuracy = 0.0235556
I0705 14:17:57.052675 28973 solver.cpp:404]     Test net output #1: loss = 0.665707 (* 1 = 0.665707 loss)
I0705 14:17:57.389834 28973 solver.cpp:228] Iteration 0, loss = 0.67622
I0705 14:17:57.394201 28973 solver.cpp:244]     Train net output #0: loss = 0.67622 (* 1 = 0.67622 loss)
I0705 14:17:57.394243 28973 sgd_solver.cpp:106] Iteration 0, lr = 1e-07
I0705 14:18:09.036914 28973 solver.cpp:228] Iteration 20, loss = 0.675073
I0705 14:18:09.036974 28973 solver.cpp:244]     Train net output #0: loss = 0.672821 (* 1 = 0.672821 loss)
I0705 14:18:09.036989 28973 sgd_solver.cpp:106] Iteration 20, lr = 1e-07
I0705 14:18:20.571318 28973 solver.cpp:228] Iteration 40, loss = 0.6751
I0705 14:18:20.571419 28973 solver.cpp:244]     Train net output #0: loss = 0.679776 (* 1 = 0.679776 loss)
I0705 14:18:20.571429 28973 sgd_solver.cpp:106] Iteration 40, lr = 1e-07
I0705 14:18:32.377281 28973 solver.cpp:228] Iteration 60, loss = 0.675651
I0705 14:18:32.377336 28973 solver.cpp:244]     Train net output #0: loss = 0.673577 (* 1 = 0.673577 loss)
I0705 14:18:32.377348 28973 sgd_solver.cpp:106] Iteration 60, lr = 1e-07
I0705 14:18:44.210301 28973 solver.cpp:228] Iteration 80, loss = 0.675275
I0705 14:18:44.210360 28973 solver.cpp:244]     Train net output #0: loss = 0.67582 (* 1 = 0.67582 loss)
I0705 14:18:44.210373 28973 sgd_solver.cpp:106] Iteration 80, lr = 1e-07
I0705 14:18:55.349079 28973 solver.cpp:337] Iteration 100, Testing net (#0)
I0705 14:18:55.349197 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:19:01.905282 28973 solver.cpp:404]     Test net output #0: accuracy = 0.820731
I0705 14:19:01.905346 28973 solver.cpp:404]     Test net output #1: loss = 0.664973 (* 1 = 0.664973 loss)
I0705 14:19:02.287688 28973 solver.cpp:228] Iteration 100, loss = 0.675981
I0705 14:19:02.287745 28973 solver.cpp:244]     Train net output #0: loss = 0.678568 (* 1 = 0.678568 loss)
I0705 14:19:02.287760 28973 sgd_solver.cpp:106] Iteration 100, lr = 1e-07
I0705 14:19:14.957159 28973 solver.cpp:228] Iteration 120, loss = 0.67534
I0705 14:19:14.957228 28973 solver.cpp:244]     Train net output #0: loss = 0.679224 (* 1 = 0.679224 loss)
I0705 14:19:14.957243 28973 sgd_solver.cpp:106] Iteration 120, lr = 1e-07
I0705 14:19:27.301987 28973 solver.cpp:228] Iteration 140, loss = 0.675758
I0705 14:19:27.302104 28973 solver.cpp:244]     Train net output #0: loss = 0.674913 (* 1 = 0.674913 loss)
I0705 14:19:27.302120 28973 sgd_solver.cpp:106] Iteration 140, lr = 1e-07
I0705 14:19:39.491951 28973 solver.cpp:228] Iteration 160, loss = 0.676053
I0705 14:19:39.492008 28973 solver.cpp:244]     Train net output #0: loss = 0.677334 (* 1 = 0.677334 loss)
I0705 14:19:39.492022 28973 sgd_solver.cpp:106] Iteration 160, lr = 1e-07
I0705 14:19:52.467797 28973 solver.cpp:228] Iteration 180, loss = 0.6749
I0705 14:19:52.467856 28973 solver.cpp:244]     Train net output #0: loss = 0.676146 (* 1 = 0.676146 loss)
I0705 14:19:52.467870 28973 sgd_solver.cpp:106] Iteration 180, lr = 1e-07
I0705 14:20:04.375167 28973 solver.cpp:337] Iteration 200, Testing net (#0)
I0705 14:20:04.375267 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:20:11.274755 28973 solver.cpp:404]     Test net output #0: accuracy = 0.826466
I0705 14:20:11.274794 28973 solver.cpp:404]     Test net output #1: loss = 0.665439 (* 1 = 0.665439 loss)
I0705 14:20:11.643056 28973 solver.cpp:228] Iteration 200, loss = 0.676342
I0705 14:20:11.643108 28973 solver.cpp:244]     Train net output #0: loss = 0.677282 (* 1 = 0.677282 loss)
I0705 14:20:11.643123 28973 sgd_solver.cpp:106] Iteration 200, lr = 1e-07
I0705 14:20:24.039975 28973 solver.cpp:228] Iteration 220, loss = 0.675674
I0705 14:20:24.040035 28973 solver.cpp:244]     Train net output #0: loss = 0.673852 (* 1 = 0.673852 loss)
I0705 14:20:24.040048 28973 sgd_solver.cpp:106] Iteration 220, lr = 1e-07
I0705 14:20:36.569527 28973 solver.cpp:228] Iteration 240, loss = 0.676183
I0705 14:20:36.569645 28973 solver.cpp:244]     Train net output #0: loss = 0.677036 (* 1 = 0.677036 loss)
I0705 14:20:36.569664 28973 sgd_solver.cpp:106] Iteration 240, lr = 1e-07
I0705 14:20:48.915969 28973 solver.cpp:228] Iteration 260, loss = 0.675563
I0705 14:20:48.916015 28973 solver.cpp:244]     Train net output #0: loss = 0.673021 (* 1 = 0.673021 loss)
I0705 14:20:48.916025 28973 sgd_solver.cpp:106] Iteration 260, lr = 1e-07
I0705 14:21:01.235630 28973 solver.cpp:228] Iteration 280, loss = 0.675359
I0705 14:21:01.235687 28973 solver.cpp:244]     Train net output #0: loss = 0.672342 (* 1 = 0.672342 loss)
I0705 14:21:01.235702 28973 sgd_solver.cpp:106] Iteration 280, lr = 1e-07
I0705 14:21:13.004969 28973 solver.cpp:337] Iteration 300, Testing net (#0)
I0705 14:21:13.005069 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:21:19.415223 28973 solver.cpp:404]     Test net output #0: accuracy = 0.813569
I0705 14:21:19.415262 28973 solver.cpp:404]     Test net output #1: loss = 0.665189 (* 1 = 0.665189 loss)
I0705 14:21:19.758797 28973 solver.cpp:228] Iteration 300, loss = 0.674781
I0705 14:21:19.758852 28973 solver.cpp:244]     Train net output #0: loss = 0.673632 (* 1 = 0.673632 loss)
I0705 14:21:19.758867 28973 sgd_solver.cpp:106] Iteration 300, lr = 1e-07
I0705 14:21:31.455888 28973 solver.cpp:228] Iteration 320, loss = 0.675276
I0705 14:21:31.455948 28973 solver.cpp:244]     Train net output #0: loss = 0.674669 (* 1 = 0.674669 loss)
I0705 14:21:31.455965 28973 sgd_solver.cpp:106] Iteration 320, lr = 1e-07
I0705 14:21:43.042727 28973 solver.cpp:228] Iteration 340, loss = 0.67538
I0705 14:21:43.045719 28973 solver.cpp:244]     Train net output #0: loss = 0.675159 (* 1 = 0.675159 loss)
I0705 14:21:43.045738 28973 sgd_solver.cpp:106] Iteration 340, lr = 1e-07
I0705 14:21:54.645612 28973 solver.cpp:228] Iteration 360, loss = 0.676231
I0705 14:21:54.645668 28973 solver.cpp:244]     Train net output #0: loss = 0.67578 (* 1 = 0.67578 loss)
I0705 14:21:54.645680 28973 sgd_solver.cpp:106] Iteration 360, lr = 1e-07
I0705 14:22:06.250507 28973 solver.cpp:228] Iteration 380, loss = 0.676254
I0705 14:22:06.250555 28973 solver.cpp:244]     Train net output #0: loss = 0.675497 (* 1 = 0.675497 loss)
I0705 14:22:06.250563 28973 sgd_solver.cpp:106] Iteration 380, lr = 1e-07
I0705 14:22:17.247159 28973 solver.cpp:337] Iteration 400, Testing net (#0)
I0705 14:22:17.247246 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:22:23.483314 28973 solver.cpp:404]     Test net output #0: accuracy = 0.836824
I0705 14:22:23.483353 28973 solver.cpp:404]     Test net output #1: loss = 0.665521 (* 1 = 0.665521 loss)
I0705 14:22:23.840536 28973 solver.cpp:228] Iteration 400, loss = 0.675524
I0705 14:22:23.840598 28973 solver.cpp:244]     Train net output #0: loss = 0.674165 (* 1 = 0.674165 loss)
I0705 14:22:23.840612 28973 sgd_solver.cpp:106] Iteration 400, lr = 1e-07
I0705 14:22:35.330003 28973 solver.cpp:228] Iteration 420, loss = 0.675898
I0705 14:22:35.330062 28973 solver.cpp:244]     Train net output #0: loss = 0.675115 (* 1 = 0.675115 loss)
I0705 14:22:35.330075 28973 sgd_solver.cpp:106] Iteration 420, lr = 1e-07
I0705 14:22:46.751375 28973 solver.cpp:228] Iteration 440, loss = 0.675314
I0705 14:22:46.751421 28973 solver.cpp:244]     Train net output #0: loss = 0.675744 (* 1 = 0.675744 loss)
I0705 14:22:46.751432 28973 sgd_solver.cpp:106] Iteration 440, lr = 1e-07
I0705 14:22:58.404184 28973 solver.cpp:228] Iteration 460, loss = 0.675632
I0705 14:22:58.404301 28973 solver.cpp:244]     Train net output #0: loss = 0.672383 (* 1 = 0.672383 loss)
I0705 14:22:58.404314 28973 sgd_solver.cpp:106] Iteration 460, lr = 1e-07
I0705 14:23:10.233487 28973 solver.cpp:228] Iteration 480, loss = 0.67584
I0705 14:23:10.233546 28973 solver.cpp:244]     Train net output #0: loss = 0.676883 (* 1 = 0.676883 loss)
I0705 14:23:10.233557 28973 sgd_solver.cpp:106] Iteration 480, lr = 1e-07
I0705 14:23:20.995012 28973 solver.cpp:337] Iteration 500, Testing net (#0)
I0705 14:23:20.995054 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:23:27.189878 28973 solver.cpp:404]     Test net output #0: accuracy = 0.854714
I0705 14:23:27.189918 28973 solver.cpp:404]     Test net output #1: loss = 0.665471 (* 1 = 0.665471 loss)
I0705 14:23:27.553251 28973 solver.cpp:228] Iteration 500, loss = 0.675051
I0705 14:23:27.553297 28973 solver.cpp:244]     Train net output #0: loss = 0.673047 (* 1 = 0.673047 loss)
I0705 14:23:27.553308 28973 sgd_solver.cpp:106] Iteration 500, lr = 1e-07
I0705 14:23:39.126220 28973 solver.cpp:228] Iteration 520, loss = 0.675457
I0705 14:23:39.126338 28973 solver.cpp:244]     Train net output #0: loss = 0.672751 (* 1 = 0.672751 loss)
I0705 14:23:39.126353 28973 sgd_solver.cpp:106] Iteration 520, lr = 1e-07
I0705 14:23:50.706360 28973 solver.cpp:228] Iteration 540, loss = 0.676674
I0705 14:23:50.706418 28973 solver.cpp:244]     Train net output #0: loss = 0.68019 (* 1 = 0.68019 loss)
I0705 14:23:50.706432 28973 sgd_solver.cpp:106] Iteration 540, lr = 1e-07
I0705 14:24:02.204843 28973 solver.cpp:228] Iteration 560, loss = 0.675326
I0705 14:24:02.204900 28973 solver.cpp:244]     Train net output #0: loss = 0.676851 (* 1 = 0.676851 loss)
I0705 14:24:02.204916 28973 sgd_solver.cpp:106] Iteration 560, lr = 1e-07
I0705 14:24:13.696705 28973 solver.cpp:228] Iteration 580, loss = 0.675177
I0705 14:24:13.696825 28973 solver.cpp:244]     Train net output #0: loss = 0.67621 (* 1 = 0.67621 loss)
I0705 14:24:13.696841 28973 sgd_solver.cpp:106] Iteration 580, lr = 1e-07
I0705 14:24:24.632459 28973 solver.cpp:337] Iteration 600, Testing net (#0)
I0705 14:24:24.632499 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:24:30.885921 28973 solver.cpp:404]     Test net output #0: accuracy = 0.80787
I0705 14:24:30.885975 28973 solver.cpp:404]     Test net output #1: loss = 0.665165 (* 1 = 0.665165 loss)
I0705 14:24:31.257148 28973 solver.cpp:228] Iteration 600, loss = 0.675431
I0705 14:24:31.257210 28973 solver.cpp:244]     Train net output #0: loss = 0.675243 (* 1 = 0.675243 loss)
I0705 14:24:31.257225 28973 sgd_solver.cpp:106] Iteration 600, lr = 1e-07
I0705 14:24:42.325263 28973 solver.cpp:228] Iteration 620, loss = 0.675313
I0705 14:24:42.325325 28973 solver.cpp:244]     Train net output #0: loss = 0.6714 (* 1 = 0.6714 loss)
I0705 14:24:42.325338 28973 sgd_solver.cpp:106] Iteration 620, lr = 1e-07
I0705 14:24:53.455801 28973 solver.cpp:228] Iteration 640, loss = 0.675067
I0705 14:24:53.455932 28973 solver.cpp:244]     Train net output #0: loss = 0.674822 (* 1 = 0.674822 loss)
I0705 14:24:53.455948 28973 sgd_solver.cpp:106] Iteration 640, lr = 1e-07
I0705 14:25:04.556046 28973 solver.cpp:228] Iteration 660, loss = 0.675681
I0705 14:25:04.556102 28973 solver.cpp:244]     Train net output #0: loss = 0.677747 (* 1 = 0.677747 loss)
I0705 14:25:04.556118 28973 sgd_solver.cpp:106] Iteration 660, lr = 1e-07
I0705 14:25:15.717707 28973 solver.cpp:228] Iteration 680, loss = 0.675112
I0705 14:25:15.717763 28973 solver.cpp:244]     Train net output #0: loss = 0.673988 (* 1 = 0.673988 loss)
I0705 14:25:15.717778 28973 sgd_solver.cpp:106] Iteration 680, lr = 1e-07
I0705 14:25:26.221627 28973 solver.cpp:337] Iteration 700, Testing net (#0)
I0705 14:25:26.221732 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:25:32.523761 28973 solver.cpp:404]     Test net output #0: accuracy = 0.811863
I0705 14:25:32.523802 28973 solver.cpp:404]     Test net output #1: loss = 0.665612 (* 1 = 0.665612 loss)
I0705 14:25:32.921057 28973 solver.cpp:228] Iteration 700, loss = 0.676053
I0705 14:25:32.921118 28973 solver.cpp:244]     Train net output #0: loss = 0.679686 (* 1 = 0.679686 loss)
I0705 14:25:32.921134 28973 sgd_solver.cpp:106] Iteration 700, lr = 1e-07
I0705 14:25:44.609868 28973 solver.cpp:228] Iteration 720, loss = 0.675865
I0705 14:25:44.609925 28973 solver.cpp:244]     Train net output #0: loss = 0.674273 (* 1 = 0.674273 loss)
I0705 14:25:44.609937 28973 sgd_solver.cpp:106] Iteration 720, lr = 1e-07
I0705 14:25:56.229604 28973 solver.cpp:228] Iteration 740, loss = 0.675978
I0705 14:25:56.229727 28973 solver.cpp:244]     Train net output #0: loss = 0.67218 (* 1 = 0.67218 loss)
I0705 14:25:56.229743 28973 sgd_solver.cpp:106] Iteration 740, lr = 1e-07
I0705 14:26:07.836794 28973 solver.cpp:228] Iteration 760, loss = 0.676339
I0705 14:26:07.836853 28973 solver.cpp:244]     Train net output #0: loss = 0.677072 (* 1 = 0.677072 loss)
I0705 14:26:07.836869 28973 sgd_solver.cpp:106] Iteration 760, lr = 1e-07
I0705 14:26:19.200166 28973 solver.cpp:228] Iteration 780, loss = 0.676074
I0705 14:26:19.200222 28973 solver.cpp:244]     Train net output #0: loss = 0.675743 (* 1 = 0.675743 loss)
I0705 14:26:19.200237 28973 sgd_solver.cpp:106] Iteration 780, lr = 1e-07
I0705 14:26:30.257706 28973 solver.cpp:337] Iteration 800, Testing net (#0)
I0705 14:26:30.257812 28973 net.cpp:684] Ignoring source layer training_cells
I0705 14:26:36.449496 28973 solver.cpp:404]     Test net output #0: accuracy = 0.830645
I0705 14:26:36.449554 28973 solver.cpp:404]     Test net output #1: loss = 0.66496 (* 1 = 0.66496 loss)
I0705 14:26:36.797356 28973 solver.cpp:228] Iteration 800, loss = 0.676789
I0705 14:26:36.797410 28973 solver.cpp:244]     Train net output #0: loss = 0.676649 (* 1 = 0.676649 loss)
I0705 14:26:36.797423 28973 sgd_solver.cpp:106] Iteration 800, lr = 1e-07
I0705 14:26:48.535048 28973 solver.cpp:228] Iteration 820, loss = 0.675571
I0705 14:26:48.535107 28973 solver.cpp:244]     Train net output #0: loss = 0.676606 (* 1 = 0.676606 loss)
I0705 14:26:48.535122 28973 sgd_solver.cpp:106] Iteration 820, lr = 1e-07
I0705 14:27:00.288800 28973 solver.cpp:228] Iteration 840, loss = 0.676266
I0705 14:27:00.288934 28973 solver.cpp:244]     Train net output #0: loss = 0.675863 (* 1 = 0.675863 loss)
I0705 14:27:00.288949 28973 sgd_solver.cpp:106] Iteration 840, lr = 1e-07
I0705 14:27:11.964170 28973 solver.cpp:228] Iteration 860, loss = 0.675303
I0705 14:27:11.964228 28973 solver.cpp:244]     Train net output #0: loss = 0.673412 (* 1 = 0.673412 loss)
I0705 14:27:11.964243 28973 sgd_solver.cpp:106] Iteration 860, lr = 1e-07
I0705 14:27:20.326254 28973 solver.cpp:454] Snapshotting to binary proto file fish_net_deconv_output_iter_875.caffemodel
I0705 14:27:20.663420 28973 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_deconv_output_iter_875.solverstate
I0705 14:27:20.664755 28973 solver.cpp:301] Optimization stopped early.
I0705 14:27:20.664772 28973 caffe.cpp:222] Optimization Done.
