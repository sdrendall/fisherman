Log file created at: 2016/07/02 10:25:22
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0702 10:25:22.238551  7210 caffe.cpp:185] Using GPUs 0
I0702 10:25:22.331118  7210 caffe.cpp:190] GPU 0: GeForce GTX 760
I0702 10:25:22.493175  7210 solver.cpp:48] Initializing solver from parameters: 
test_iter: 50
test_interval: 500
base_lr: 1e-07
display: 20
max_iter: 300000
lr_policy: "fixed"
momentum: 0.75
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "fish_net_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_train_test.prototxt"
average_loss: 20
I0702 10:25:22.493397  7210 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_train_test.prototxt
I0702 10:25:22.493784  7210 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0702 10:25:22.493814  7210 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0702 10:25:22.493894  7210 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "ChunkingFishFovDataLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'seed\': 1337, \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_same_res/\', \'split\': \'train\', \'n_samples\': 20, \'chunker_params\': {\'chunk_size\': 254, \'window_size\': 1}}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip_conv1"
  type: "Convolution"
  bottom: "pool2"
  top: "ip_conv_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip_conv2"
  type: "Convolution"
  bottom: "ip_conv_1"
  top: "ip_conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "ip_conv2"
  top: "upsample"
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 2
    bias_term: false
    kernel_size: 149
    stride: 15
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "upsample"
  bottom: "label"
  top: "loss"
  loss_param {
    normalize: false
  }
}
I0702 10:25:22.494247  7210 layer_factory.hpp:77] Creating layer training_cells
I0702 10:25:23.262255  7210 net.cpp:91] Creating Layer training_cells
I0702 10:25:23.262290  7210 net.cpp:399] training_cells -> image
I0702 10:25:23.262312  7210 net.cpp:399] training_cells -> label
I0702 10:25:23.337190  7210 net.cpp:141] Setting up training_cells
I0702 10:25:23.337239  7210 net.cpp:148] Top shape: 20 2 254 254 (2580640)
I0702 10:25:23.337255  7210 net.cpp:148] Top shape: 20 1 254 254 (1290320)
I0702 10:25:23.337260  7210 net.cpp:156] Memory required for data: 15483840
I0702 10:25:23.337324  7210 layer_factory.hpp:77] Creating layer conv1
I0702 10:25:23.337347  7210 net.cpp:91] Creating Layer conv1
I0702 10:25:23.337353  7210 net.cpp:425] conv1 <- image
I0702 10:25:23.337368  7210 net.cpp:399] conv1 -> conv1
I0702 10:25:23.338521  7210 net.cpp:141] Setting up conv1
I0702 10:25:23.338546  7210 net.cpp:148] Top shape: 20 15 240 240 (17280000)
I0702 10:25:23.338551  7210 net.cpp:156] Memory required for data: 84603840
I0702 10:25:23.338567  7210 layer_factory.hpp:77] Creating layer pool1
I0702 10:25:23.338577  7210 net.cpp:91] Creating Layer pool1
I0702 10:25:23.338582  7210 net.cpp:425] pool1 <- conv1
I0702 10:25:23.338588  7210 net.cpp:399] pool1 -> pool1
I0702 10:25:23.338627  7210 net.cpp:141] Setting up pool1
I0702 10:25:23.338634  7210 net.cpp:148] Top shape: 20 15 48 48 (691200)
I0702 10:25:23.338639  7210 net.cpp:156] Memory required for data: 87368640
I0702 10:25:23.338644  7210 layer_factory.hpp:77] Creating layer conv2
I0702 10:25:23.338654  7210 net.cpp:91] Creating Layer conv2
I0702 10:25:23.338659  7210 net.cpp:425] conv2 <- pool1
I0702 10:25:23.338665  7210 net.cpp:399] conv2 -> conv2
I0702 10:25:23.339189  7210 net.cpp:141] Setting up conv2
I0702 10:25:23.339203  7210 net.cpp:148] Top shape: 20 5 42 42 (176400)
I0702 10:25:23.339208  7210 net.cpp:156] Memory required for data: 88074240
I0702 10:25:23.339217  7210 layer_factory.hpp:77] Creating layer pool2
I0702 10:25:23.339226  7210 net.cpp:91] Creating Layer pool2
I0702 10:25:23.339231  7210 net.cpp:425] pool2 <- conv2
I0702 10:25:23.339237  7210 net.cpp:399] pool2 -> pool2
I0702 10:25:23.339262  7210 net.cpp:141] Setting up pool2
I0702 10:25:23.339269  7210 net.cpp:148] Top shape: 20 5 14 14 (19600)
I0702 10:25:23.339273  7210 net.cpp:156] Memory required for data: 88152640
I0702 10:25:23.339277  7210 layer_factory.hpp:77] Creating layer ip_conv1
I0702 10:25:23.339287  7210 net.cpp:91] Creating Layer ip_conv1
I0702 10:25:23.339292  7210 net.cpp:425] ip_conv1 <- pool2
I0702 10:25:23.339298  7210 net.cpp:399] ip_conv1 -> ip_conv_1
I0702 10:25:23.339481  7210 net.cpp:141] Setting up ip_conv1
I0702 10:25:23.339489  7210 net.cpp:148] Top shape: 20 32 8 8 (40960)
I0702 10:25:23.339493  7210 net.cpp:156] Memory required for data: 88316480
I0702 10:25:23.339503  7210 layer_factory.hpp:77] Creating layer relu1
I0702 10:25:23.339509  7210 net.cpp:91] Creating Layer relu1
I0702 10:25:23.339514  7210 net.cpp:425] relu1 <- ip_conv_1
I0702 10:25:23.339520  7210 net.cpp:386] relu1 -> ip_conv_1 (in-place)
I0702 10:25:23.339529  7210 net.cpp:141] Setting up relu1
I0702 10:25:23.339534  7210 net.cpp:148] Top shape: 20 32 8 8 (40960)
I0702 10:25:23.339537  7210 net.cpp:156] Memory required for data: 88480320
I0702 10:25:23.339542  7210 layer_factory.hpp:77] Creating layer drop1
I0702 10:25:23.339550  7210 net.cpp:91] Creating Layer drop1
I0702 10:25:23.339555  7210 net.cpp:425] drop1 <- ip_conv_1
I0702 10:25:23.339561  7210 net.cpp:386] drop1 -> ip_conv_1 (in-place)
I0702 10:25:23.339579  7210 net.cpp:141] Setting up drop1
I0702 10:25:23.339586  7210 net.cpp:148] Top shape: 20 32 8 8 (40960)
I0702 10:25:23.339589  7210 net.cpp:156] Memory required for data: 88644160
I0702 10:25:23.339594  7210 layer_factory.hpp:77] Creating layer ip_conv2
I0702 10:25:23.339604  7210 net.cpp:91] Creating Layer ip_conv2
I0702 10:25:23.339609  7210 net.cpp:425] ip_conv2 <- ip_conv_1
I0702 10:25:23.339615  7210 net.cpp:399] ip_conv2 -> ip_conv2
I0702 10:25:23.339745  7210 net.cpp:141] Setting up ip_conv2
I0702 10:25:23.339753  7210 net.cpp:148] Top shape: 20 2 8 8 (2560)
I0702 10:25:23.339758  7210 net.cpp:156] Memory required for data: 88654400
I0702 10:25:23.339764  7210 layer_factory.hpp:77] Creating layer upsample
I0702 10:25:23.339774  7210 net.cpp:91] Creating Layer upsample
I0702 10:25:23.339778  7210 net.cpp:425] upsample <- ip_conv2
I0702 10:25:23.339786  7210 net.cpp:399] upsample -> upsample
I0702 10:25:23.341984  7210 net.cpp:141] Setting up upsample
I0702 10:25:23.341997  7210 net.cpp:148] Top shape: 20 2 254 254 (2580640)
I0702 10:25:23.342033  7210 net.cpp:156] Memory required for data: 98976960
I0702 10:25:23.342042  7210 layer_factory.hpp:77] Creating layer loss
I0702 10:25:23.342051  7210 net.cpp:91] Creating Layer loss
I0702 10:25:23.342056  7210 net.cpp:425] loss <- upsample
I0702 10:25:23.342061  7210 net.cpp:425] loss <- label
I0702 10:25:23.342067  7210 net.cpp:399] loss -> loss
I0702 10:25:23.342082  7210 layer_factory.hpp:77] Creating layer loss
I0702 10:25:23.346252  7210 net.cpp:141] Setting up loss
I0702 10:25:23.346287  7210 net.cpp:148] Top shape: (1)
I0702 10:25:23.346293  7210 net.cpp:151]     with loss weight 1
I0702 10:25:23.346313  7210 net.cpp:156] Memory required for data: 98976964
I0702 10:25:23.346319  7210 net.cpp:217] loss needs backward computation.
I0702 10:25:23.346326  7210 net.cpp:217] upsample needs backward computation.
I0702 10:25:23.346331  7210 net.cpp:217] ip_conv2 needs backward computation.
I0702 10:25:23.346335  7210 net.cpp:217] drop1 needs backward computation.
I0702 10:25:23.346340  7210 net.cpp:217] relu1 needs backward computation.
I0702 10:25:23.346344  7210 net.cpp:217] ip_conv1 needs backward computation.
I0702 10:25:23.346349  7210 net.cpp:217] pool2 needs backward computation.
I0702 10:25:23.346354  7210 net.cpp:217] conv2 needs backward computation.
I0702 10:25:23.346359  7210 net.cpp:217] pool1 needs backward computation.
I0702 10:25:23.346364  7210 net.cpp:217] conv1 needs backward computation.
I0702 10:25:23.346369  7210 net.cpp:219] training_cells does not need backward computation.
I0702 10:25:23.346374  7210 net.cpp:261] This network produces output loss
I0702 10:25:23.346385  7210 net.cpp:274] Network initialization done.
I0702 10:25:23.346743  7210 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_train_test.prototxt
I0702 10:25:23.346801  7210 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0702 10:25:23.346899  7210 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "ChunkingFishFovDataLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'seed\': 1337, \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_same_res/\', \'split\': \'test\', \'n_samples\': 5, \'chunker_params\': {\'chunk_size\': 254, \'window_size\': 1}}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip_conv1"
  type: "Convolution"
  bottom: "pool2"
  top: "ip_conv_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip_conv_1"
  top: "ip_conv_1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip_conv2"
  type: "Convolution"
  bottom: "ip_conv_1"
  top: "ip_conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "ip_conv2"
  top: "upsample"
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 2
    bias_term: false
    kernel_size: 149
    stride: 15
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "upsample"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "upsample"
  bottom: "label"
  top: "loss"
  loss_param {
    normalize: false
  }
}
I0702 10:25:23.347333  7210 layer_factory.hpp:77] Creating layer testing_cells
I0702 10:25:23.347388  7210 net.cpp:91] Creating Layer testing_cells
I0702 10:25:23.347407  7210 net.cpp:399] testing_cells -> image
I0702 10:25:23.347416  7210 net.cpp:399] testing_cells -> label
I0702 10:25:23.364709  7210 net.cpp:141] Setting up testing_cells
I0702 10:25:23.364759  7210 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0702 10:25:23.364766  7210 net.cpp:148] Top shape: 5 1 254 254 (322580)
I0702 10:25:23.364770  7210 net.cpp:156] Memory required for data: 3870960
I0702 10:25:23.364778  7210 layer_factory.hpp:77] Creating layer label_testing_cells_1_split
I0702 10:25:23.364795  7210 net.cpp:91] Creating Layer label_testing_cells_1_split
I0702 10:25:23.364802  7210 net.cpp:425] label_testing_cells_1_split <- label
I0702 10:25:23.364809  7210 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_0
I0702 10:25:23.364820  7210 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_1
I0702 10:25:23.364845  7210 net.cpp:141] Setting up label_testing_cells_1_split
I0702 10:25:23.364852  7210 net.cpp:148] Top shape: 5 1 254 254 (322580)
I0702 10:25:23.364857  7210 net.cpp:148] Top shape: 5 1 254 254 (322580)
I0702 10:25:23.364861  7210 net.cpp:156] Memory required for data: 6451600
I0702 10:25:23.364866  7210 layer_factory.hpp:77] Creating layer conv1
I0702 10:25:23.364878  7210 net.cpp:91] Creating Layer conv1
I0702 10:25:23.364882  7210 net.cpp:425] conv1 <- image
I0702 10:25:23.364888  7210 net.cpp:399] conv1 -> conv1
I0702 10:25:23.365123  7210 net.cpp:141] Setting up conv1
I0702 10:25:23.365133  7210 net.cpp:148] Top shape: 5 15 240 240 (4320000)
I0702 10:25:23.365137  7210 net.cpp:156] Memory required for data: 23731600
I0702 10:25:23.365147  7210 layer_factory.hpp:77] Creating layer pool1
I0702 10:25:23.365155  7210 net.cpp:91] Creating Layer pool1
I0702 10:25:23.365160  7210 net.cpp:425] pool1 <- conv1
I0702 10:25:23.365166  7210 net.cpp:399] pool1 -> pool1
I0702 10:25:23.365200  7210 net.cpp:141] Setting up pool1
I0702 10:25:23.365206  7210 net.cpp:148] Top shape: 5 15 48 48 (172800)
I0702 10:25:23.365211  7210 net.cpp:156] Memory required for data: 24422800
I0702 10:25:23.365216  7210 layer_factory.hpp:77] Creating layer conv2
I0702 10:25:23.365226  7210 net.cpp:91] Creating Layer conv2
I0702 10:25:23.365229  7210 net.cpp:425] conv2 <- pool1
I0702 10:25:23.365245  7210 net.cpp:399] conv2 -> conv2
I0702 10:25:23.365397  7210 net.cpp:141] Setting up conv2
I0702 10:25:23.365404  7210 net.cpp:148] Top shape: 5 5 42 42 (44100)
I0702 10:25:23.365409  7210 net.cpp:156] Memory required for data: 24599200
I0702 10:25:23.365417  7210 layer_factory.hpp:77] Creating layer pool2
I0702 10:25:23.365423  7210 net.cpp:91] Creating Layer pool2
I0702 10:25:23.365428  7210 net.cpp:425] pool2 <- conv2
I0702 10:25:23.365433  7210 net.cpp:399] pool2 -> pool2
I0702 10:25:23.365455  7210 net.cpp:141] Setting up pool2
I0702 10:25:23.365463  7210 net.cpp:148] Top shape: 5 5 14 14 (4900)
I0702 10:25:23.365466  7210 net.cpp:156] Memory required for data: 24618800
I0702 10:25:23.365470  7210 layer_factory.hpp:77] Creating layer ip_conv1
I0702 10:25:23.365478  7210 net.cpp:91] Creating Layer ip_conv1
I0702 10:25:23.365483  7210 net.cpp:425] ip_conv1 <- pool2
I0702 10:25:23.365535  7210 net.cpp:399] ip_conv1 -> ip_conv_1
I0702 10:25:23.365713  7210 net.cpp:141] Setting up ip_conv1
I0702 10:25:23.365721  7210 net.cpp:148] Top shape: 5 32 8 8 (10240)
I0702 10:25:23.365726  7210 net.cpp:156] Memory required for data: 24659760
I0702 10:25:23.365734  7210 layer_factory.hpp:77] Creating layer relu1
I0702 10:25:23.365741  7210 net.cpp:91] Creating Layer relu1
I0702 10:25:23.365746  7210 net.cpp:425] relu1 <- ip_conv_1
I0702 10:25:23.365751  7210 net.cpp:386] relu1 -> ip_conv_1 (in-place)
I0702 10:25:23.365757  7210 net.cpp:141] Setting up relu1
I0702 10:25:23.365762  7210 net.cpp:148] Top shape: 5 32 8 8 (10240)
I0702 10:25:23.365767  7210 net.cpp:156] Memory required for data: 24700720
I0702 10:25:23.365772  7210 layer_factory.hpp:77] Creating layer drop1
I0702 10:25:23.365778  7210 net.cpp:91] Creating Layer drop1
I0702 10:25:23.365782  7210 net.cpp:425] drop1 <- ip_conv_1
I0702 10:25:23.365787  7210 net.cpp:386] drop1 -> ip_conv_1 (in-place)
I0702 10:25:23.365803  7210 net.cpp:141] Setting up drop1
I0702 10:25:23.365809  7210 net.cpp:148] Top shape: 5 32 8 8 (10240)
I0702 10:25:23.365813  7210 net.cpp:156] Memory required for data: 24741680
I0702 10:25:23.365818  7210 layer_factory.hpp:77] Creating layer ip_conv2
I0702 10:25:23.365825  7210 net.cpp:91] Creating Layer ip_conv2
I0702 10:25:23.365830  7210 net.cpp:425] ip_conv2 <- ip_conv_1
I0702 10:25:23.365836  7210 net.cpp:399] ip_conv2 -> ip_conv2
I0702 10:25:23.365972  7210 net.cpp:141] Setting up ip_conv2
I0702 10:25:23.365979  7210 net.cpp:148] Top shape: 5 2 8 8 (640)
I0702 10:25:23.365983  7210 net.cpp:156] Memory required for data: 24744240
I0702 10:25:23.365990  7210 layer_factory.hpp:77] Creating layer upsample
I0702 10:25:23.365998  7210 net.cpp:91] Creating Layer upsample
I0702 10:25:23.366003  7210 net.cpp:425] upsample <- ip_conv2
I0702 10:25:23.366008  7210 net.cpp:399] upsample -> upsample
I0702 10:25:23.366184  7210 net.cpp:141] Setting up upsample
I0702 10:25:23.366191  7210 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0702 10:25:23.366196  7210 net.cpp:156] Memory required for data: 27324880
I0702 10:25:23.366204  7210 layer_factory.hpp:77] Creating layer upsample_upsample_0_split
I0702 10:25:23.366209  7210 net.cpp:91] Creating Layer upsample_upsample_0_split
I0702 10:25:23.366214  7210 net.cpp:425] upsample_upsample_0_split <- upsample
I0702 10:25:23.366219  7210 net.cpp:399] upsample_upsample_0_split -> upsample_upsample_0_split_0
I0702 10:25:23.366226  7210 net.cpp:399] upsample_upsample_0_split -> upsample_upsample_0_split_1
I0702 10:25:23.366246  7210 net.cpp:141] Setting up upsample_upsample_0_split
I0702 10:25:23.366253  7210 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0702 10:25:23.366257  7210 net.cpp:148] Top shape: 5 2 254 254 (645160)
I0702 10:25:23.366262  7210 net.cpp:156] Memory required for data: 32486160
I0702 10:25:23.366266  7210 layer_factory.hpp:77] Creating layer accuracy
I0702 10:25:23.366273  7210 net.cpp:91] Creating Layer accuracy
I0702 10:25:23.366277  7210 net.cpp:425] accuracy <- upsample_upsample_0_split_0
I0702 10:25:23.366282  7210 net.cpp:425] accuracy <- label_testing_cells_1_split_0
I0702 10:25:23.366288  7210 net.cpp:399] accuracy -> accuracy
I0702 10:25:23.366295  7210 net.cpp:141] Setting up accuracy
I0702 10:25:23.366302  7210 net.cpp:148] Top shape: (1)
I0702 10:25:23.366305  7210 net.cpp:156] Memory required for data: 32486164
I0702 10:25:23.366309  7210 layer_factory.hpp:77] Creating layer loss
I0702 10:25:23.366315  7210 net.cpp:91] Creating Layer loss
I0702 10:25:23.366319  7210 net.cpp:425] loss <- upsample_upsample_0_split_1
I0702 10:25:23.366324  7210 net.cpp:425] loss <- label_testing_cells_1_split_1
I0702 10:25:23.366330  7210 net.cpp:399] loss -> loss
I0702 10:25:23.366338  7210 layer_factory.hpp:77] Creating layer loss
I0702 10:25:23.367367  7210 net.cpp:141] Setting up loss
I0702 10:25:23.367382  7210 net.cpp:148] Top shape: (1)
I0702 10:25:23.367386  7210 net.cpp:151]     with loss weight 1
I0702 10:25:23.367396  7210 net.cpp:156] Memory required for data: 32486168
I0702 10:25:23.367429  7210 net.cpp:217] loss needs backward computation.
I0702 10:25:23.367434  7210 net.cpp:219] accuracy does not need backward computation.
I0702 10:25:23.367440  7210 net.cpp:217] upsample_upsample_0_split needs backward computation.
I0702 10:25:23.367444  7210 net.cpp:217] upsample needs backward computation.
I0702 10:25:23.367449  7210 net.cpp:217] ip_conv2 needs backward computation.
I0702 10:25:23.367454  7210 net.cpp:217] drop1 needs backward computation.
I0702 10:25:23.367458  7210 net.cpp:217] relu1 needs backward computation.
I0702 10:25:23.367462  7210 net.cpp:217] ip_conv1 needs backward computation.
I0702 10:25:23.367466  7210 net.cpp:217] pool2 needs backward computation.
I0702 10:25:23.367470  7210 net.cpp:217] conv2 needs backward computation.
I0702 10:25:23.367475  7210 net.cpp:217] pool1 needs backward computation.
I0702 10:25:23.367480  7210 net.cpp:217] conv1 needs backward computation.
I0702 10:25:23.367485  7210 net.cpp:219] label_testing_cells_1_split does not need backward computation.
I0702 10:25:23.367489  7210 net.cpp:219] testing_cells does not need backward computation.
I0702 10:25:23.367493  7210 net.cpp:261] This network produces output accuracy
I0702 10:25:23.367498  7210 net.cpp:261] This network produces output loss
I0702 10:25:23.367509  7210 net.cpp:274] Network initialization done.
I0702 10:25:23.367568  7210 solver.cpp:60] Solver scaffolding done.
I0702 10:25:23.367760  7210 caffe.cpp:219] Starting Optimization
I0702 10:25:23.367766  7210 solver.cpp:279] Solving fish_filter
I0702 10:25:23.367770  7210 solver.cpp:280] Learning Rate Policy: fixed
I0702 10:25:23.370126  7210 solver.cpp:337] Iteration 0, Testing net (#0)
I0702 10:25:23.370138  7210 net.cpp:684] Ignoring source layer training_cells
I0702 10:25:28.660322  7210 solver.cpp:404]     Test net output #0: accuracy = 0.0255369
I0702 10:25:28.660362  7210 solver.cpp:404]     Test net output #1: loss = 44719.1 (* 1 = 44719.1 loss)
I0702 10:25:28.860709  7210 solver.cpp:228] Iteration 0, loss = 44719
I0702 10:25:28.860762  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:25:28.860772  7210 sgd_solver.cpp:106] Iteration 0, lr = 1e-07
I0702 10:25:45.798203  7210 solver.cpp:228] Iteration 20, loss = 44719.1
I0702 10:25:45.798239  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:25:45.798246  7210 sgd_solver.cpp:106] Iteration 20, lr = 1e-07
I0702 10:26:02.198631  7210 solver.cpp:228] Iteration 40, loss = 44719.1
I0702 10:26:02.198724  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:26:02.198734  7210 sgd_solver.cpp:106] Iteration 40, lr = 1e-07
I0702 10:26:18.955880  7210 solver.cpp:228] Iteration 60, loss = 44719.1
I0702 10:26:18.955917  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:26:18.955925  7210 sgd_solver.cpp:106] Iteration 60, lr = 1e-07
I0702 10:26:35.341716  7210 solver.cpp:228] Iteration 80, loss = 44719.1
I0702 10:26:35.341799  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:26:35.341809  7210 sgd_solver.cpp:106] Iteration 80, lr = 1e-07
I0702 10:26:51.620647  7210 solver.cpp:228] Iteration 100, loss = 44719.1
I0702 10:26:51.620683  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:26:51.620692  7210 sgd_solver.cpp:106] Iteration 100, lr = 1e-07
I0702 10:27:07.917999  7210 solver.cpp:228] Iteration 120, loss = 44719.1
I0702 10:27:07.918118  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:27:07.918128  7210 sgd_solver.cpp:106] Iteration 120, lr = 1e-07
I0702 10:27:24.073730  7210 solver.cpp:228] Iteration 140, loss = 44719.1
I0702 10:27:24.073770  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:27:24.073786  7210 sgd_solver.cpp:106] Iteration 140, lr = 1e-07
I0702 10:27:40.104630  7210 solver.cpp:228] Iteration 160, loss = 44719.1
I0702 10:27:40.104733  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:27:40.104744  7210 sgd_solver.cpp:106] Iteration 160, lr = 1e-07
I0702 10:27:56.274340  7210 solver.cpp:228] Iteration 180, loss = 44719.1
I0702 10:27:56.274379  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:27:56.274386  7210 sgd_solver.cpp:106] Iteration 180, lr = 1e-07
I0702 10:28:12.381042  7210 solver.cpp:228] Iteration 200, loss = 44719.1
I0702 10:28:12.381141  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:28:12.381151  7210 sgd_solver.cpp:106] Iteration 200, lr = 1e-07
I0702 10:28:28.057291  7210 solver.cpp:228] Iteration 220, loss = 44719.1
I0702 10:28:28.057327  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:28:28.057335  7210 sgd_solver.cpp:106] Iteration 220, lr = 1e-07
I0702 10:28:43.791344  7210 solver.cpp:228] Iteration 240, loss = 44719.1
I0702 10:28:43.791431  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:28:43.791440  7210 sgd_solver.cpp:106] Iteration 240, lr = 1e-07
I0702 10:28:59.388057  7210 solver.cpp:228] Iteration 260, loss = 44719.1
I0702 10:28:59.388092  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:28:59.388100  7210 sgd_solver.cpp:106] Iteration 260, lr = 1e-07
I0702 10:29:14.896051  7210 solver.cpp:228] Iteration 280, loss = 44719.1
I0702 10:29:14.896152  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:29:14.896162  7210 sgd_solver.cpp:106] Iteration 280, lr = 1e-07
I0702 10:29:30.454733  7210 solver.cpp:228] Iteration 300, loss = 44719.1
I0702 10:29:30.454768  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:29:30.454777  7210 sgd_solver.cpp:106] Iteration 300, lr = 1e-07
I0702 10:29:45.931857  7210 solver.cpp:228] Iteration 320, loss = 44719.1
I0702 10:29:45.931949  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:29:45.931958  7210 sgd_solver.cpp:106] Iteration 320, lr = 1e-07
I0702 10:30:01.455039  7210 solver.cpp:228] Iteration 340, loss = 44719.1
I0702 10:30:01.455073  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:30:01.455081  7210 sgd_solver.cpp:106] Iteration 340, lr = 1e-07
I0702 10:30:16.917436  7210 solver.cpp:228] Iteration 360, loss = 44719.1
I0702 10:30:16.917524  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:30:16.917534  7210 sgd_solver.cpp:106] Iteration 360, lr = 1e-07
I0702 10:30:32.498281  7210 solver.cpp:228] Iteration 380, loss = 44719.1
I0702 10:30:32.498318  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:30:32.498327  7210 sgd_solver.cpp:106] Iteration 380, lr = 1e-07
I0702 10:30:47.931491  7210 solver.cpp:228] Iteration 400, loss = 44719.1
I0702 10:30:47.931586  7210 solver.cpp:244]     Train net output #0: loss = 44719 (* 1 = 44719 loss)
I0702 10:30:47.931596  7210 sgd_solver.cpp:106] Iteration 400, lr = 1e-07
I0702 10:30:51.064476  7210 solver.cpp:454] Snapshotting to binary proto file fish_net_output_iter_405.caffemodel
I0702 10:30:51.666496  7210 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_output_iter_405.solverstate
I0702 10:30:51.667779  7210 solver.cpp:301] Optimization stopped early.
I0702 10:30:51.667790  7210 caffe.cpp:222] Optimization Done.
