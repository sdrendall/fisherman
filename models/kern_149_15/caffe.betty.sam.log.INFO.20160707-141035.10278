Log file created at: 2016/07/07 14:10:35
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0707 14:10:35.078656 10278 caffe.cpp:185] Using GPUs 0
I0707 14:10:35.133040 10278 caffe.cpp:190] GPU 0: GeForce GTX 760
I0707 14:10:35.250360 10278 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 1000
base_lr: 0.001
display: 25
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.4
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "fish_net_memory_map_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt"
I0707 14:10:35.250569 10278 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0707 14:10:35.250908 10278 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0707 14:10:35.250938 10278 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0707 14:10:35.251008 10278 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'train\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0707 14:10:35.251328 10278 layer_factory.hpp:77] Creating layer training_cells
I0707 14:10:36.086036 10278 net.cpp:91] Creating Layer training_cells
I0707 14:10:36.086071 10278 net.cpp:399] training_cells -> image
I0707 14:10:36.086093 10278 net.cpp:399] training_cells -> label
I0707 14:12:18.736140 10278 net.cpp:141] Setting up training_cells
I0707 14:12:18.736224 10278 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0707 14:12:18.736232 10278 net.cpp:148] Top shape: 512 (512)
I0707 14:12:18.736238 10278 net.cpp:156] Memory required for data: 90937344
I0707 14:12:18.736248 10278 layer_factory.hpp:77] Creating layer conv1
I0707 14:12:18.736271 10278 net.cpp:91] Creating Layer conv1
I0707 14:12:18.736279 10278 net.cpp:425] conv1 <- image
I0707 14:12:18.736291 10278 net.cpp:399] conv1 -> conv1
I0707 14:12:18.737479 10278 net.cpp:141] Setting up conv1
I0707 14:12:18.737504 10278 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0707 14:12:18.737509 10278 net.cpp:156] Memory required for data: 650809344
I0707 14:12:18.737524 10278 layer_factory.hpp:77] Creating layer pool1
I0707 14:12:18.737534 10278 net.cpp:91] Creating Layer pool1
I0707 14:12:18.737540 10278 net.cpp:425] pool1 <- conv1
I0707 14:12:18.737545 10278 net.cpp:399] pool1 -> pool1
I0707 14:12:18.737582 10278 net.cpp:141] Setting up pool1
I0707 14:12:18.737591 10278 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0707 14:12:18.737594 10278 net.cpp:156] Memory required for data: 673204224
I0707 14:12:18.737599 10278 layer_factory.hpp:77] Creating layer conv2
I0707 14:12:18.737608 10278 net.cpp:91] Creating Layer conv2
I0707 14:12:18.737614 10278 net.cpp:425] conv2 <- pool1
I0707 14:12:18.737620 10278 net.cpp:399] conv2 -> conv2
I0707 14:12:18.740258 10278 net.cpp:141] Setting up conv2
I0707 14:12:18.740283 10278 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0707 14:12:18.740288 10278 net.cpp:156] Memory required for data: 677720064
I0707 14:12:18.740298 10278 layer_factory.hpp:77] Creating layer pool2
I0707 14:12:18.740306 10278 net.cpp:91] Creating Layer pool2
I0707 14:12:18.740311 10278 net.cpp:425] pool2 <- conv2
I0707 14:12:18.740319 10278 net.cpp:399] pool2 -> pool2
I0707 14:12:18.740345 10278 net.cpp:141] Setting up pool2
I0707 14:12:18.740352 10278 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0707 14:12:18.740357 10278 net.cpp:156] Memory required for data: 678221824
I0707 14:12:18.740362 10278 layer_factory.hpp:77] Creating layer ip1
I0707 14:12:18.740370 10278 net.cpp:91] Creating Layer ip1
I0707 14:12:18.740375 10278 net.cpp:425] ip1 <- pool2
I0707 14:12:18.740381 10278 net.cpp:399] ip1 -> ip1
I0707 14:12:18.740492 10278 net.cpp:141] Setting up ip1
I0707 14:12:18.740499 10278 net.cpp:148] Top shape: 512 32 (16384)
I0707 14:12:18.740504 10278 net.cpp:156] Memory required for data: 678287360
I0707 14:12:18.740512 10278 layer_factory.hpp:77] Creating layer relu1
I0707 14:12:18.740520 10278 net.cpp:91] Creating Layer relu1
I0707 14:12:18.740525 10278 net.cpp:425] relu1 <- ip1
I0707 14:12:18.740530 10278 net.cpp:386] relu1 -> ip1 (in-place)
I0707 14:12:18.740537 10278 net.cpp:141] Setting up relu1
I0707 14:12:18.740543 10278 net.cpp:148] Top shape: 512 32 (16384)
I0707 14:12:18.740547 10278 net.cpp:156] Memory required for data: 678352896
I0707 14:12:18.740552 10278 layer_factory.hpp:77] Creating layer drop1
I0707 14:12:18.740561 10278 net.cpp:91] Creating Layer drop1
I0707 14:12:18.740564 10278 net.cpp:425] drop1 <- ip1
I0707 14:12:18.740571 10278 net.cpp:386] drop1 -> ip1 (in-place)
I0707 14:12:18.740589 10278 net.cpp:141] Setting up drop1
I0707 14:12:18.740595 10278 net.cpp:148] Top shape: 512 32 (16384)
I0707 14:12:18.740600 10278 net.cpp:156] Memory required for data: 678418432
I0707 14:12:18.740605 10278 layer_factory.hpp:77] Creating layer ip2
I0707 14:12:18.740613 10278 net.cpp:91] Creating Layer ip2
I0707 14:12:18.740617 10278 net.cpp:425] ip2 <- ip1
I0707 14:12:18.740624 10278 net.cpp:399] ip2 -> ip2
I0707 14:12:18.740679 10278 net.cpp:141] Setting up ip2
I0707 14:12:18.740685 10278 net.cpp:148] Top shape: 512 2 (1024)
I0707 14:12:18.740689 10278 net.cpp:156] Memory required for data: 678422528
I0707 14:12:18.740696 10278 layer_factory.hpp:77] Creating layer loss
I0707 14:12:18.740703 10278 net.cpp:91] Creating Layer loss
I0707 14:12:18.740708 10278 net.cpp:425] loss <- ip2
I0707 14:12:18.740715 10278 net.cpp:425] loss <- label
I0707 14:12:18.740720 10278 net.cpp:399] loss -> loss
I0707 14:12:18.740732 10278 layer_factory.hpp:77] Creating layer loss
I0707 14:12:18.740816 10278 net.cpp:141] Setting up loss
I0707 14:12:18.740824 10278 net.cpp:148] Top shape: (1)
I0707 14:12:18.740829 10278 net.cpp:151]     with loss weight 1
I0707 14:12:18.740844 10278 net.cpp:156] Memory required for data: 678422532
I0707 14:12:18.740849 10278 net.cpp:217] loss needs backward computation.
I0707 14:12:18.740855 10278 net.cpp:217] ip2 needs backward computation.
I0707 14:12:18.740859 10278 net.cpp:217] drop1 needs backward computation.
I0707 14:12:18.740864 10278 net.cpp:217] relu1 needs backward computation.
I0707 14:12:18.740869 10278 net.cpp:217] ip1 needs backward computation.
I0707 14:12:18.740874 10278 net.cpp:217] pool2 needs backward computation.
I0707 14:12:18.740878 10278 net.cpp:217] conv2 needs backward computation.
I0707 14:12:18.740883 10278 net.cpp:217] pool1 needs backward computation.
I0707 14:12:18.740887 10278 net.cpp:217] conv1 needs backward computation.
I0707 14:12:18.740892 10278 net.cpp:219] training_cells does not need backward computation.
I0707 14:12:18.740897 10278 net.cpp:261] This network produces output loss
I0707 14:12:18.740907 10278 net.cpp:274] Network initialization done.
I0707 14:12:18.741214 10278 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0707 14:12:18.741238 10278 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0707 14:12:18.741250 10278 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer drop1
I0707 14:12:18.741317 10278 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'test\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0707 14:12:18.741614 10278 layer_factory.hpp:77] Creating layer testing_cells
I0707 14:12:18.741659 10278 net.cpp:91] Creating Layer testing_cells
I0707 14:12