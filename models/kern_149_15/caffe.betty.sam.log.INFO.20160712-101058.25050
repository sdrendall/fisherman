Log file created at: 2016/07/12 10:10:58
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0712 10:10:58.917995 25050 caffe.cpp:185] Using GPUs 0
I0712 10:10:58.995093 25050 caffe.cpp:190] GPU 0: GeForce GTX 760
I0712 10:10:59.142527 25050 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 1000
base_lr: 0.001
display: 25
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.4
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "fish_net_memory_map_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt"
I0712 10:10:59.143291 25050 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0712 10:10:59.145184 25050 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0712 10:10:59.145215 25050 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0712 10:10:59.145422 25050 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'train\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0712 10:10:59.146280 25050 layer_factory.hpp:77] Creating layer training_cells
I0712 10:11:00.808683 25050 net.cpp:91] Creating Layer training_cells
I0712 10:11:00.808871 25050 net.cpp:399] training_cells -> image
I0712 10:11:00.809206 25050 net.cpp:399] training_cells -> label
I0712 10:12:39.781968 25050 net.cpp:141] Setting up training_cells
I0712 10:12:39.782176 25050 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0712 10:12:39.782186 25050 net.cpp:148] Top shape: 512 (512)
I0712 10:12:39.782191 25050 net.cpp:156] Memory required for data: 90937344
I0712 10:12:39.782202 25050 layer_factory.hpp:77] Creating layer conv1
I0712 10:12:39.782232 25050 net.cpp:91] Creating Layer conv1
I0712 10:12:39.782243 25050 net.cpp:425] conv1 <- image
I0712 10:12:39.782256 25050 net.cpp:399] conv1 -> conv1
I0712 10:12:39.785605 25050 net.cpp:141] Setting up conv1
I0712 10:12:39.785619 25050 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0712 10:12:39.785624 25050 net.cpp:156] Memory required for data: 650809344
I0712 10:12:39.785642 25050 layer_factory.hpp:77] Creating layer pool1
I0712 10:12:39.785652 25050 net.cpp:91] Creating Layer pool1
I0712 10:12:39.785657 25050 net.cpp:425] pool1 <- conv1
I0712 10:12:39.785663 25050 net.cpp:399] pool1 -> pool1
I0712 10:12:39.785904 25050 net.cpp:141] Setting up pool1
I0712 10:12:39.785914 25050 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0712 10:12:39.785919 25050 net.cpp:156] Memory required for data: 673204224
I0712 10:12:39.785923 25050 layer_factory.hpp:77] Creating layer conv2
I0712 10:12:39.785933 25050 net.cpp:91] Creating Layer conv2
I0712 10:12:39.785938 25050 net.cpp:425] conv2 <- pool1
I0712 10:12:39.785943 25050 net.cpp:399] conv2 -> conv2
I0712 10:12:39.786595 25050 net.cpp:141] Setting up conv2
I0712 10:12:39.786608 25050 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0712 10:12:39.786613 25050 net.cpp:156] Memory required for data: 677720064
I0712 10:12:39.786622 25050 layer_factory.hpp:77] Creating layer pool2
I0712 10:12:39.786629 25050 net.cpp:91] Creating Layer pool2
I0712 10:12:39.786634 25050 net.cpp:425] pool2 <- conv2
I0712 10:12:39.786639 25050 net.cpp:399] pool2 -> pool2
I0712 10:12:39.786665 25050 net.cpp:141] Setting up pool2
I0712 10:12:39.786672 25050 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0712 10:12:39.786676 25050 net.cpp:156] Memory required for data: 678221824
I0712 10:12:39.786680 25050 layer_factory.hpp:77] Creating layer ip1
I0712 10:12:39.786798 25050 net.cpp:91] Creating Layer ip1
I0712 10:12:39.786806 25050 net.cpp:425] ip1 <- pool2
I0712 10:12:39.786813 25050 net.cpp:399] ip1 -> ip1
I0712 10:12:39.787029 25050 net.cpp:141] Setting up ip1
I0712 10:12:39.787039 25050 net.cpp:148] Top shape: 512 32 (16384)
I0712 10:12:39.787042 25050 net.cpp:156] Memory required for data: 678287360
I0712 10:12:39.787050 25050 layer_factory.hpp:77] Creating layer relu1
I0712 10:12:39.787057 25050 net.cpp:91] Creating Layer relu1
I0712 10:12:39.787062 25050 net.cpp:425] relu1 <- ip1
I0712 10:12:39.787067 25050 net.cpp:386] relu1 -> ip1 (in-place)
I0712 10:12:39.787180 25050 net.cpp:141] Setting up relu1
I0712 10:12:39.787189 25050 net.cpp:148] Top shape: 512 32 (16384)
I0712 10:12:39.787194 25050 net.cpp:156] Memory required for data: 678352896
I0712 10:12:39.787197 25050 layer_factory.hpp:77] Creating layer drop1
I0712 10:12:39.787310 25050 net.cpp:91] Creating Layer drop1
I0712 10:12:39.787317 25050 net.cpp:425] drop1 <- ip1
I0712 10:12:39.787323 25050 net.cpp:386] drop1 -> ip1 (in-place)
I0712 10:12:39.787447 25050 net.cpp:141] Setting up drop1
I0712 10:12:39.787456 25050 net.cpp:148] Top shape: 512 32 (16384)
I0712 10:12:39.787461 25050 net.cpp:156] Memory required for data: 678418432
I0712 10:12:39.787464 25050 layer_factory.hpp:77] Creating layer ip2
I0712 10:12:39.787472 25050 net.cpp:91] Creating Layer ip2
I0712 10:12:39.787477 25050 net.cpp:425] ip2 <- ip1
I0712 10:12:39.787483 25050 net.cpp:399] ip2 -> ip2
I0712 10:12:39.787539 25050 net.cpp:141] Setting up ip2
I0712 10:12:39.787545 25050 net.cpp:148] Top shape: 512 2 (1024)
I0712 10:12:39.787550 25050 net.cpp:156] Memory required for data: 678422528
I0712 10:12:39.787556 25050 layer_factory.hpp:77] Creating layer loss
I0712 10:12:39.787689 25050 net.cpp:91] Creating Layer loss
I0712 10:12:39.787696 25050 net.cpp:425] loss <- ip2
I0712 10:12:39.787701 25050 net.cpp:425] loss <- label
I0712 10:12:39.787708 25050 net.cpp:399] loss -> loss
I0712 10:12:39.788172 25050 layer_factory.hpp:77] Creating layer loss
I0712 10:12:39.788363 25050 net.cpp:141] Setting up loss
I0712 10:12:39.788373 25050 net.cpp:148] Top shape: (1)
I0712 10:12:39.788378 25050 net.cpp:151]     with loss weight 1
I0712 10:12:39.788391 25050 net.cpp:156] Memory required for data: 678422532
I0712 10:12:39.788395 25050 net.cpp:217] loss needs backward computation.
I0712 10:12:39.788400 25050 net.cpp:217] ip2 needs backward computation.
I0712 10:12:39.788406 25050 net.cpp:217] drop1 needs backward computation.
I0712 10:12:39.788411 25050 net.cpp:217] relu1 needs backward computation.
I0712 10:12:39.788414 25050 net.cpp:217] ip1 needs backward computation.
I0712 10:12:39.788419 25050 net.cpp:217] pool2 needs backward computation.
I0712 10:12:39.788424 25050 net.cpp:217] conv2 needs backward computation.
I0712 10:12:39.788427 25050 net.cpp:217] pool1 needs backward computation.
I0712 10:12:39.788431 25050 net.cpp:217] conv1 needs backward computation.
I0712 10:12:39.788436 25050 net.cpp:219] training_cells does not need backward computation.
I0712 10:12:39.788440 25050 net.cpp:261] This network produces output loss
I0712 10:12:39.788450 25050 net.cpp:274] Network initialization done.
F0712 10:12:39.788514 25050 io.cpp:36] Check failed: fd != -1 (-1 vs. -1) File not found: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
