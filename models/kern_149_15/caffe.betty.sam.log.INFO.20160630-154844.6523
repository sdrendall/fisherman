Log file created at: 2016/06/30 15:48:44
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0630 15:48:44.132036  6523 caffe.cpp:178] Use CPU.
I0630 15:48:44.315467  6523 solver.cpp:48] Initializing solver from parameters: 
test_iter: 10
test_interval: 500
base_lr: 0.001
display: 25
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.4
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "fish_net_pretrain_output"
solver_mode: CPU
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_pretrainer.prototxt"
I0630 15:48:44.315681  6523 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_pretrainer.prototxt
I0630 15:48:44.316046  6523 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0630 15:48:44.316076  6523 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0630 15:48:44.316144  6523 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Data"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/home/sam/code/fisherman/data/original_and_partial_qc_2_channel_k149_scaled/training_db"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0630 15:48:44.316452  6523 layer_factory.hpp:77] Creating layer training_cells
I0630 15:48:44.317504  6523 net.cpp:91] Creating Layer training_cells
I0630 15:48:44.317528  6523 net.cpp:399] training_cells -> image
I0630 15:48:44.317550  6523 net.cpp:399] training_cells -> label
I0630 15:48:44.318163  6535 db_lmdb.cpp:35] Opened lmdb /home/sam/code/fisherman/data/original_and_partial_qc_2_channel_k149_scaled/training_db
I0630 15:48:44.318455  6523 data_layer.cpp:41] output data size: 256,2,149,149
I0630 15:48:44.334108  6523 net.cpp:141] Setting up training_cells
I0630 15:48:44.334147  6523 net.cpp:148] Top shape: 256 2 149 149 (11366912)
I0630 15:48:44.334153  6523 net.cpp:148] Top shape: 256 (256)
I0630 15:48:44.334158  6523 net.cpp:156] Memory required for data: 45468672
I0630 15:48:44.334170  6523 layer_factory.hpp:77] Creating layer conv1
I0630 15:48:44.334197  6523 net.cpp:91] Creating Layer conv1
I0630 15:48:44.334203  6523 net.cpp:425] conv1 <- image
I0630 15:48:44.334218  6523 net.cpp:399] conv1 -> conv1
I0630 15:48:44.334331  6523 net.cpp:141] Setting up conv1
I0630 15:48:44.334339  6523 net.cpp:148] Top shape: 256 15 135 135 (69984000)
I0630 15:48:44.334393  6523 net.cpp:156] Memory required for data: 325404672
I0630 15:48:44.334408  6523 layer_factory.hpp:77] Creating layer pool1
I0630 15:48:44.334419  6523 net.cpp:91] Creating Layer pool1
I0630 15:48:44.334424  6523 net.cpp:425] pool1 <- conv1
I0630 15:48:44.334430  6523 net.cpp:399] pool1 -> pool1
I0630 15:48:44.334447  6523 net.cpp:141] Setting up pool1
I0630 15:48:44.334455  6523 net.cpp:148] Top shape: 256 15 27 27 (2799360)
I0630 15:48:44.334458  6523 net.cpp:156] Memory required for data: 336602112
I0630 15:48:44.334472  6523 layer_factory.hpp:77] Creating layer conv2
I0630 15:48:44.334481  6523 net.cpp:91] Creating Layer conv2
I0630 15:48:44.334486  6523 net.cpp:425] conv2 <- pool1
I0630 15:48:44.334494  6523 net.cpp:399] conv2 -> conv2
I0630 15:48:44.334544  6523 net.cpp:141] Setting up conv2
I0630 15:48:44.334552  6523 net.cpp:148] Top shape: 256 5 21 21 (564480)
I0630 15:48:44.334555  6523 net.cpp:156] Memory required for data: 338860032
I0630 15:48:44.334563  6523 layer_factory.hpp:77] Creating layer pool2
I0630 15:48:44.334570  6523 net.cpp:91] Creating Layer pool2
I0630 15:48:44.334575  6523 net.cpp:425] pool2 <- conv2
I0630 15:48:44.334580  6523 net.cpp:399] pool2 -> pool2
I0630 15:48:44.334589  6523 net.cpp:141] Setting up pool2
I0630 15:48:44.334594  6523 net.cpp:148] Top shape: 256 5 7 7 (62720)
I0630 15:48:44.334597  6523 net.cpp:156] Memory required for data: 339110912
I0630 15:48:44.334601  6523 layer_factory.hpp:77] Creating layer ip1
I0630 15:48:44.334609  6523 net.cpp:91] Creating Layer ip1
I0630 15:48:44.334614  6523 net.cpp:425] ip1 <- pool2
I0630 15:48:44.334620  6523 net.cpp:399] ip1 -> ip1
I0630 15:48:44.334677  6523 net.cpp:141] Setting up ip1
I0630 15:48:44.334684  6523 net.cpp:148] Top shape: 256 32 (8192)
I0630 15:48:44.334688  6523 net.cpp:156] Memory required for data: 339143680
I0630 15:48:44.334697  6523 layer_factory.hpp:77] Creating layer relu1
I0630 15:48:44.334702  6523 net.cpp:91] Creating Layer relu1
I0630 15:48:44.334707  6523 net.cpp:425] relu1 <- ip1
I0630 15:48:44.334712  6523 net.cpp:386] relu1 -> ip1 (in-place)
I0630 15:48:44.334718  6523 net.cpp:141] Setting up relu1
I0630 15:48:44.334723  6523 net.cpp:148] Top shape: 256 32 (8192)
I0630 15:48:44.334728  6523 net.cpp:156] Memory required for data: 339176448
I0630 15:48:44.334731  6523 layer_factory.hpp:77] Creating layer drop1
I0630 15:48:44.334743  6523 net.cpp:91] Creating Layer drop1
I0630 15:48:44.334748  6523 net.cpp:425] drop1 <- ip1
I0630 15:48:44.334753  6523 net.cpp:386] drop1 -> ip1 (in-place)
I0630 15:48:44.334761  6523 net.cpp:141] Setting up drop1
I0630 15:48:44.334766  6523 net.cpp:148] Top shape: 256 32 (8192)
I0630 15:48:44.334770  6523 net.cpp:156] Memory required for data: 339209216
I0630 15:48:44.334775  6523 layer_factory.hpp:77] Creating layer ip2
I0630 15:48:44.334784  6523 net.cpp:91] Creating Layer ip2
I0630 15:48:44.334789  6523 net.cpp:425] ip2 <- ip1
I0630 15:48:44.334794  6523 net.cpp:399] ip2 -> ip2
I0630 15:48:44.334805  6523 net.cpp:141] Setting up ip2
I0630 15:48:44.334811  6523 net.cpp:148] Top shape: 256 2 (512)
I0630 15:48:44.334815  6523 net.cpp:156] Memory required for data: 339211264
I0630 15:48:44.334820  6523 layer_factory.hpp:77] Creating layer loss
I0630 15:48:44.334827  6523 net.cpp:91] Creating Layer loss
I0630 15:48:44.334831  6523 net.cpp:425] loss <- ip2
I0630 15:48:44.334836  6523 net.cpp:425] loss <- label
I0630 15:48:44.334841  6523 net.cpp:399] loss -> loss
I0630 15:48:44.334852  6523 layer_factory.hpp:77] Creating layer loss
I0630 15:48:44.334869  6523 net.cpp:141] Setting up loss
I0630 15:48:44.334877  6523 net.cpp:148] Top shape: (1)
I0630 15:48:44.334882  6523 net.cpp:151]     with loss weight 1
I0630 15:48:44.334898  6523 net.cpp:156] Memory required for data: 339211268
I0630 15:48:44.334903  6523 net.cpp:217] loss needs backward computation.
I0630 15:48:44.334908  6523 net.cpp:217] ip2 needs backward computation.
I0630 15:48:44.334913  6523 net.cpp:217] drop1 needs backward computation.
I0630 15:48:44.334939  6523 net.cpp:217] relu1 needs backward computation.
I0630 15:48:44.334942  6523 net.cpp:217] ip1 needs backward computation.
I0630 15:48:44.334946  6523 net.cpp:217] pool2 needs backward computation.
I0630 15:48:44.334951  6523 net.cpp:217] conv2 needs backward computation.
I0630 15:48:44.334955  6523 net.cpp:217] pool1 needs backward computation.
I0630 15:48:44.334959  6523 net.cpp:217] conv1 needs backward computation.
I0630 15:48:44.334964  6523 net.cpp:219] training_cells does not need backward computation.
I0630 15:48:44.334969  6523 net.cpp:261] This network produces output loss
I0630 15:48:44.334978  6523 net.cpp:274] Network initialization done.
I0630 15:48:44.335288  6523 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_pretrainer.prototxt
I0630 15:48:44.335314  6523 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0630 15:48:44.335324  6523 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer drop1
I0630 15:48:44.335397  6523 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Data"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "/home/sam/code/fisherman/data/original_and_partial_qc_2_channel_k149_scaled/testing_db"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0630 15:48:44.335753  6523 layer_factory.hpp:77] Creating layer testing_cells
I0630 15:48:44.335938  6523 net.cpp:91] Creating Layer testing_cells
I0630 15:48:44.335960  6523 net.cpp:399] testing_cells -> image
I0630 15:48:44.335971  6523 net.cpp:399] testing_cells -> label
I0630 15:48:44.336617  6537 db_lmdb.cpp:35] Opened lmdb /home/sam/code/fisherman/data/original_and_partial_qc_2_channel_k149_scaled/testing_db
I0630 15:48:44.336962  6523 data_layer.cpp:41] output data size: 256,2,149,149
I0630 15:48:44.353520  6536 blocking_queue.cpp:50] Waiting for data
I0630 15:48:44.359522  6523 net.cpp:141] Setting up testing_cells
I0630 15:48:44.359565  6523 net.cpp:148] Top shape: 256 2 149 149 (11366912)
I0630 15:48:44.359572  6523 net.cpp:148] Top shape: 256 (256)
I0630 15:48:44.359577  6523 net.cpp:156] Memory required for data: 45468672
I0630 15:48:44.359650  6523 layer_factory.hpp:77] Creating layer label_testing_cells_1_split
I0630 15:48:44.359675  6523 net.cpp:91] Creating Layer label_testing_cells_1_split
I0630 15:48:44.359683  6523 net.cpp:425] label_testing_cells_1_split <- label
I0630 15:48:44.359694  6523 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_0
I0630 15:48:44.359706  6523 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_1
I0630 15:48:44.359720  6523 net.cpp:141] Setting up label_testing_cells_1_split
I0630 15:48:44.359727  6523 net.cpp:148] Top shape: 256 (256)
I0630 15:48:44.359732  6523 net.cpp:148] Top shape: 256 (256)
I0630 15:48:44.359736  6523 net.cpp:156] Memory required for data: 45470720
I0630 15:48:44.359741  6523 layer_factory.hpp:77] Creating layer conv1
I0630 15:48:44.359753  6523 net.cpp:91] Creating Layer conv1
I0630 15:48:44.359758  6523 net.cpp:425] conv1 <- image
I0630 15:48:44.359771  6523 net.cpp:399] conv1 -> conv1
I0630 15:48:44.359866  6523 net.cpp:141] Setting up conv1
I0630 15:48:44.359875  6523 net.cpp:148] Top shape: 256 15 135 135 (69984000)
I0630 15:48:44.359879  6523 net.cpp:156] Memory required for data: 325406720
I0630 15:48:44.359890  6523 layer_factory.hpp:77] Creating layer pool1
I0630 15:48:44.359910  6523 net.cpp:91] Creating Layer pool1
I0630 15:48:44.359916  6523 net.cpp:425] pool1 <- conv1
I0630 15:48:44.359923  6523 net.cpp:399] pool1 -> pool1
I0630 15:48:44.359940  6523 net.cpp:141] Setting up pool1
I0630 15:48:44.359949  6523 net.cpp:148] Top shape: 256 15 27 27 (2799360)
I0630 15:48:44.359956  6523 net.cpp:156] Memory required for data: 336604160
I0630 15:48:44.359963  6523 layer_factory.hpp:77] Creating layer conv2
I0630 15:48:44.359978  6523 net.cpp:91] Creating Layer conv2
I0630 15:48:44.359987  6523 net.cpp:425] conv2 <- pool1
I0630 15:48:44.359997  6523 net.cpp:399] conv2 -> conv2
I0630 15:48:44.360055  6523 net.cpp:141] Setting up conv2
I0630 15:48:44.360066  6523 net.cpp:148] Top shape: 256 5 21 21 (564480)
I0630 15:48:44.360074  6523 net.cpp:156] Memory required for data: 338862080
I0630 15:48:44.360085  6523 layer_factory.hpp:77] Creating layer pool2
I0630 15:48:44.360095  6523 net.cpp:91] Creating Layer pool2
I0630 15:48:44.360100  6523 net.cpp:425] pool2 <- conv2
I0630 15:48:44.360105  6523 net.cpp:399] pool2 -> pool2
I0630 15:48:44.360113  6523 net.cpp:141] Setting up pool2
I0630 15:48:44.360118  6523 net.cpp:148] Top shape: 256 5 7 7 (62720)
I0630 15:48:44.360122  6523 net.cpp:156] Memory required for data: 339112960
I0630 15:48:44.360127  6523 layer_factory.hpp:77] Creating layer ip1
I0630 15:48:44.360134  6523 net.cpp:91] Creating Layer ip1
I0630 15:48:44.360139  6523 net.cpp:425] ip1 <- pool2
I0630 15:48:44.360144  6523 net.cpp:399] ip1 -> ip1
I0630 15:48:44.361908  6523 net.cpp:141] Setting up ip1
I0630 15:48:44.361930  6523 net.cpp:148] Top shape: 256 32 (8192)
I0630 15:48:44.361938  6523 net.cpp:156] Memory required for data: 339145728
I0630 15:48:44.361953  6523 layer_factory.hpp:77] Creating layer relu1
I0630 15:48:44.361968  6523 net.cpp:91] Creating Layer relu1
I0630 15:48:44.361979  6523 net.cpp:425] relu1 <- ip1
I0630 15:48:44.361989  6523 net.cpp:386] relu1 -> ip1 (in-place)
I0630 15:48:44.361999  6523 net.cpp:141] Setting up relu1
I0630 15:48:44.362010  6523 net.cpp:148] Top shape: 256 32 (8192)
I0630 15:48:44.362017  6523 net.cpp:156] Memory required for data: 339178496
I0630 15:48:44.362025  6523 layer_factory.hpp:77] Creating layer ip2
I0630 15:48:44.362040  6523 net.cpp:91] Creating Layer ip2
I0630 15:48:44.362048  6523 net.cpp:425] ip2 <- ip1
I0630 15:48:44.362061  6523 net.cpp:399] ip2 -> ip2
I0630 15:48:44.362087  6523 net.cpp:141] Setting up ip2
I0630 15:48:44.362099  6523 net.cpp:148] Top shape: 256 2 (512)
I0630 15:48:44.362107  6523 net.cpp:156] Memory required for data: 339180544
I0630 15:48:44.362118  6523 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0630 15:48:44.362130  6523 net.cpp:91] Creating Layer ip2_ip2_0_split
I0630 15:48:44.362138  6523 net.cpp:425] ip2_ip2_0_split <- ip2
I0630 15:48:44.362191  6523 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0630 15:48:44.362207  6523 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0630 15:48:44.362221  6523 net.cpp:141] Setting up ip2_ip2_0_split
I0630 15:48:44.362233  6523 net.cpp:148] Top shape: 256 2 (512)
I0630 15:48:44.362243  6523 net.cpp:148] Top shape: 256 2 (512)
I0630 15:48:44.362251  6523 net.cpp:156] Memory required for data: 339184640
I0630 15:48:44.362260  6523 layer_factory.hpp:77] Creating layer accuracy
I0630 15:48:44.362277  6523 net.cpp:91] Creating Layer accuracy
I0630 15:48:44.362285  6523 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0630 15:48:44.362295  6523 net.cpp:425] accuracy <- label_testing_cells_1_split_0
I0630 15:48:44.362306  6523 net.cpp:399] accuracy -> accuracy
I0630 15:48:44.362321  6523 net.cpp:141] Setting up accuracy
I0630 15:48:44.362331  6523 net.cpp:148] Top shape: (1)
I0630 15:48:44.362339  6523 net.cpp:156] Memory required for data: 339184644
I0630 15:48:44.362346  6523 layer_factory.hpp:77] Creating layer loss
I0630 15:48:44.362361  6523 net.cpp:91] Creating Layer loss
I0630 15:48:44.362371  6523 net.cpp:425] loss <- ip2_ip2_0_split_1
I0630 15:48:44.362380  6523 net.cpp:425] loss <- label_testing_cells_1_split_1
I0630 15:48:44.362390  6523 net.cpp:399] loss -> loss
I0630 15:48:44.362406  6523 layer_factory.hpp:77] Creating layer loss
I0630 15:48:44.362437  6523 net.cpp:141] Setting up loss
I0630 15:48:44.362452  6523 net.cpp:148] Top shape: (1)
I0630 15:48:44.362468  6523 net.cpp:151]     with loss weight 1
I0630 15:48:44.362493  6523 net.cpp:156] Memory required for data: 339184648
I0630 15:48:44.362501  6523 net.cpp:217] loss needs backward computation.
I0630 15:48:44.362510  6523 net.cpp:219] accuracy does not need backward computation.
I0630 15:48:44.362519  6523 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0630 15:48:44.362525  6523 net.cpp:217] ip2 needs backward computation.
I0630 15:48:44.362532  6523 net.cpp:217] relu1 needs backward computation.
I0630 15:48:44.362540  6523 net.cpp:217] ip1 needs backward computation.
I0630 15:48:44.362547  6523 net.cpp:217] pool2 needs backward computation.
I0630 15:48:44.362555  6523 net.cpp:217] conv2 needs backward computation.
I0630 15:48:44.362562  6523 net.cpp:217] pool1 needs backward computation.
I0630 15:48:44.362570  6523 net.cpp:217] conv1 needs backward computation.
I0630 15:48:44.362577  6523 net.cpp:219] label_testing_cells_1_split does not need backward computation.
I0630 15:48:44.362586  6523 net.cpp:219] testing_cells does not need backward computation.
I0630 15:48:44.362592  6523 net.cpp:261] This network produces output accuracy
I0630 15:48:44.362599  6523 net.cpp:261] This network produces output loss
I0630 15:48:44.362615  6523 net.cpp:274] Network initialization done.
I0630 15:48:44.362741  6523 solver.cpp:60] Solver scaffolding done.
I0630 15:48:44.362787  6523 caffe.cpp:209] Resuming from fish_net_pretrain_output_iter_4464.solverstate
I0630 15:48:44.363466  6523 sgd_solver.cpp:318] SGDSolver: restoring history
I0630 15:48:44.363533  6523 caffe.cpp:219] Starting Optimization
I0630 15:48:44.363545  6523 solver.cpp:279] Solving fish_filter
I0630 15:48:44.363554  6523 solver.cpp:280] Learning Rate Policy: inv
I0630 15:48:44.363590  6523 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:48:44.487890  6538 blocking_queue.cpp:50] Waiting for data
I0630 15:50:36.703667  6523 solver.cpp:228] Iteration 4475, loss = 0.122777
I0630 15:50:36.703770  6523 solver.cpp:244]     Train net output #0: loss = 0.122777 (* 1 = 0.122777 loss)
I0630 15:50:36.703783  6523 sgd_solver.cpp:106] Iteration 4475, lr = 0.000757768
I0630 15:54:25.446207  6523 solver.cpp:337] Iteration 4500, Testing net (#0)
I0630 15:54:25.446300  6523 net.cpp:684] Ignoring source layer training_cells
I0630 15:54:25.446316  6523 net.cpp:684] Ignoring source layer drop1
I0630 15:55:20.366927  6523 solver.cpp:404]     Test net output #0: accuracy = 0.950781
I0630 15:55:20.367048  6523 solver.cpp:404]     Test net output #1: loss = 0.139813 (* 1 = 0.139813 loss)
I0630 15:55:29.767952  6523 solver.cpp:228] Iteration 4500, loss = 0.172645
I0630 15:55:29.768000  6523 solver.cpp:244]     Train net output #0: loss = 0.172645 (* 1 = 0.172645 loss)
I0630 15:55:29.768010  6523 sgd_solver.cpp:106] Iteration 4500, lr = 0.000756788
I0630 15:59:27.649114  6523 solver.cpp:228] Iteration 4525, loss = 0.368444
I0630 15:59:27.649222  6523 solver.cpp:244]     Train net output #0: loss = 0.368444 (* 1 = 0.368444 loss)
I0630 15:59:27.649235  6523 sgd_solver.cpp:106] Iteration 4525, lr = 0.00075581
I0630 16:03:22.515856  6523 solver.cpp:228] Iteration 4550, loss = 0.203378
I0630 16:03:22.515959  6523 solver.cpp:244]     Train net output #0: loss = 0.203378 (* 1 = 0.203378 loss)
I0630 16:03:22.515972  6523 sgd_solver.cpp:106] Iteration 4550, lr = 0.000754836
I0630 16:07:19.107465  6523 solver.cpp:228] Iteration 4575, loss = 0.145075
I0630 16:07:19.107583  6523 solver.cpp:244]     Train net output #0: loss = 0.145075 (* 1 = 0.145075 loss)
I0630 16:07:19.107595  6523 sgd_solver.cpp:106] Iteration 4575, lr = 0.000753865
I0630 16:11:15.990458  6523 solver.cpp:228] Iteration 4600, loss = 0.101649
I0630 16:11:15.990559  6523 solver.cpp:244]     Train net output #0: loss = 0.101648 (* 1 = 0.101648 loss)
I0630 16:11:15.990571  6523 sgd_solver.cpp:106] Iteration 4600, lr = 0.000752897
I0630 16:15:09.902925  6523 solver.cpp:228] Iteration 4625, loss = 0.0576982
I0630 16:15:09.903012  6523 solver.cpp:244]     Train net output #0: loss = 0.0576982 (* 1 = 0.0576982 loss)
I0630 16:15:09.903023  6523 sgd_solver.cpp:106] Iteration 4625, lr = 0.000751931
I0630 16:19:05.239506  6523 solver.cpp:228] Iteration 4650, loss = 0.161239
I0630 16:19:05.239603  6523 solver.cpp:244]     Train net output #0: loss = 0.161239 (* 1 = 0.161239 loss)
I0630 16:19:05.239614  6523 sgd_solver.cpp:106] Iteration 4650, lr = 0.000750969
I0630 16:23:00.620085  6523 solver.cpp:228] Iteration 4675, loss = 0.129992
I0630 16:23:00.620198  6523 solver.cpp:244]     Train net output #0: loss = 0.129992 (* 1 = 0.129992 loss)
I0630 16:23:00.620208  6523 sgd_solver.cpp:106] Iteration 4675, lr = 0.000750009
I0630 16:26:55.439728  6523 solver.cpp:228] Iteration 4700, loss = 0.304817
I0630 16:26:55.439823  6523 solver.cpp:244]     Train net output #0: loss = 0.304817 (* 1 = 0.304817 loss)
I0630 16:26:55.439834  6523 sgd_solver.cpp:106] Iteration 4700, lr = 0.000749052
I0630 16:30:59.377233  6523 solver.cpp:228] Iteration 4725, loss = 0.192417
I0630 16:30:59.377337  6523 solver.cpp:244]     Train net output #0: loss = 0.192417 (* 1 = 0.192417 loss)
I0630 16:30:59.377349  6523 sgd_solver.cpp:106] Iteration 4725, lr = 0.000748098
I0630 16:34:51.153795  6523 solver.cpp:228] Iteration 4750, loss = 0.111462
I0630 16:34:51.153901  6523 solver.cpp:244]     Train net output #0: loss = 0.111462 (* 1 = 0.111462 loss)
I0630 16:34:51.153913  6523 sgd_solver.cpp:106] Iteration 4750, lr = 0.000747147
I0630 16:38:43.896185  6523 solver.cpp:228] Iteration 4775, loss = 0.259386
I0630 16:38:43.896296  6523 solver.cpp:244]     Train net output #0: loss = 0.259386 (* 1 = 0.259386 loss)
I0630 16:38:43.896307  6523 sgd_solver.cpp:106] Iteration 4775, lr = 0.000746199
I0630 16:42:36.527294  6523 solver.cpp:228] Iteration 4800, loss = 0.118612
I0630 16:42:36.527400  6523 solver.cpp:244]     Train net output #0: loss = 0.118612 (* 1 = 0.118612 loss)
I0630 16:42:36.527413  6523 sgd_solver.cpp:106] Iteration 4800, lr = 0.000745253
I0630 16:46:28.644353  6523 solver.cpp:228] Iteration 4825, loss = 0.171523
I0630 16:46:28.644636  6523 solver.cpp:244]     Train net output #0: loss = 0.171522 (* 1 = 0.171522 loss)
I0630 16:46:28.644649  6523 sgd_solver.cpp:106] Iteration 4825, lr = 0.00074431
I0630 16:50:22.012457  6523 solver.cpp:228] Iteration 4850, loss = 0.235687
I0630 16:50:22.012554  6523 solver.cpp:244]     Train net output #0: loss = 0.235687 (* 1 = 0.235687 loss)
I0630 16:50:22.012570  6523 sgd_solver.cpp:106] Iteration 4850, lr = 0.00074337
I0630 16:54:14.104943  6523 solver.cpp:228] Iteration 4875, loss = 0.185786
I0630 16:54:14.105060  6523 solver.cpp:244]     Train net output #0: loss = 0.185786 (* 1 = 0.185786 loss)
I0630 16:54:14.105077  6523 sgd_solver.cpp:106] Iteration 4875, lr = 0.000742433
I0630 16:58:06.424233  6523 solver.cpp:228] Iteration 4900, loss = 0.19428
I0630 16:58:06.424340  6523 solver.cpp:244]     Train net output #0: loss = 0.19428 (* 1 = 0.19428 loss)
I0630 16:58:06.424351  6523 sgd_solver.cpp:106] Iteration 4900, lr = 0.000741499
I0630 17:01:58.795554  6523 solver.cpp:228] Iteration 4925, loss = 0.148039
I0630 17:01:58.795660  6523 solver.cpp:244]     Train net output #0: loss = 0.148039 (* 1 = 0.148039 loss)
I0630 17:01:58.795672  6523 sgd_solver.cpp:106] Iteration 4925, lr = 0.000740567
I0630 17:05:55.886654  6523 solver.cpp:228] Iteration 4950, loss = 0.201153
I0630 17:05:55.888706  6523 solver.cpp:244]     Train net output #0: loss = 0.201153 (* 1 = 0.201153 loss)
I0630 17:05:55.888718  6523 sgd_solver.cpp:106] Iteration 4950, lr = 0.000739638
I0630 17:09:48.725458  6523 solver.cpp:228] Iteration 4975, loss = 0.199746
I0630 17:09:48.725558  6523 solver.cpp:244]     Train net output #0: loss = 0.199745 (* 1 = 0.199745 loss)
I0630 17:09:48.725574  6523 sgd_solver.cpp:106] Iteration 4975, lr = 0.000738712
I0630 17:13:31.083047  6523 solver.cpp:454] Snapshotting to binary proto file fish_net_pretrain_output_iter_5000.caffemodel
I0630 17:13:31.083634  6523 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_pretrain_output_iter_5000.solverstate
I0630 17:13:31.083909  6523 solver.cpp:337] Iteration 5000, Testing net (#0)
I0630 17:13:31.083920  6523 net.cpp:684] Ignoring source layer training_cells
I0630 17:13:31.083927  6523 net.cpp:684] Ignoring source layer drop1
I0630 17:14:24.958781  6523 solver.cpp:404]     Test net output #0: accuracy = 0.959375
I0630 17:14:24.958885  6523 solver.cpp:404]     Test net output #1: loss = 0.140831 (* 1 = 0.140831 loss)
I0630 17:14:34.262657  6523 solver.cpp:228] Iteration 5000, loss = 0.166515
I0630 17:14:34.262708  6523 solver.cpp:244]     Train net output #0: loss = 0.166514 (* 1 = 0.166514 loss)
I0630 17:14:34.262718  6523 sgd_solver.cpp:106] Iteration 5000, lr = 0.000737788
I0630 17:18:25.739857  6523 solver.cpp:228] Iteration 5025, loss = 0.138658
I0630 17:18:25.739958  6523 solver.cpp:244]     Train net output #0: loss = 0.138658 (* 1 = 0.138658 loss)
I0630 17:18:25.739969  6523 sgd_solver.cpp:106] Iteration 5025, lr = 0.000736867
I0630 17:22:17.741808  6523 solver.cpp:228] Iteration 5050, loss = 0.156448
I0630 17:22:17.741919  6523 solver.cpp:244]     Train net output #0: loss = 0.156448 (* 1 = 0.156448 loss)
I0630 17:22:17.741931  6523 sgd_solver.cpp:106] Iteration 5050, lr = 0.000735949
I0630 17:26:08.977273  6523 solver.cpp:228] Iteration 5075, loss = 0.239776
I0630 17:26:08.977388  6523 solver.cpp:244]     Train net output #0: loss = 0.239776 (* 1 = 0.239776 loss)
I0630 17:26:08.977401  6523 sgd_solver.cpp:106] Iteration 5075, lr = 0.000735033
I0630 17:30:00.631151  6523 solver.cpp:228] Iteration 5100, loss = 0.0742729
I0630 17:30:00.631253  6523 solver.cpp:244]     Train net output #0: loss = 0.0742728 (* 1 = 0.0742728 loss)
I0630 17:30:00.631264  6523 sgd_solver.cpp:106] Iteration 5100, lr = 0.00073412
I0630 17:33:52.010731  6523 solver.cpp:228] Iteration 5125, loss = 0.112616
I0630 17:33:52.010839  6523 solver.cpp:244]     Train net output #0: loss = 0.112616 (* 1 = 0.112616 loss)
I0630 17:33:52.010851  6523 sgd_solver.cpp:106] Iteration 5125, lr = 0.00073321
I0630 17:37:44.738561  6523 solver.cpp:228] Iteration 5150, loss = 0.1621
I0630 17:37:44.738670  6523 solver.cpp:244]     Train net output #0: loss = 0.1621 (* 1 = 0.1621 loss)
I0630 17:37:44.738682  6523 sgd_solver.cpp:106] Iteration 5150, lr = 0.000732303
I0630 17:41:36.909560  6523 solver.cpp:228] Iteration 5175, loss = 0.195152
I0630 17:41:36.909665  6523 solver.cpp:244]     Train net output #0: loss = 0.195152 (* 1 = 0.195152 loss)
I0630 17:41:36.909677  6523 sgd_solver.cpp:106] Iteration 5175, lr = 0.000731398
I0630 17:45:28.041695  6523 solver.cpp:228] Iteration 5200, loss = 0.241572
I0630 17:45:28.041810  6523 solver.cpp:244]     Train net output #0: loss = 0.241572 (* 1 = 0.241572 loss)
I0630 17:45:28.041822  6523 sgd_solver.cpp:106] Iteration 5200, lr = 0.000730495
I0630 17:49:20.222782  6523 solver.cpp:228] Iteration 5225, loss = 0.131033
I0630 17:49:20.222889  6523 solver.cpp:244]     Train net output #0: loss = 0.131033 (* 1 = 0.131033 loss)
I0630 17:49:20.222900  6523 sgd_solver.cpp:106] Iteration 5225, lr = 0.000729595
I0630 17:53:12.182821  6523 solver.cpp:228] Iteration 5250, loss = 0.126216
I0630 17:53:12.182930  6523 solver.cpp:244]     Train net output #0: loss = 0.126216 (* 1 = 0.126216 loss)
I0630 17:53:12.182942  6523 sgd_solver.cpp:106] Iteration 5250, lr = 0.000728698
I0630 17:57:06.664329  6523 solver.cpp:228] Iteration 5275, loss = 0.11143
I0630 17:57:06.664440  6523 solver.cpp:244]     Train net output #0: loss = 0.11143 (* 1 = 0.11143 loss)
I0630 17:57:06.664453  6523 sgd_solver.cpp:106] Iteration 5275, lr = 0.000727803
I0630 18:00:59.924608  6523 solver.cpp:228] Iteration 5300, loss = 0.188441
I0630 18:00:59.924701  6523 solver.cpp:244]     Train net output #0: loss = 0.188441 (* 1 = 0.188441 loss)
I0630 18:00:59.924712  6523 sgd_solver.cpp:106] Iteration 5300, lr = 0.000726911
I0630 18:04:52.414104  6523 solver.cpp:228] Iteration 5325, loss = 0.128615
I0630 18:04:52.414204  6523 solver.cpp:244]     Train net output #0: loss = 0.128615 (* 1 = 0.128615 loss)
I0630 18:04:52.414216  6523 sgd_solver.cpp:106] Iteration 5325, lr = 0.000726022
I0630 18:08:44.547579  6523 solver.cpp:228] Iteration 5350, loss = 0.69349
I0630 18:08:44.547683  6523 solver.cpp:244]     Train net output #0: loss = 0.69349 (* 1 = 0.69349 loss)
I0630 18:08:44.547694  6523 sgd_solver.cpp:106] Iteration 5350, lr = 0.000725135
I0630 18:12:36.527221  6523 solver.cpp:228] Iteration 5375, loss = 0.103193
I0630 18:12:36.527325  6523 solver.cpp:244]     Train net output #0: loss = 0.103193 (* 1 = 0.103193 loss)
I0630 18:12:36.527338  6523 sgd_solver.cpp:106] Iteration 5375, lr = 0.00072425
I0630 18:16:32.553809  6523 solver.cpp:228] Iteration 5400, loss = 0.0742461
I0630 18:16:32.553920  6523 solver.cpp:244]     Train net output #0: loss = 0.0742459 (* 1 = 0.0742459 loss)
I0630 18:16:32.553936  6523 sgd_solver.cpp:106] Iteration 5400, lr = 0.000723368
I0630 18:20:26.574887  6523 solver.cpp:228] Iteration 5425, loss = 0.124782
I0630 18:20:26.574986  6523 solver.cpp:244]     Train net output #0: loss = 0.124782 (* 1 = 0.124782 loss)
I0630 18:20:26.574997  6523 sgd_solver.cpp:106] Iteration 5425, lr = 0.000722489
I0630 18:24:18.737049  6523 solver.cpp:228] Iteration 5450, loss = 0.0882139
I0630 18:24:18.737148  6523 solver.cpp:244]     Train net output #0: loss = 0.0882138 (* 1 = 0.0882138 loss)
I0630 18:24:18.737159  6523 sgd_solver.cpp:106] Iteration 5450, lr = 0.000721612
I0630 18:28:10.519953  6523 solver.cpp:228] Iteration 5475, loss = 0.219001
I0630 18:28:10.520061  6523 solver.cpp:244]     Train net output #0: loss = 0.219001 (* 1 = 0.219001 loss)
I0630 18:28:10.520072  6523 sgd_solver.cpp:106] Iteration 5475, lr = 0.000720737
I0630 18:31:53.547401  6523 solver.cpp:337] Iteration 5500, Testing net (#0)
I0630 18:31:53.547488  6523 net.cpp:684] Ignoring source layer training_cells
I0630 18:31:53.547497  6523 net.cpp:684] Ignoring source layer drop1
I0630 18:32:47.680094  6523 solver.cpp:404]     Test net output #0: accuracy = 0.952344
I0630 18:32:47.680204  6523 solver.cpp:404]     Test net output #1: loss = 0.135906 (* 1 = 0.135906 loss)
I0630 18:32:56.976835  6523 solver.cpp:228] Iteration 5500, loss = 0.0908659
I0630 18:32:56.976882  6523 solver.cpp:244]     Train net output #0: loss = 0.0908658 (* 1 = 0.0908658 loss)
I0630 18:32:56.976892  6523 sgd_solver.cpp:106] Iteration 5500, lr = 0.000719865
I0630 18:36:48.604835  6523 solver.cpp:228] Iteration 5525, loss = 0.0890019
I0630 18:36:48.604933  6523 solver.cpp:244]     Train net output #0: loss = 0.0890018 (* 1 = 0.0890018 loss)
I0630 18:36:48.604943  6523 sgd_solver.cpp:106] Iteration 5525, lr = 0.000718996
I0630 18:40:40.656486  6523 solver.cpp:228] Iteration 5550, loss = 0.120465
I0630 18:40:40.656607  6523 solver.cpp:244]     Train net output #0: loss = 0.120465 (* 1 = 0.120465 loss)
I0630 18:40:40.656623  6523 sgd_solver.cpp:106] Iteration 5550, lr = 0.000718129
I0630 18:44:34.987848  6523 solver.cpp:228] Iteration 5575, loss = 0.3069
I0630 18:44:34.987952  6523 solver.cpp:244]     Train net output #0: loss = 0.3069 (* 1 = 0.3069 loss)
I0630 18:44:34.987963  6523 sgd_solver.cpp:106] Iteration 5575, lr = 0.000717264
I0630 18:48:32.216087  6523 solver.cpp:228] Iteration 5600, loss = 0.319487
I0630 18:48:32.216197  6523 solver.cpp:244]     Train net output #0: loss = 0.319487 (* 1 = 0.319487 loss)
I0630 18:48:32.216208  6523 sgd_solver.cpp:106] Iteration 5600, lr = 0.000716402
I0630 18:52:22.969507  6523 solver.cpp:228] Iteration 5625, loss = 0.276462
I0630 18:52:22.969610  6523 solver.cpp:244]     Train net output #0: loss = 0.276462 (* 1 = 0.276462 loss)
I0630 18:52:22.969621  6523 sgd_solver.cpp:106] Iteration 5625, lr = 0.000715542
I0630 18:56:16.864815  6523 solver.cpp:228] Iteration 5650, loss = 0.0777521
I0630 18:56:16.864928  6523 solver.cpp:244]     Train net output #0: loss = 0.077752 (* 1 = 0.077752 loss)
I0630 18:56:16.864940  6523 sgd_solver.cpp:106] Iteration 5650, lr = 0.000714684
I0630 19:00:37.941756  6523 solver.cpp:228] Iteration 5675, loss = 0.0915029
I0630 19:00:37.941864  6523 solver.cpp:244]     Train net output #0: loss = 0.0915028 (* 1 = 0.0915028 loss)
I0630 19:00:37.941875  6523 sgd_solver.cpp:106] Iteration 5675, lr = 0.000713829
I0630 19:08:00.792124  6523 solver.cpp:228] Iteration 5700, loss = 0.252273
I0630 19:08:00.792242  6523 solver.cpp:244]     Train net output #0: loss = 0.252273 (* 1 = 0.252273 loss)
I0630 19:08:00.792256  6523 sgd_solver.cpp:106] Iteration 5700, lr = 0.000712977
I0630 19:14:18.230823  6523 solver.cpp:228] Iteration 5725, loss = 0.220317
I0630 19:14:18.232204  6523 solver.cpp:244]     Train net output #0: loss = 0.220317 (* 1 = 0.220317 loss)
I0630 19:14:18.232215  6523 sgd_solver.cpp:106] Iteration 5725, lr = 0.000712126
I0630 19:20:10.003545  6523 solver.cpp:228] Iteration 5750, loss = 0.202984
I0630 19:20:10.003646  6523 solver.cpp:244]     Train net output #0: loss = 0.202984 (* 1 = 0.202984 loss)
I0630 19:20:10.003659  6523 sgd_solver.cpp:106] Iteration 5750, lr = 0.000711278
I0630 19:25:58.506808  6523 solver.cpp:228] Iteration 5775, loss = 0.0977703
I0630 19:25:58.506916  6523 solver.cpp:244]     Train net output #0: loss = 0.0977701 (* 1 = 0.0977701 loss)
I0630 19:25:58.506927  6523 sgd_solver.cpp:106] Iteration 5775, lr = 0.000710433
I0630 19:31:37.562316  6523 solver.cpp:228] Iteration 5800, loss = 0.136935
I0630 19:31:37.562433  6523 solver.cpp:244]     Train net output #0: loss = 0.136934 (* 1 = 0.136934 loss)
I0630 19:31:37.562445  6523 sgd_solver.cpp:106] Iteration 5800, lr = 0.00070959
I0630 19:37:17.837116  6523 solver.cpp:228] Iteration 5825, loss = 0.104909
I0630 19:37:17.837229  6523 solver.cpp:244]     Train net output #0: loss = 0.104909 (* 1 = 0.104909 loss)
I0630 19:37:17.837245  6523 sgd_solver.cpp:106] Iteration 5825, lr = 0.000708749
I0630 19:42:44.010661  6523 solver.cpp:228] Iteration 5850, loss = 0.17699
I0630 19:42:44.010771  6523 solver.cpp:244]     Train net output #0: loss = 0.17699 (* 1 = 0.17699 loss)
I0630 19:42:44.010782  6523 sgd_solver.cpp:106] Iteration 5850, lr = 0.00070791
I0630 19:48:07.171718  6523 solver.cpp:228] Iteration 5875, loss = 0.122342
I0630 19:48:07.171826  6523 solver.cpp:244]     Train net output #0: loss = 0.122342 (* 1 = 0.122342 loss)
I0630 19:48:07.171838  6523 sgd_solver.cpp:106] Iteration 5875, lr = 0.000707074
I0630 19:53:29.260447  6523 solver.cpp:228] Iteration 5900, loss = 0.253356
I0630 19:53:29.260553  6523 solver.cpp:244]     Train net output #0: loss = 0.253356 (* 1 = 0.253356 loss)
I0630 19:53:29.260568  6523 sgd_solver.cpp:106] Iteration 5900, lr = 0.00070624
I0630 19:58:51.092793  6523 solver.cpp:228] Iteration 5925, loss = 0.198631
I0630 19:58:51.092931  6523 solver.cpp:244]     Train net output #0: loss = 0.198631 (* 1 = 0.198631 loss)
I0630 19:58:51.092942  6523 sgd_solver.cpp:106] Iteration 5925, lr = 0.000705408
I0630 20:04:14.671778  6523 solver.cpp:228] Iteration 5950, loss = 0.124606
I0630 20:04:14.675263  6523 solver.cpp:244]     Train net output #0: loss = 0.124606 (* 1 = 0.124606 loss)
I0630 20:04:14.675276  6523 sgd_solver.cpp:106] Iteration 5950, lr = 0.000704579
I0630 20:09:45.808809  6523 solver.cpp:228] Iteration 5975, loss = 0.0876533
I0630 20:09:45.809015  6523 solver.cpp:244]     Train net output #0: loss = 0.087653 (* 1 = 0.087653 loss)
I0630 20:09:45.809026  6523 sgd_solver.cpp:106] Iteration 5975, lr = 0.000703752
I0630 20:15:12.557754  6523 solver.cpp:454] Snapshotting to binary proto file fish_net_pretrain_output_iter_6000.caffemodel
I0630 20:15:12.558495  6523 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_pretrain_output_iter_6000.solverstate
I0630 20:15:12.558904  6523 solver.cpp:337] Iteration 6000, Testing net (#0)
I0630 20:15:12.558924  6523 net.cpp:684] Ignoring source layer training_cells
I0630 20:15:12.558936  6523 net.cpp:684] Ignoring source layer drop1
I0630 20:16:29.073050  6523 solver.cpp:404]     Test net output #0: accuracy = 0.965234
I0630 20:16:29.073477  6523 solver.cpp:404]     Test net output #1: loss = 0.129824 (* 1 = 0.129824 loss)
I0630 20:16:42.051862  6523 solver.cpp:228] Iteration 6000, loss = 0.156053
I0630 20:16:42.051919  6523 solver.cpp:244]     Train net output #0: loss = 0.156053 (* 1 = 0.156053 loss)
I0630 20:16:42.051929  6523 sgd_solver.cpp:106] Iteration 6000, lr = 0.000702927
I0630 20:22:23.538239  6523 solver.cpp:228] Iteration 6025, loss = 0.334586
I0630 20:22:23.539633  6523 solver.cpp:244]     Train net output #0: loss = 0.334586 (* 1 = 0.334586 loss)
I0630 20:22:23.539644  6523 sgd_solver.cpp:106] Iteration 6025, lr = 0.000702104
I0630 20:28:06.719837  6523 solver.cpp:228] Iteration 6050, loss = 0.157315
I0630 20:28:06.719959  6523 solver.cpp:244]     Train net output #0: loss = 0.157315 (* 1 = 0.157315 loss)
I0630 20:28:06.719971  6523 sgd_solver.cpp:106] Iteration 6050, lr = 0.000701284
I0630 20:33:47.239241  6523 solver.cpp:228] Iteration 6075, loss = 0.237836
I0630 20:33:47.239354  6523 solver.cpp:244]     Train net output #0: loss = 0.237836 (* 1 = 0.237836 loss)
I0630 20:33:47.239370  6523 sgd_solver.cpp:106] Iteration 6075, lr = 0.000700466
I0630 20:39:35.989557  6523 solver.cpp:228] Iteration 6100, loss = 0.0570543
I0630 20:39:35.990025  6523 solver.cpp:244]     Train net output #0: loss = 0.057054 (* 1 = 0.057054 loss)
I0630 20:39:35.990036  6523 sgd_solver.cpp:106] Iteration 6100, lr = 0.00069965
I0630 20:45:21.154500  6523 solver.cpp:228] Iteration 6125, loss = 0.123363
I0630 20:45:21.155699  6523 solver.cpp:244]     Train net output #0: loss = 0.123362 (* 1 = 0.123362 loss)
I0630 20:45:21.155711  6523 sgd_solver.cpp:106] Iteration 6125, lr = 0.000698836
I0630 20:51:05.761653  6523 solver.cpp:228] Iteration 6150, loss = 0.135516
I0630 20:51:05.764679  6523 solver.cpp:244]     Train net output #0: loss = 0.135515 (* 1 = 0.135515 loss)
I0630 20:51:05.764695  6523 sgd_solver.cpp:106] Iteration 6150, lr = 0.000698024
I0630 20:56:54.290154  6523 solver.cpp:228] Iteration 6175, loss = 0.211534
I0630 20:56:54.292112  6523 solver.cpp:244]     Train net output #0: loss = 0.211534 (* 1 = 0.211534 loss)
I0630 20:56:54.292124  6523 sgd_solver.cpp:106] Iteration 6175, lr = 0.000697215
I0630 21:02:41.818740  6523 solver.cpp:228] Iteration 6200, loss = 0.214622
I0630 21:02:41.818861  6523 solver.cpp:244]     Train net output #0: loss = 0.214622 (* 1 = 0.214622 loss)
I0630 21:02:41.818881  6523 sgd_solver.cpp:106] Iteration 6200, lr = 0.000696408
I0630 21:08:19.147347  6523 solver.cpp:228] Iteration 6225, loss = 0.104827
I0630 21:08:19.147464  6523 solver.cpp:244]     Train net output #0: loss = 0.104827 (* 1 = 0.104827 loss)
I0630 21:08:19.147480  6523 sgd_solver.cpp:106] Iteration 6225, lr = 0.000695603
I0630 21:14:00.840059  6523 solver.cpp:228] Iteration 6250, loss = 0.171022
I0630 21:14:00.840204  6523 solver.cpp:244]     Train net output #0: loss = 0.171022 (* 1 = 0.171022 loss)
I0630 21:14:00.840219  6523 sgd_solver.cpp:106] Iteration 6250, lr = 0.0006948
I0630 21:19:46.291501  6523 solver.cpp:228] Iteration 6275, loss = 0.130167
I0630 21:19:46.291625  6523 solver.cpp:244]     Train net output #0: loss = 0.130167 (* 1 = 0.130167 loss)
I0630 21:19:46.291646  6523 sgd_solver.cpp:106] Iteration 6275, lr = 0.000694
I0630 21:25:16.283102  6523 solver.cpp:228] Iteration 6300, loss = 0.127575
I0630 21:25:16.283211  6523 solver.cpp:244]     Train net output #0: loss = 0.127574 (* 1 = 0.127574 loss)
I0630 21:25:16.283223  6523 sgd_solver.cpp:106] Iteration 6300, lr = 0.000693201
I0630 21:30:42.615584  6523 solver.cpp:228] Iteration 6325, loss = 0.144834
I0630 21:30:42.615703  6523 solver.cpp:244]     Train net output #0: loss = 0.144834 (* 1 = 0.144834 loss)
I0630 21:30:42.615720  6523 sgd_solver.cpp:106] Iteration 6325, lr = 0.000692405
I0630 21:36:11.515285  6523 solver.cpp:228] Iteration 6350, loss = 0.238393
I0630 21:36:11.516592  6523 solver.cpp:244]     Train net output #0: loss = 0.238392 (* 1 = 0.238392 loss)
I0630 21:36:11.516607  6523 sgd_solver.cpp:106] Iteration 6350, lr = 0.000691611
I0630 21:42:04.065109  6523 solver.cpp:228] Iteration 6375, loss = 0.141801
I0630 21:42:04.065220  6523 solver.cpp:244]     Train net output #0: loss = 0.141801 (* 1 = 0.141801 loss)
I0630 21:42:04.065232  6523 sgd_solver.cpp:106] Iteration 6375, lr = 0.000690819
I0630 21:47:50.486433  6523 solver.cpp:228] Iteration 6400, loss = 0.367621
I0630 21:47:50.488008  6523 solver.cpp:244]     Train net output #0: loss = 0.36762 (* 1 = 0.36762 loss)
I0630 21:47:50.488019  6523 sgd_solver.cpp:106] Iteration 6400, lr = 0.000690029
I0630 21:53:41.265314  6523 solver.cpp:228] Iteration 6425, loss = 0.215899
I0630 21:53:41.265416  6523 solver.cpp:244]     Train net output #0: loss = 0.215899 (* 1 = 0.215899 loss)
I0630 21:53:41.265427  6523 sgd_solver.cpp:106] Iteration 6425, lr = 0.000689241
I0630 21:59:26.128463  6523 solver.cpp:228] Iteration 6450, loss = 0.188413
I0630 21:59:26.128582  6523 solver.cpp:244]     Train net output #0: loss = 0.188412 (* 1 = 0.188412 loss)
I0630 21:59:26.128597  6523 sgd_solver.cpp:106] Iteration 6450, lr = 0.000688455
I0630 22:05:05.289540  6523 solver.cpp:228] Iteration 6475, loss = 0.0933775
I0630 22:05:05.290400  6523 solver.cpp:244]     Train net output #0: loss = 0.0933773 (* 1 = 0.0933773 loss)
I0630 22:05:05.290413  6523 sgd_solver.cpp:106] Iteration 6475, lr = 0.000687671
I0630 22:10:47.600499  6523 solver.cpp:337] Iteration 6500, Testing net (#0)
I0630 22:10:47.600603  6523 net.cpp:684] Ignoring source layer training_cells
I0630 22:10:47.600612  6523 net.cpp:684] Ignoring source layer drop1
I0630 22:12:02.943353  6523 solver.cpp:404]     Test net output #0: accuracy = 0.958203
I0630 22:12:02.943465  6523 solver.cpp:404]     Test net output #1: loss = 0.125186 (* 1 = 0.125186 loss)
I0630 22:12:17.327415  6523 solver.cpp:228] Iteration 6500, loss = 0.134255
I0630 22:12:17.328112  6523 solver.cpp:244]     Train net output #0: loss = 0.134255 (* 1 = 0.134255 loss)
I0630 22:12:17.328122  6523 sgd_solver.cpp:106] Iteration 6500, lr = 0.00068689
I0630 22:18:06.991571  6523 solver.cpp:228] Iteration 6525, loss = 0.294332
I0630 22:18:06.993229  6523 solver.cpp:244]     Train net output #0: loss = 0.294332 (* 1 = 0.294332 loss)
I0630 22:18:06.993247  6523 sgd_solver.cpp:106] Iteration 6525, lr = 0.00068611
I0630 22:23:52.136643  6523 solver.cpp:228] Iteration 6550, loss = 0.143581
I0630 22:23:52.136752  6523 solver.cpp:244]     Train net output #0: loss = 0.143581 (* 1 = 0.143581 loss)
I0630 22:23:52.136768  6523 sgd_solver.cpp:106] Iteration 6550, lr = 0.000685333
I0630 22:29:38.057090  6523 solver.cpp:228] Iteration 6575, loss = 0.174292
I0630 22:29:38.057196  6523 solver.cpp:244]     Train net output #0: loss = 0.174292 (* 1 = 0.174292 loss)
I0630 22:29:38.057207  6523 sgd_solver.cpp:106] Iteration 6575, lr = 0.000684557
I0630 22:35:24.542009  6523 solver.cpp:228] Iteration 6600, loss = 0.213489
I0630 22:35:24.542155  6523 solver.cpp:244]     Train net output #0: loss = 0.213489 (* 1 = 0.213489 loss)
I0630 22:35:24.542181  6523 sgd_solver.cpp:106] Iteration 6600, lr = 0.000683784
I0630 22:41:07.312542  6523 solver.cpp:228] Iteration 6625, loss = 0.0672718
I0630 22:41:07.313948  6523 solver.cpp:244]     Train net output #0: loss = 0.0672716 (* 1 = 0.0672716 loss)
I0630 22:41:07.313961  6523 sgd_solver.cpp:106] Iteration 6625, lr = 0.000683013
I0630 22:46:36.101619  6523 solver.cpp:228] Iteration 6650, loss = 0.0788645
I0630 22:46:36.101716  6523 solver.cpp:244]     Train net output #0: loss = 0.0788643 (* 1 = 0.0788643 loss)
I0630 22:46:36.101727  6523 sgd_solver.cpp:106] Iteration 6650, lr = 0.000682243
I0630 22:52:07.665767  6523 solver.cpp:228] Iteration 6675, loss = 0.156603
I0630 22:52:07.665882  6523 solver.cpp:244]     Train net output #0: loss = 0.156603 (* 1 = 0.156603 loss)
I0630 22:52:07.665894  6523 sgd_solver.cpp:106] Iteration 6675, lr = 0.000681476
I0630 22:57:40.571836  6523 solver.cpp:228] Iteration 6700, loss = 0.196756
I0630 22:57:40.571949  6523 solver.cpp:244]     Train net output #0: loss = 0.196756 (* 1 = 0.196756 loss)
I0630 22:57:40.571964  6523 sgd_solver.cpp:106] Iteration 6700, lr = 0.000680711
I0630 23:03:08.201673  6523 solver.cpp:228] Iteration 6725, loss = 0.050992
I0630 23:03:08.201781  6523 solver.cpp:244]     Train net output #0: loss = 0.0509917 (* 1 = 0.0509917 loss)
I0630 23:03:08.201797  6523 sgd_solver.cpp:106] Iteration 6725, lr = 0.000679948
I0630 23:08:37.454324  6523 solver.cpp:228] Iteration 6750, loss = 0.0783235
I0630 23:08:37.454421  6523 solver.cpp:244]     Train net output #0: loss = 0.0783232 (* 1 = 0.0783232 loss)
I0630 23:08:37.454433  6523 sgd_solver.cpp:106] Iteration 6750, lr = 0.000679186
I0630 23:14:07.148125  6523 solver.cpp:228] Iteration 6775, loss = 0.214719
I0630 23:14:07.148244  6523 solver.cpp:244]     Train net output #0: loss = 0.214718 (* 1 = 0.214718 loss)
I0630 23:14:07.148262  6523 sgd_solver.cpp:106] Iteration 6775, lr = 0.000678427
I0630 23:19:51.195515  6523 solver.cpp:228] Iteration 6800, loss = 0.213555
I0630 23:19:51.197625  6523 solver.cpp:244]     Train net output #0: loss = 0.213554 (* 1 = 0.213554 loss)
I0630 23:19:51.197638  6523 sgd_solver.cpp:106] Iteration 6800, lr = 0.00067767
I0630 23:25:25.157521  6523 solver.cpp:228] Iteration 6825, loss = 0.208644
I0630 23:25:25.157824  6523 solver.cpp:244]     Train net output #0: loss = 0.208643 (* 1 = 0.208643 loss)
I0630 23:25:25.157838  6523 sgd_solver.cpp:106] Iteration 6825, lr = 0.000676914
I0630 23:31:01.209390  6523 solver.cpp:228] Iteration 6850, loss = 0.0425239
I0630 23:31:01.209508  6523 solver.cpp:244]     Train net output #0: loss = 0.0425237 (* 1 = 0.0425237 loss)
I0630 23:31:01.209524  6523 sgd_solver.cpp:106] Iteration 6850, lr = 0.000676161
I0630 23:36:41.213376  6523 solver.cpp:228] Iteration 6875, loss = 0.110443
I0630 23:36:41.213487  6523 solver.cpp:244]     Train net output #0: loss = 0.110442 (* 1 = 0.110442 loss)
I0630 23:36:41.213500  6523 sgd_solver.cpp:106] Iteration 6875, lr = 0.00067541
I0630 23:42:30.170251  6523 solver.cpp:228] Iteration 6900, loss = 0.112799
I0630 23:42:30.170367  6523 solver.cpp:244]     Train net output #0: loss = 0.112799 (* 1 = 0.112799 loss)
I0630 23:42:30.170377  6523 sgd_solver.cpp:106] Iteration 6900, lr = 0.00067466
I0630 23:48:06.605919  6523 solver.cpp:228] Iteration 6925, loss = 0.201195
I0630 23:48:06.606497  6523 solver.cpp:244]     Train net output #0: loss = 0.201194 (* 1 = 0.201194 loss)
I0630 23:48:06.606508  6523 sgd_solver.cpp:106] Iteration 6925, lr = 0.000673913
I0630 23:53:42.462067  6523 solver.cpp:228] Iteration 6950, loss = 0.0739946
I0630 23:53:42.462178  6523 solver.cpp:244]     Train net output #0: loss = 0.0739944 (* 1 = 0.0739944 loss)
I0630 23:53:42.462189  6523 sgd_solver.cpp:106] Iteration 6950, lr = 0.000673167
I0630 23:59:18.905424  6523 solver.cpp:228] Iteration 6975, loss = 0.0987544
I0630 23:59:18.905540  6523 solver.cpp:244]     Train net output #0: loss = 0.0987542 (* 1 = 0.0987542 loss)
I0630 23:59:18.905556  6523 sgd_solver.cpp:106] Iteration 6975, lr = 0.000672423
I0701 00:04:46.298609  6523 solver.cpp:454] Snapshotting to binary proto file fish_net_pretrain_output_iter_7000.caffemodel
I0701 00:04:46.299157  6523 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_pretrain_output_iter_7000.solverstate
I0701 00:04:46.299445  6523 solver.cpp:337] Iteration 7000, Testing net (#0)
I0701 00:04:46.299455  6523 net.cpp:684] Ignoring source layer training_cells
I0701 00:04:46.299473  6523 net.cpp:684] Ignoring source layer drop1
I0701 00:05:59.706447  6523 solver.cpp:404]     Test net output #0: accuracy = 0.964844
I0701 00:05:59.708420  6523 solver.cpp:404]     Test net output #1: loss = 0.126846 (* 1 = 0.126846 loss)
I0701 00:06:13.301790  6523 solver.cpp:228] Iteration 7000, loss = 0.254357
I0701 00:06:13.302009  6523 solver.cpp:244]     Train net output #0: loss = 0.254357 (* 1 = 0.254357 loss)
I0701 00:06:13.302021  6523 sgd_solver.cpp:106] Iteration 7000, lr = 0.000671681
I0701 00:11:56.009774  6523 solver.cpp:228] Iteration 7025, loss = 0.0893968
I0701 00:11:56.011416  6523 solver.cpp:244]     Train net output #0: loss = 0.0893966 (* 1 = 0.0893966 loss)
I0701 00:11:56.011428  6523 sgd_solver.cpp:106] Iteration 7025, lr = 0.000670942
I0701 00:17:40.868541  6523 solver.cpp:228] Iteration 7050, loss = 0.256598
I0701 00:17:40.868648  6523 solver.cpp:244]     Train net output #0: loss = 0.256597 (* 1 = 0.256597 loss)
I0701 00:17:40.868659  6523 sgd_solver.cpp:106] Iteration 7050, lr = 0.000670204
I0701 00:23:18.112035  6523 solver.cpp:228] Iteration 7075, loss = 0.0664149
I0701 00:23:18.112805  6523 solver.cpp:244]     Train net output #0: loss = 0.0664146 (* 1 = 0.0664146 loss)
I0701 00:23:18.112818  6523 sgd_solver.cpp:106] Iteration 7075, lr = 0.000669467
I0701 00:28:49.906277  6523 solver.cpp:228] Iteration 7100, loss = 0.234198
I0701 00:28:49.907058  6523 solver.cpp:244]     Train net output #0: loss = 0.234198 (* 1 = 0.234198 loss)
I0701 00:28:49.907069  6523 sgd_solver.cpp:106] Iteration 7100, lr = 0.000668733
I0701 00:34:14.809234  6523 solver.cpp:228] Iteration 7125, loss = 0.164218
I0701 00:34:14.811183  6523 solver.cpp:244]     Train net output #0: loss = 0.164218 (* 1 = 0.164218 loss)
I0701 00:34:14.811197  6523 sgd_solver.cpp:106] Iteration 7125, lr = 0.000668001
I0701 00:39:53.286401  6523 solver.cpp:228] Iteration 7150, loss = 0.100104
I0701 00:39:53.287919  6523 solver.cpp:244]     Train net output #0: loss = 0.100104 (* 1 = 0.100104 loss)
I0701 00:39:53.287931  6523 sgd_solver.cpp:106] Iteration 7150, lr = 0.000667271
I0701 00:45:31.973894  6523 solver.cpp:228] Iteration 7175, loss = 0.0918141
I0701 00:45:31.974004  6523 solver.cpp:244]     Train net output #0: loss = 0.0918138 (* 1 = 0.0918138 loss)
I0701 00:45:31.974015  6523 sgd_solver.cpp:106] Iteration 7175, lr = 0.000666542
I0701 00:51:10.697120  6523 solver.cpp:228] Iteration 7200, loss = 0.0885075
I0701 00:51:10.698794  6523 solver.cpp:244]     Train net output #0: loss = 0.0885072 (* 1 = 0.0885072 loss)
I0701 00:51:10.698810  6523 sgd_solver.cpp:106] Iteration 7200, lr = 0.000665815
I0701 00:56:53.529409  6523 solver.cpp:228] Iteration 7225, loss = 0.122362
I0701 00:56:53.531153  6523 solver.cpp:244]     Train net output #0: loss = 0.122362 (* 1 = 0.122362 loss)
I0701 00:56:53.531165  6523 sgd_solver.cpp:106] Iteration 7225, lr = 0.00066509
I0701 01:02:38.493744  6523 solver.cpp:228] Iteration 7250, loss = 0.123526
I0701 01:02:38.493850  6523 solver.cpp:244]     Train net output #0: loss = 0.123526 (* 1 = 0.123526 loss)
I0701 01:02:38.493862  6523 sgd_solver.cpp:106] Iteration 7250, lr = 0.000664367
I0701 01:08:20.854214  6523 solver.cpp:228] Iteration 7275, loss = 0.0858821
I0701 01:08:20.854359  6523 solver.cpp:244]     Train net output #0: loss = 0.0858818 (* 1 = 0.0858818 loss)
I0701 01:08:20.854377  6523 sgd_solver.cpp:106] Iteration 7275, lr = 0.000663646
I0701 01:13:43.538518  6523 solver.cpp:228] Iteration 7300, loss = 0.304784
I0701 01:13:43.538640  6523 solver.cpp:244]     Train net output #0: loss = 0.304783 (* 1 = 0.304783 loss)
I0701 01:13:43.538655  6523 sgd_solver.cpp:106] Iteration 7300, lr = 0.000662927
I0701 01:19:11.559494  6523 solver.cpp:228] Iteration 7325, loss = 0.0714494
I0701 01:19:11.559639  6523 solver.cpp:244]     Train net output #0: loss = 0.0714491 (* 1 = 0.0714491 loss)
I0701 01:19:11.559651  6523 sgd_solver.cpp:106] Iteration 7325, lr = 0.000662209
I0701 01:24:51.111961  6523 solver.cpp:228] Iteration 7350, loss = 0.205654
I0701 01:24:51.112056  6523 solver.cpp:244]     Train net output #0: loss = 0.205654 (* 1 = 0.205654 loss)
I0701 01:24:51.112068  6523 sgd_solver.cpp:106] Iteration 7350, lr = 0.000661493
I0701 01:30:33.114289  6523 solver.cpp:228] Iteration 7375, loss = 0.22111
I0701 01:30:33.114616  6523 solver.cpp:244]     Train net output #0: loss = 0.22111 (* 1 = 0.22111 loss)
I0701 01:30:33.114632  6523 sgd_solver.cpp:106] Iteration 7375, lr = 0.000660779
I0701 01:36:02.644676  6523 solver.cpp:228] Iteration 7400, loss = 0.092578
I0701 01:36:02.644781  6523 solver.cpp:244]     Train net output #0: loss = 0.0925777 (* 1 = 0.0925777 loss)
I0701 01:36:02.644793  6523 sgd_solver.cpp:106] Iteration 7400, lr = 0.000660067
I0701 01:41:34.575130  6523 solver.cpp:228] Iteration 7425, loss = 0.189465
I0701 01:41:34.575232  6523 solver.cpp:244]     Train net output #0: loss = 0.189464 (* 1 = 0.189464 loss)
I0701 01:41:34.575244  6523 sgd_solver.cpp:106] Iteration 7425, lr = 0.000659357
I0701 01:47:33.585672  6523 solver.cpp:228] Iteration 7450, loss = 0.185267
I0701 01:47:33.585784  6523 solver.cpp:244]     Train net output #0: loss = 0.185266 (* 1 = 0.185266 loss)
I0701 01:47:33.585795  6523 sgd_solver.cpp:106] Iteration 7450, lr = 0.000658648
I0701 01:53:11.910965  6523 solver.cpp:228] Iteration 7475, loss = 0.230998
I0701 01:53:11.911074  6523 solver.cpp:244]     Train net output #0: loss = 0.230997 (* 1 = 0.230997 loss)
I0701 01:53:11.911088  6523 sgd_solver.cpp:106] Iteration 7475, lr = 0.000657941
I0701 01:58:45.637001  6523 solver.cpp:337] Iteration 7500, Testing net (#0)
I0701 01:58:45.637626  6523 net.cpp:684] Ignoring source layer training_cells
I0701 01:58:45.637635  6523 net.cpp:684] Ignoring source layer drop1
I0701 02:00:02.462882  6523 solver.cpp:404]     Test net output #0: accuracy = 0.959375
I0701 02:00:02.462990  6523 solver.cpp:404]     Test net output #1: loss = 0.127703 (* 1 = 0.127703 loss)
I0701 02:00:17.441453  6523 solver.cpp:228] Iteration 7500, loss = 0.203057
I0701 02:00:17.441509  6523 solver.cpp:244]     Train net output #0: loss = 0.203057 (* 1 = 0.203057 loss)
I0701 02:00:17.441519  6523 sgd_solver.cpp:106] Iteration 7500, lr = 0.000657236
I0701 02:06:15.389758  6523 solver.cpp:228] Iteration 7525, loss = 0.105425
I0701 02:06:15.389880  6523 solver.cpp:244]     Train net output #0: loss = 0.105425 (* 1 = 0.105425 loss)
I0701 02:06:15.389896  6523 sgd_solver.cpp:106] Iteration 7525, lr = 0.000656533
I0701 02:12:02.356287  6523 solver.cpp:228] Iteration 7550, loss = 0.122595
I0701 02:12:02.356410  6523 solver.cpp:244]     Train net output #0: loss = 0.122595 (* 1 = 0.122595 loss)
I0701 02:12:02.356425  6523 sgd_solver.cpp:106] Iteration 7550, lr = 0.000655831
I0701 02:17:40.166844  6523 solver.cpp:228] Iteration 7575, loss = 0.148121
I0701 02:17:40.167455  6523 solver.cpp:244]     Train net output #0: loss = 0.148121 (* 1 = 0.148121 loss)
I0701 02:17:40.167467  6523 sgd_solver.cpp:106] Iteration 7575, lr = 0.000655132
I0701 02:23:20.145455  6523 solver.cpp:228] Iteration 7600, loss = 0.447452
I0701 02:23:20.147394  6523 solver.cpp:244]     Train net output #0: loss = 0.447451 (* 1 = 0.447451 loss)
I0701 02:23:20.147411  6523 sgd_solver.cpp:106] Iteration 7600, lr = 0.000654434
I0701 02:28:57.966048  6523 solver.cpp:228] Iteration 7625, loss = 0.164939
I0701 02:28:57.966167  6523 solver.cpp:244]     Train net output #0: loss = 0.164939 (* 1 = 0.164939 loss)
I0701 02:28:57.966184  6523 sgd_solver.cpp:106] Iteration 7625, lr = 0.000653737
I0701 02:34:36.333825  6523 solver.cpp:228] Iteration 7650, loss = 0.145304
I0701 02:34:36.333947  6523 solver.cpp:244]     Train net output #0: loss = 0.145304 (* 1 = 0.145304 loss)
I0701 02:34:36.333959  6523 sgd_solver.cpp:106] Iteration 7650, lr = 0.000653043
I0701 02:40:18.543985  6523 solver.cpp:228] Iteration 7675, loss = 0.106955
I0701 02:40:18.544109  6523 solver.cpp:244]     Train net output #0: loss = 0.106954 (* 1 = 0.106954 loss)
I0701 02:40:18.544126  6523 sgd_solver.cpp:106] Iteration 7675, lr = 0.00065235
I0701 02:45:56.259775  6523 solver.cpp:228] Iteration 7700, loss = 0.124529
I0701 02:45:56.262492  6523 solver.cpp:244]     Train net output #0: loss = 0.124528 (* 1 = 0.124528 loss)
I0701 02:45:56.262509  6523 sgd_solver.cpp:106] Iteration 7700, lr = 0.000651659
I0701 02:51:25.630957  6523 solver.cpp:228] Iteration 7725, loss = 0.296577
I0701 02:51:25.631063  6523 solver.cpp:244]     Train net output #0: loss = 0.296576 (* 1 = 0.296576 loss)
I0701 02:51:25.631075  6523 sgd_solver.cpp:106] Iteration 7725, lr = 0.000650969
I0701 02:56:58.063184  6523 solver.cpp:228] Iteration 7750, loss = 0.261763
I0701 02:56:58.064985  6523 solver.cpp:244]     Train net output #0: loss = 0.261762 (* 1 = 0.261762 loss)
I0701 02:56:58.064997  6523 sgd_solver.cpp:106] Iteration 7750, lr = 0.000650281
I0701 03:02:32.134150  6523 solver.cpp:228] Iteration 7775, loss = 0.152934
I0701 03:02:32.135239  6523 solver.cpp:244]     Train net output #0: loss = 0.152933 (* 1 = 0.152933 loss)
I0701 03:02:32.135251  6523 sgd_solver.cpp:106] Iteration 7775, lr = 0.000649595
I0701 03:07:59.847630  6523 solver.cpp:228] Iteration 7800, loss = 0.116769
I0701 03:07:59.847753  6523 solver.cpp:244]     Train net output #0: loss = 0.116769 (* 1 = 0.116769 loss)
I0701 03:07:59.847769  6523 sgd_solver.cpp:106] Iteration 7800, lr = 0.000648911
I0701 03:13:22.771654  6523 solver.cpp:228] Iteration 7825, loss = 0.0910733
I0701 03:13:22.771765  6523 solver.cpp:244]     Train net output #0: loss = 0.0910729 (* 1 = 0.0910729 loss)
I0701 03:13:22.771777  6523 sgd_solver.cpp:106] Iteration 7825, lr = 0.000648228
I0701 03:18:44.036232  6523 solver.cpp:228] Iteration 7850, loss = 0.0576503
I0701 03:18:44.036345  6523 solver.cpp:244]     Train net output #0: loss = 0.0576499 (* 1 = 0.0576499 loss)
I0701 03:18:44.036358  6523 sgd_solver.cpp:106] Iteration 7850, lr = 0.000647547
I0701 03:24:17.231322  6523 solver.cpp:228] Iteration 7875, loss = 0.23725
I0701 03:24:17.231429  6523 solver.cpp:244]     Train net output #0: loss = 0.23725 (* 1 = 0.23725 loss)
I0701 03:24:17.231446  6523 sgd_solver.cpp:106] Iteration 7875, lr = 0.000646868
I0701 03:29:53.419842  6523 solver.cpp:228] Iteration 7900, loss = 0.11101
I0701 03:29:53.420155  6523 solver.cpp:244]     Train net output #0: loss = 0.111009 (* 1 = 0.111009 loss)
I0701 03:29:53.420171  6523 sgd_solver.cpp:106] Iteration 7900, lr = 0.00064619
I0701 03:35:26.603591  6523 solver.cpp:228] Iteration 7925, loss = 0.180085
I0701 03:35:26.603688  6523 solver.cpp:244]     Train net output #0: loss = 0.180084 (* 1 = 0.180084 loss)
I0701 03:35:26.603700  6523 sgd_solver.cpp:106] Iteration 7925, lr = 0.000645514
I0701 03:41:04.542848  6523 solver.cpp:228] Iteration 7950, loss = 0.131705
I0701 03:41:04.542961  6523 solver.cpp:244]     Train net output #0: loss = 0.131705 (* 1 = 0.131705 loss)
I0701 03:41:04.542978  6523 sgd_solver.cpp:106] Iteration 7950, lr = 0.00064484
I0701 03:46:27.970779  6523 solver.cpp:228] Iteration 7975, loss = 0.0728134
I0701 03:46:27.970891  6523 solver.cpp:244]     Train net output #0: loss = 0.0728131 (* 1 = 0.0728131 loss)
I0701 03:46:27.970903  6523 sgd_solver.cpp:106] Iteration 7975, lr = 0.000644167
I0701 03:51:39.801738  6523 solver.cpp:454] Snapshotting to binary proto file fish_net_pretrain_output_iter_8000.caffemodel
I0701 03:51:39.805711  6523 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_pretrain_output_iter_8000.solverstate
I0701 03:51:39.806030  6523 solver.cpp:337] Iteration 8000, Testing net (#0)
I0701 03:51:39.806051  6523 net.cpp:684] Ignoring source layer training_cells
I0701 03:51:39.806059  6523 net.cpp:684] Ignoring source layer drop1
I0701 03:52:50.397894  6523 solver.cpp:404]     Test net output #0: accuracy = 0.966406
I0701 03:52:50.403066  6523 solver.cpp:404]     Test net output #1: loss = 0.124917 (* 1 = 0.124917 loss)
I0701 03:53:03.441213  6523 solver.cpp:228] Iteration 8000, loss = 0.166888
I0701 03:53:03.441612  6523 solver.cpp:244]     Train net output #0: loss = 0.166888 (* 1 = 0.166888 loss)
I0701 03:53:03.441623  6523 sgd_solver.cpp:106] Iteration 8000, lr = 0.000643496
I0701 03:58:31.512166  6523 solver.cpp:228] Iteration 8025, loss = 0.10632
I0701 03:58:31.512275  6523 solver.cpp:244]     Train net output #0: loss = 0.10632 (* 1 = 0.10632 loss)
I0701 03:58:31.512289  6523 sgd_solver.cpp:106] Iteration 8025, lr = 0.000642826
I0701 04:04:07.961210  6523 solver.cpp:228] Iteration 8050, loss = 0.172867
I0701 04:04:07.961694  6523 solver.cpp:244]     Train net output #0: loss = 0.172867 (* 1 = 0.172867 loss)
I0701 04:04:07.961704  6523 sgd_solver.cpp:106] Iteration 8050, lr = 0.000642158
I0701 04:09:39.568027  6523 solver.cpp:228] Iteration 8075, loss = 0.157769
I0701 04:09:39.568325  6523 solver.cpp:244]     Train net output #0: loss = 0.157769 (* 1 = 0.157769 loss)
I0701 04:09:39.568337  6523 sgd_solver.cpp:106] Iteration 8075, lr = 0.000641492
I0701 04:15:06.197273  6523 solver.cpp:228] Iteration 8100, loss = 0.100569
I0701 04:15:06.197384  6523 solver.cpp:244]     Train net output #0: loss = 0.100569 (* 1 = 0.100569 loss)
I0701 04:15:06.197396  6523 sgd_solver.cpp:106] Iteration 8100, lr = 0.000640827
I0701 04:20:30.139225  6523 solver.cpp:228] Iteration 8125, loss = 0.12912
I0701 04:20:30.139338  6523 solver.cpp:244]     Train net output #0: loss = 0.129119 (* 1 = 0.129119 loss)
I0701 04:20:30.139350  6523 sgd_solver.cpp:106] Iteration 8125, lr = 0.000640164
I0701 04:26:10.050457  6523 solver.cpp:228] Iteration 8150, loss = 0.108073
I0701 04:26:10.053134  6523 solver.cpp:244]     Train net output #0: loss = 0.108073 (* 1 = 0.108073 loss)
I0701 04:26:10.053150  6523 sgd_solver.cpp:106] Iteration 8150, lr = 0.000639503
I0701 04:31:55.478113  6523 solver.cpp:228] Iteration 8175, loss = 0.124544
I0701 04:31:55.479393  6523 solver.cpp:244]     Train net output #0: loss = 0.124544 (* 1 = 0.124544 loss)
I0701 04:31:55.479404  6523 sgd_solver.cpp:106] Iteration 8175, lr = 0.000638843
I0701 04:37:28.546694  6523 solver.cpp:228] Iteration 8200, loss = 0.191942
I0701 04:37:28.546799  6523 solver.cpp:244]     Train net output #0: loss = 0.191942 (* 1 = 0.191942 loss)
I0701 04:37:28.546811  6523 sgd_solver.cpp:106] Iteration 8200, lr = 0.000638185
I0701 04:43:01.959707  6523 solver.cpp:228] Iteration 8225, loss = 0.159527
I0701 04:43:01.959835  6523 solver.cpp:244]     Train net output #0: loss = 0.159527 (* 1 = 0.159527 loss)
I0701 04:43:01.959851  6523 sgd_solver.cpp:106] Iteration 8225, lr = 0.000637528
I0701 04:48:39.475024  6523 solver.cpp:228] Iteration 8250, loss = 0.232543
I0701 04:48:39.476172  6523 solver.cpp:244]     Train net output #0: loss = 0.232543 (* 1 = 0.232543 loss)
I0701 04:48:39.476184  6523 sgd_solver.cpp:106] Iteration 8250, lr = 0.000636873
I0701 04:54:06.590107  6523 solver.cpp:228] Iteration 8275, loss = 0.112372
I0701 04:54:06.590221  6523 solver.cpp:244]     Train net output #0: loss = 0.112372 (* 1 = 0.112372 loss)
I0701 04:54:06.590239  6523 sgd_solver.cpp:106] Iteration 8275, lr = 0.00063622
I0701 04:59:45.952163  6523 solver.cpp:228] Iteration 8300, loss = 0.224562
I0701 04:59:45.952267  6523 solver.cpp:244]     Train net output #0: loss = 0.224561 (* 1 = 0.224561 loss)
I0701 04:59:45.952278  6523 sgd_solver.cpp:106] Iteration 8300, lr = 0.000635568
I0701 05:05:26.460635  6523 solver.cpp:228] Iteration 8325, loss = 0.0552577
I0701 05:05:26.460747  6523 solver.cpp:244]     Train net output #0: loss = 0.0552575 (* 1 = 0.0552575 loss)
I0701 05:05:26.460758  6523 sgd_solver.cpp:106] Iteration 8325, lr = 0.000634917
I0701 05:11:12.464155  6523 solver.cpp:228] Iteration 8350, loss = 0.0598893
I0701 05:11:12.464257  6523 solver.cpp:244]     Train net output #0: loss = 0.059889 (* 1 = 0.059889 loss)
I0701 05:11:12.464272  6523 sgd_solver.cpp:106] Iteration 8350, lr = 0.000634268
I0701 05:16:47.117233  6523 solver.cpp:228] Iteration 8375, loss = 0.089336
I0701 05:16:47.117352  6523 solver.cpp:244]     Train net output #0: loss = 0.0893357 (* 1 = 0.0893357 loss)
I0701 05:16:47.117363  6523 sgd_solver.cpp:106] Iteration 8375, lr = 0.000633621
I0701 05:22:16.283005  6523 solver.cpp:228] Iteration 8400, loss = 0.135822
I0701 05:22:16.284771  6523 solver.cpp:244]     Train net output #0: loss = 0.135822 (* 1 = 0.135822 loss)
I0701 05:22:16.284783  6523 sgd_solver.cpp:106] Iteration 8400, lr = 0.000632975
I0701 05:27:45.098011  6523 solver.cpp:228] Iteration 8425, loss = 0.265132
I0701 05:27:45.098119  6523 solver.cpp:244]     Train net output #0: loss = 0.265132 (* 1 = 0.265132 loss)
I0701 05:27:45.098130  6523 sgd_solver.cpp:106] Iteration 8425, lr = 0.000632331
I0701 05:33:22.438289  6523 solver.cpp:228] Iteration 8450, loss = 0.0291409
I0701 05:33:22.438400  6523 solver.cpp:244]     Train net output #0: loss = 0.0291406 (* 1 = 0.0291406 loss)
I0701 05:33:22.438411  6523 sgd_solver.cpp:106] Iteration 8450, lr = 0.000631688
I0701 05:39:03.946586  6523 solver.cpp:228] Iteration 8475, loss = 0.0940787
I0701 05:39:03.946689  6523 solver.cpp:244]     Train net output #0: loss = 0.0940784 (* 1 = 0.0940784 loss)
I0701 05:39:03.946702  6523 sgd_solver.cpp:106] Iteration 8475, lr = 0.000631047
I0701 05:44:37.028838  6523 solver.cpp:337] Iteration 8500, Testing net (#0)
I0701 05:44:37.030772  6523 net.cpp:684] Ignoring source layer training_cells
I0701 05:44:37.030784  6523 net.cpp:684] Ignoring source layer drop1
I0701 05:45:50.833864  6523 solver.cpp:404]     Test net output #0: accuracy = 0.969922
I0701 05:45:50.833986  6523 solver.cpp:404]     Test net output #1: loss = 0.0942297 (* 1 = 0.0942297 loss)
I0701 05:46:04.646322  6523 solver.cpp:228] Iteration 8500, loss = 0.0851381
I0701 05:46:04.646365  6523 solver.cpp:244]     Train net output #0: loss = 0.0851379 (* 1 = 0.0851379 loss)
I0701 05:46:04.646376  6523 sgd_solver.cpp:106] Iteration 8500, lr = 0.000630407
I0701 05:51:55.115494  6523 solver.cpp:228] Iteration 8525, loss = 0.149086
I0701 05:51:55.115608  6523 solver.cpp:244]     Train net output #0: loss = 0.149086 (* 1 = 0.149086 loss)
I0701 05:51:55.115619  6523 sgd_solver.cpp:106] Iteration 8525, lr = 0.000629769
I0701 05:57:41.513000  6523 solver.cpp:228] Iteration 8550, loss = 0.150547
I0701 05:57:41.513109  6523 solver.cpp:244]     Train net output #0: loss = 0.150547 (* 1 = 0.150547 loss)
I0701 05:57:41.513125  6523 sgd_solver.cpp:106] Iteration 8550, lr = 0.000629132
I0701 06:03:33.442848  6523 solver.cpp:228] Iteration 8575, loss = 0.485474
I0701 06:03:33.444569  6523 solver.cpp:244]     Train net output #0: loss = 0.485473 (* 1 = 0.485473 loss)
I0701 06:03:33.444582  6523 sgd_solver.cpp:106] Iteration 8575, lr = 0.000628497
I0701 06:09:28.185000  6523 solver.cpp:228] Iteration 8600, loss = 0.0984342
I0701 06:09:28.185359  6523 solver.cpp:244]     Train net output #0: loss = 0.0984339 (* 1 = 0.0984339 loss)
I0701 06:09:28.185374  6523 sgd_solver.cpp:106] Iteration 8600, lr = 0.000627864
I0701 06:15:07.893504  6523 solver.cpp:228] Iteration 8625, loss = 0.0446928
I0701 06:15:07.894619  6523 solver.cpp:244]     Train net output #0: loss = 0.0446925 (* 1 = 0.0446925 loss)
I0701 06:15:07.894634  6523 sgd_solver.cpp:106] Iteration 8625, lr = 0.000627231
I0701 06:20:53.643718  6523 solver.cpp:228] Iteration 8650, loss = 0.106461
I0701 06:20:53.643827  6523 solver.cpp:244]     Train net output #0: loss = 0.10646 (* 1 = 0.10646 loss)
I0701 06:20:53.643839  6523 sgd_solver.cpp:106] Iteration 8650, lr = 0.000626601
I0701 06:26:48.432890  6523 solver.cpp:228] Iteration 8675, loss = 0.0627742
I0701 06:26:48.434602  6523 solver.cpp:244]     Train net output #0: loss = 0.0627739 (* 1 = 0.0627739 loss)
I0701 06:26:48.434613  6523 sgd_solver.cpp:106] Iteration 8675, lr = 0.000625971
I0701 06:32:29.796603  6523 solver.cpp:228] Iteration 8700, loss = 0.411548
I0701 06:32:29.797246  6523 solver.cpp:244]     Train net output #0: loss = 0.411547 (* 1 = 0.411547 loss)
I0701 06:32:29.797258  6523 sgd_solver.cpp:106] Iteration 8700, lr = 0.000625344
I0701 06:38:04.932992  6523 solver.cpp:228] Iteration 8725, loss = 0.0737174
I0701 06:38:04.933120  6523 solver.cpp:244]     Train net output #0: loss = 0.0737171 (* 1 = 0.0737171 loss)
I0701 06:38:04.933131  6523 sgd_solver.cpp:106] Iteration 8725, lr = 0.000624718
I0701 06:43:38.454802  6523 solver.cpp:228] Iteration 8750, loss = 0.0778646
I0701 06:43:38.454926  6523 solver.cpp:244]     Train net output #0: loss = 0.0778643 (* 1 = 0.0778643 loss)
I0701 06:43:38.454942  6523 sgd_solver.cpp:106] Iteration 8750, lr = 0.000624093
I0701 06:49:08.056195  6523 solver.cpp:228] Iteration 8775, loss = 0.136194
I0701 06:49:08.058215  6523 solver.cpp:244]     Train net output #0: loss = 0.136193 (* 1 = 0.136193 loss)
I0701 06:49:08.058226  6523 sgd_solver.cpp:106] Iteration 8775, lr = 0.000623469
I0701 06:54:40.956785  6523 solver.cpp:228] Iteration 8800, loss = 0.212907
I0701 06:54:40.956887  6523 solver.cpp:244]     Train net output #0: loss = 0.212907 (* 1 = 0.212907 loss)
I0701 06:54:40.956897  6523 sgd_solver.cpp:106] Iteration 8800, lr = 0.000622847
I0701 07:00:21.718495  6523 solver.cpp:228] Iteration 8825, loss = 0.260002
I0701 07:00:21.718601  6523 solver.cpp:244]     Train net output #0: loss = 0.260002 (* 1 = 0.260002 loss)
I0701 07:00:21.718617  6523 sgd_solver.cpp:106] Iteration 8825, lr = 0.000622227
I0701 07:06:01.445065  6523 solver.cpp:228] Iteration 8850, loss = 0.123981
I0701 07:06:01.445180  6523 solver.cpp:244]     Train net output #0: loss = 0.123981 (* 1 = 0.123981 loss)
I0701 07:06:01.445193  6523 sgd_solver.cpp:106] Iteration 8850, lr = 0.000621608
I0701 07:11:44.648888  6523 solver.cpp:228] Iteration 8875, loss = 0.0469633
I0701 07:11:44.649005  6523 solver.cpp:244]     Train net output #0: loss = 0.0469631 (* 1 = 0.0469631 loss)
I0701 07:11:44.649022  6523 sgd_solver.cpp:106] Iteration 8875, lr = 0.00062099
I0701 07:17:24.287575  6523 solver.cpp:228] Iteration 8900, loss = 0.069013
I0701 07:17:24.289118  6523 solver.cpp:244]     Train net output #0: loss = 0.0690128 (* 1 = 0.0690128 loss)
I0701 07:17:24.289129  6523 sgd_solver.cpp:106] Iteration 8900, lr = 0.000620374
I0701 07:23:01.264236  6523 solver.cpp:228] Iteration 8925, loss = 0.222119
I0701 07:23:01.264350  6523 solver.cpp:244]     Train net output #0: loss = 0.222119 (* 1 = 0.222119 loss)
I0701 07:23:01.264363  6523 sgd_solver.cpp:106] Iteration 8925, lr = 0.000619759
I0701 07:28:31.639824  6523 solver.cpp:228] Iteration 8950, loss = 0.247421
I0701 07:28:31.639942  6523 solver.cpp:244]     Train net output #0: loss = 0.247421 (* 1 = 0.247421 loss)
I0701 07:28:31.639955  6523 sgd_solver.cpp:106] Iteration 8950, lr = 0.000619146
I0701 07:34:00.659184  6523 solver.cpp:228] Iteration 8975, loss = 0.183251
I0701 07:34:00.659291  6523 solver.cpp:244]     Train net output #0: loss = 0.183251 (* 1 = 0.183251 loss)
I0701 07:34:00.659302  6523 sgd_solver.cpp:106] Iteration 8975, lr = 0.000618534
I0701 07:39:14.431746  6523 solver.cpp:454] Snapshotting to binary proto file fish_net_pretrain_output_iter_9000.caffemodel
I0701 07:39:14.432268  6523 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_pretrain_output_iter_9000.solverstate
I0701 07:39:14.432534  6523 solver.cpp:337] Iteration 9000, Testing net (#0)
I0701 07:39:14.432545  6523 net.cpp:684] Ignoring source layer training_cells
I0701 07:39:14.432552  6523 net.cpp:684] Ignoring source layer drop1
I0701 07:40:24.210717  6523 solver.cpp:404]     Test net output #0: accuracy = 0.963281
I0701 07:40:24.211539  6523 solver.cpp:404]     Test net output #1: loss = 0.115271 (* 1 = 0.115271 loss)
I0701 07:40:37.631836  6523 solver.cpp:228] Iteration 9000, loss = 0.0772994
I0701 07:40:37.631903  6523 solver.cpp:244]     Train net output #0: loss = 0.0772992 (* 1 = 0.0772992 loss)
I0701 07:40:37.631925  6523 sgd_solver.cpp:106] Iteration 9000, lr = 0.000617924
I0701 07:46:11.557569  6523 solver.cpp:228] Iteration 9025, loss = 0.0880602
I0701 07:46:11.557675  6523 solver.cpp:244]     Train net output #0: loss = 0.08806 (* 1 = 0.08806 loss)
I0701 07:46:11.557687  6523 sgd_solver.cpp:106] Iteration 9025, lr = 0.000617315
I0701 07:51:40.279745  6523 solver.cpp:228] Iteration 9050, loss = 0.13984
I0701 07:51:40.279871  6523 solver.cpp:244]     Train net output #0: loss = 0.13984 (* 1 = 0.13984 loss)
I0701 07:51:40.279883  6523 sgd_solver.cpp:106] Iteration 9050, lr = 0.000616707
I0701 07:57:05.921428  6523 solver.cpp:228] Iteration 9075, loss = 0.149526
I0701 07:57:05.921932  6523 solver.cpp:244]     Train net output #0: loss = 0.149526 (* 1 = 0.149526 loss)
I0701 07:57:05.921944  6523 sgd_solver.cpp:106] Iteration 9075, lr = 0.000616101
I0701 08:02:29.259955  6523 solver.cpp:228] Iteration 9100, loss = 0.147741
I0701 08:02:29.260072  6523 solver.cpp:244]     Train net output #0: loss = 0.147741 (* 1 = 0.147741 loss)
I0701 08:02:29.260088  6523 sgd_solver.cpp:106] Iteration 9100, lr = 0.000615496
I0701 08:07:54.661250  6523 solver.cpp:228] Iteration 9125, loss = 0.0747077
I0701 08:07:54.661366  6523 solver.cpp:244]     Train net output #0: loss = 0.0747075 (* 1 = 0.0747075 loss)
I0701 08:07:54.661380  6523 sgd_solver.cpp:106] Iteration 9125, lr = 0.000614892
I0701 08:13:25.468541  6523 solver.cpp:228] Iteration 9150, loss = 0.0956225
I0701 08:13:25.468648  6523 solver.cpp:244]     Train net output #0: loss = 0.0956222 (* 1 = 0.0956222 loss)
I0701 08:13:25.468660  6523 sgd_solver.cpp:106] Iteration 9150, lr = 0.00061429
I0701 08:18:56.551236  6523 solver.cpp:228] Iteration 9175, loss = 0.06189
I0701 08:18:56.554253  6523 solver.cpp:244]     Train net output #0: loss = 0.0618897 (* 1 = 0.0618897 loss)
I0701 08:18:56.554268  6523 sgd_solver.cpp:106] Iteration 9175, lr = 0.000613689
I0701 08:24:25.813433  6523 solver.cpp:228] Iteration 9200, loss = 0.117389
I0701 08:24:25.813539  6523 solver.cpp:244]     Train net output #0: loss = 0.117389 (* 1 = 0.117389 loss)
I0701 08:24:25.813550  6523 sgd_solver.cpp:106] Iteration 9200, lr = 0.00061309
I0701 08:29:53.484745  6523 solver.cpp:228] Iteration 9225, loss = 0.124387
I0701 08:29:53.484863  6523 solver.cpp:244]     Train net output #0: loss = 0.124386 (* 1 = 0.124386 loss)
I0701 08:29:53.484875  6523 sgd_solver.cpp:106] Iteration 9225, lr = 0.000612492
I0701 08:35:11.030241  6523 solver.cpp:228] Iteration 9250, loss = 0.426399
I0701 08:35:11.031710  6523 solver.cpp:244]     Train net output #0: loss = 0.426399 (* 1 = 0.426399 loss)
I0701 08:35:11.031721  6523 sgd_solver.cpp:106] Iteration 9250, lr = 0.000611895
I0701 08:40:43.280730  6523 solver.cpp:228] Iteration 9275, loss = 0.179722
I0701 08:40:43.280856  6523 solver.cpp:244]     Train net output #0: loss = 0.179722 (* 1 = 0.179722 loss)
I0701 08:40:43.280870  6523 sgd_solver.cpp:106] Iteration 9275, lr = 0.0006113
I0701 08:46:16.553264  6523 solver.cpp:228] Iteration 9300, loss = 0.191263
I0701 08:46:16.553412  6523 solver.cpp:244]     Train net output #0: loss = 0.191262 (* 1 = 0.191262 loss)
I0701 08:46:16.553426  6523 sgd_solver.cpp:106] Iteration 9300, lr = 0.000610706
I0701 08:51:47.464617  6523 solver.cpp:228] Iteration 9325, loss = 0.089828
I0701 08:51:47.464731  6523 solver.cpp:244]     Train net output #0: loss = 0.0898277 (* 1 = 0.0898277 loss)
I0701 08:51:47.464743  6523 sgd_solver.cpp:106] Iteration 9325, lr = 0.000610113
I0701 08:57:18.234473  6523 solver.cpp:228] Iteration 9350, loss = 0.0615481
I0701 08:57:18.236534  6523 solver.cpp:244]     Train net output #0: loss = 0.0615479 (* 1 = 0.0615479 loss)
I0701 08:57:18.236546  6523 sgd_solver.cpp:106] Iteration 9350, lr = 0.000609522
I0701 09:02:37.985363  6523 solver.cpp:228] Iteration 9375, loss = 0.129225
I0701 09:02:37.985476  6523 solver.cpp:244]     Train net output #0: loss = 0.129224 (* 1 = 0.129224 loss)
I0701 09:02:37.985488  6523 sgd_solver.cpp:106] Iteration 9375, lr = 0.000608932
I0701 09:08:09.237464  6523 solver.cpp:228] Iteration 9400, loss = 0.118004
I0701 09:08:09.237571  6523 solver.cpp:244]     Train net output #0: loss = 0.118004 (* 1 = 0.118004 loss)
I0701 09:08:09.237583  6523 sgd_solver.cpp:106] Iteration 9400, lr = 0.000608343
I0701 09:13:34.984925  6523 solver.cpp:228] Iteration 9425, loss = 0.16344
I0701 09:13:34.986073  6523 solver.cpp:244]     Train net output #0: loss = 0.16344 (* 1 = 0.16344 loss)
I0701 09:13:34.986085  6523 sgd_solver.cpp:106] Iteration 9425, lr = 0.000607756
I0701 09:19:03.424387  6523 solver.cpp:228] Iteration 9450, loss = 0.0563631
I0701 09:19:03.426745  6523 solver.cpp:244]     Train net output #0: loss = 0.0563628 (* 1 = 0.0563628 loss)
I0701 09:19:03.426756  6523 sgd_solver.cpp:106] Iteration 9450, lr = 0.00060717
I0701 09:24:39.181231  6523 solver.cpp:228] Iteration 9475, loss = 0.149012
I0701 09:24:39.181852  6523 solver.cpp:244]     Train net output #0: loss = 0.149011 (* 1 = 0.149011 loss)
I0701 09:24:39.181864  6523 sgd_solver.cpp:106] Iteration 9475, lr = 0.000606585
I0701 09:30:02.994498  6523 solver.cpp:337] Iteration 9500, Testing net (#0)
I0701 09:30:02.994596  6523 net.cpp:684] Ignoring source layer training_cells
I0701 09:30:02.994606  6523 net.cpp:684] Ignoring source layer drop1
I0701 09:31:13.545608  6523 solver.cpp:404]     Test net output #0: accuracy = 0.971094
I0701 09:31:13.545722  6523 solver.cpp:404]     Test net output #1: loss = 0.114821 (* 1 = 0.114821 loss)
I0701 09:31:26.534653  6523 solver.cpp:228] Iteration 9500, loss = 0.127481
I0701 09:31:26.534709  6523 solver.cpp:244]     Train net output #0: loss = 0.12748 (* 1 = 0.12748 loss)
I0701 09:31:26.534720  6523 sgd_solver.cpp:106] Iteration 9500, lr = 0.000606002
I0701 09:36:51.205065  6523 solver.cpp:228] Iteration 9525, loss = 0.105016
I0701 09:36:51.205173  6523 solver.cpp:244]     Train net output #0: loss = 0.105015 (* 1 = 0.105015 loss)
I0701 09:36:51.205185  6523 sgd_solver.cpp:106] Iteration 9525, lr = 0.00060542
I0701 09:42:26.398517  6523 solver.cpp:228] Iteration 9550, loss = 0.188991
I0701 09:42:26.399830  6523 solver.cpp:244]     Train net output #0: loss = 0.188991 (* 1 = 0.188991 loss)
I0701 09:42:26.399842  6523 sgd_solver.cpp:106] Iteration 9550, lr = 0.000604839
I0701 09:47:54.138514  6523 solver.cpp:228] Iteration 9575, loss = 0.197774
I0701 09:47:54.138631  6523 solver.cpp:244]     Train net output #0: loss = 0.197774 (* 1 = 0.197774 loss)
I0701 09:47:54.138643  6523 sgd_solver.cpp:106] Iteration 9575, lr = 0.00060426
I0701 09:53:30.691706  6523 solver.cpp:228] Iteration 9600, loss = 0.136573
I0701 09:53:30.691859  6523 solver.cpp:244]     Train net output #0: loss = 0.136573 (* 1 = 0.136573 loss)
I0701 09:53:30.691874  6523 sgd_solver.cpp:106] Iteration 9600, lr = 0.000603682
I0701 09:59:10.362375  6523 solver.cpp:228] Iteration 9625, loss = 0.113371
I0701 09:59:10.362483  6523 solver.cpp:244]     Train net output #0: loss = 0.11337 (* 1 = 0.11337 loss)
I0701 09:59:10.362495  6523 sgd_solver.cpp:106] Iteration 9625, lr = 0.000603105
I0701 10:04:46.513061  6523 solver.cpp:228] Iteration 9650, loss = 0.17644
I0701 10:04:46.515040  6523 solver.cpp:244]     Train net output #0: loss = 0.176439 (* 1 = 0.176439 loss)
I0701 10:04:46.515053  6523 sgd_solver.cpp:106] Iteration 9650, lr = 0.000602529
I0701 10:10:14.870113  6523 solver.cpp:228] Iteration 9675, loss = 0.135486
I0701 10:10:14.870226  6523 solver.cpp:244]     Train net output #0: loss = 0.135486 (* 1 = 0.135486 loss)
I0701 10:10:14.870244  6523 sgd_solver.cpp:106] Iteration 9675, lr = 0.000601955
I0701 10:15:44.858130  6523 solver.cpp:228] Iteration 9700, loss = 0.0826902
I0701 10:15:44.858232  6523 solver.cpp:244]     Train net output #0: loss = 0.0826899 (* 1 = 0.0826899 loss)
I0701 10:15:44.858244  6523 sgd_solver.cpp:106] Iteration 9700, lr = 0.000601382
I0701 10:21:17.111402  6523 solver.cpp:228] Iteration 9725, loss = 0.0829782
I0701 10:21:17.111515  6523 solver.cpp:244]     Train net output #0: loss = 0.082978 (* 1 = 0.082978 loss)
I0701 10:21:17.111526  6523 sgd_solver.cpp:106] Iteration 9725, lr = 0.00060081
I0701 10:27:08.088673  6523 solver.cpp:228] Iteration 9750, loss = 0.471227
I0701 10:27:08.088843  6523 solver.cpp:244]     Train net output #0: loss = 0.471227 (* 1 = 0.471227 loss)
I0701 10:27:08.088860  6523 sgd_solver.cpp:106] Iteration 9750, lr = 0.00060024
I0701 10:32:48.409498  6523 solver.cpp:228] Iteration 9775, loss = 0.13069
I0701 10:32:48.410892  6523 solver.cpp:244]     Train net output #0: loss = 0.13069 (* 1 = 0.13069 loss)
I0701 10:32:48.410904  6523 sgd_solver.cpp:106] Iteration 9775, lr = 0.00059967
I0701 10:38:21.470710  6523 solver.cpp:228] Iteration 9800, loss = 0.192514
I0701 10:38:21.470823  6523 solver.cpp:244]     Train net output #0: loss = 0.192513 (* 1 = 0.192513 loss)
I0701 10:38:21.470839  6523 sgd_solver.cpp:106] Iteration 9800, lr = 0.000599102
I0701 10:43:43.862068  6523 solver.cpp:228] Iteration 9825, loss = 0.154238
I0701 10:43:43.862184  6523 solver.cpp:244]     Train net output #0: loss = 0.154238 (* 1 = 0.154238 loss)
I0701 10:43:43.862200  6523 sgd_solver.cpp:106] Iteration 9825, lr = 0.000598536
I0701 10:49:15.947247  6523 solver.cpp:228] Iteration 9850, loss = 0.0699944
I0701 10:49:15.948593  6523 solver.cpp:244]     Train net output #0: loss = 0.0699942 (* 1 = 0.0699942 loss)
I0701 10:49:15.948606  6523 sgd_solver.cpp:106] Iteration 9850, lr = 0.00059797
I0701 10:54:48.419237  6523 solver.cpp:228] Iteration 9875, loss = 0.0691342
I0701 10:54:48.419684  6523 solver.cpp:244]     Train net output #0: loss = 0.069134 (* 1 = 0.069134 loss)
I0701 10:54:48.419697  6523 sgd_solver.cpp:106] Iteration 9875, lr = 0.000597406
I0701 11:00:21.011009  6523 solver.cpp:228] Iteration 9900, loss = 0.114784
I0701 11:00:21.011116  6523 solver.cpp:244]     Train net output #0: loss = 0.114783 (* 1 = 0.114783 loss)
I0701 11:00:21.011127  6523 sgd_solver.cpp:106] Iteration 9900, lr = 0.000596843
I0701 11:05:49.383404  6523 solver.cpp:228] Iteration 9925, loss = 0.148427
I0701 11:05:49.383513  6523 solver.cpp:244]     Train net output #0: loss = 0.148427 (* 1 = 0.148427 loss)
I0701 11:05:49.383525  6523 sgd_solver.cpp:106] Iteration 9925, lr = 0.000596281
I0701 11:11:13.946995  6523 solver.cpp:228] Iteration 9950, loss = 0.0560093
I0701 11:11:13.948698  6523 solver.cpp:244]     Train net output #0: loss = 0.0560091 (* 1 = 0.0560091 loss)
I0701 11:11:13.948714  6523 sgd_solver.cpp:106] Iteration 9950, lr = 0.000595721
I0701 11:16:39.778116  6523 solver.cpp:228] Iteration 9975, loss = 0.0891975
I0701 11:16:39.780212  6523 solver.cpp:244]     Train net output #0: loss = 0.0891973 (* 1 = 0.0891973 loss)
I0701 11:16:39.780225  6523 sgd_solver.cpp:106] Iteration 9975, lr = 0.000595162
I0701 11:21:51.768688  6523 solver.cpp:454] Snapshotting to binary proto file fish_net_pretrain_output_iter_10000.caffemodel
I0701 11:21:51.769217  6523 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_pretrain_output_iter_10000.solverstate
I0701 11:21:59.535204  6523 solver.cpp:317] Iteration 10000, loss = 0.110757
I0701 11:21:59.535902  6523 solver.cpp:337] Iteration 10000, Testing net (#0)
I0701 11:21:59.535956  6523 net.cpp:684] Ignoring source layer training_cells
I0701 11:21:59.535987  6523 net.cpp:684] Ignoring source layer drop1
I0701 11:23:17.202388  6523 solver.cpp:404]     Test net output #0: accuracy = 0.962109
I0701 11:23:17.202497  6523 solver.cpp:404]     Test net output #1: loss = 0.107828 (* 1 = 0.107828 loss)
I0701 11:23:17.202508  6523 solver.cpp:322] Optimization Done.
I0701 11:23:17.202515  6523 caffe.cpp:222] Optimization Done.
