Log file created at: 2016/07/07 09:39:20
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0707 09:39:20.467104  1460 caffe.cpp:185] Using GPUs 0
I0707 09:39:20.546242  1460 caffe.cpp:190] GPU 0: GeForce GTX 760
I0707 09:39:20.766060  1460 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 1000
base_lr: 0.001
display: 25
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.4
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "fish_net_memory_map_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt"
I0707 09:39:20.766258  1460 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0707 09:39:20.766578  1460 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0707 09:39:20.766597  1460 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0707 09:39:20.766669  1460 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'train\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0707 09:39:20.766968  1460 layer_factory.hpp:77] Creating layer training_cells
I0707 09:39:21.527240  1460 net.cpp:91] Creating Layer training_cells
I0707 09:39:21.528967  1460 net.cpp:399] training_cells -> image
I0707 09:39:21.528996  1460 net.cpp:399] training_cells -> label
I0707 09:41:01.423861  1460 net.cpp:141] Setting up training_cells
I0707 09:41:01.423938  1460 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0707 09:41:01.423949  1460 net.cpp:148] Top shape: 512 (512)
I0707 09:41:01.423954  1460 net.cpp:156] Memory required for data: 90937344
I0707 09:41:01.423964  1460 layer_factory.hpp:77] Creating layer conv1
I0707 09:41:01.423985  1460 net.cpp:91] Creating Layer conv1
I0707 09:41:01.423991  1460 net.cpp:425] conv1 <- image
I0707 09:41:01.424002  1460 net.cpp:399] conv1 -> conv1
I0707 09:41:01.426524  1460 net.cpp:141] Setting up conv1
I0707 09:41:01.426539  1460 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0707 09:41:01.426545  1460 net.cpp:156] Memory required for data: 650809344
I0707 09:41:01.426559  1460 layer_factory.hpp:77] Creating layer pool1
I0707 09:41:01.426568  1460 net.cpp:91] Creating Layer pool1
I0707 09:41:01.426573  1460 net.cpp:425] pool1 <- conv1
I0707 09:41:01.426578  1460 net.cpp:399] pool1 -> pool1
I0707 09:41:01.426614  1460 net.cpp:141] Setting up pool1
I0707 09:41:01.426620  1460 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0707 09:41:01.426625  1460 net.cpp:156] Memory required for data: 673204224
I0707 09:41:01.426630  1460 layer_factory.hpp:77] Creating layer conv2
I0707 09:41:01.426637  1460 net.cpp:91] Creating Layer conv2
I0707 09:41:01.426642  1460 net.cpp:425] conv2 <- pool1
I0707 09:41:01.426648  1460 net.cpp:399] conv2 -> conv2
I0707 09:41:01.427392  1460 net.cpp:141] Setting up conv2
I0707 09:41:01.427407  1460 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0707 09:41:01.427412  1460 net.cpp:156] Memory required for data: 677720064
I0707 09:41:01.427420  1460 layer_factory.hpp:77] Creating layer pool2
I0707 09:41:01.427428  1460 net.cpp:91] Creating Layer pool2
I0707 09:41:01.427443  1460 net.cpp:425] pool2 <- conv2
I0707 09:41:01.427449  1460 net.cpp:399] pool2 -> pool2
I0707 09:41:01.427476  1460 net.cpp:141] Setting up pool2
I0707 09:41:01.427484  1460 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0707 09:41:01.427487  1460 net.cpp:156] Memory required for data: 678221824
I0707 09:41:01.427492  1460 layer_factory.hpp:77] Creating layer ip1
I0707 09:41:01.427500  1460 net.cpp:91] Creating Layer ip1
I0707 09:41:01.427505  1460 net.cpp:425] ip1 <- pool2
I0707 09:41:01.427511  1460 net.cpp:399] ip1 -> ip1
I0707 09:41:01.427620  1460 net.cpp:141] Setting up ip1
I0707 09:41:01.427628  1460 net.cpp:148] Top shape: 512 32 (16384)
I0707 09:41:01.427634  1460 net.cpp:156] Memory required for data: 678287360
I0707 09:41:01.427640  1460 layer_factory.hpp:77] Creating layer relu1
I0707 09:41:01.427647  1460 net.cpp:91] Creating Layer relu1
I0707 09:41:01.427652  1460 net.cpp:425] relu1 <- ip1
I0707 09:41:01.427657  1460 net.cpp:386] relu1 -> ip1 (in-place)
I0707 09:41:01.427665  1460 net.cpp:141] Setting up relu1
I0707 09:41:01.427670  1460 net.cpp:148] Top shape: 512 32 (16384)
I0707 09:41:01.427675  1460 net.cpp:156] Memory required for data: 678352896
I0707 09:41:01.427678  1460 layer_factory.hpp:77] Creating layer drop1
I0707 09:41:01.427693  1460 net.cpp:91] Creating Layer drop1
I0707 09:41:01.427698  1460 net.cpp:425] drop1 <- ip1
I0707 09:41:01.427705  1460 net.cpp:386] drop1 -> ip1 (in-place)
I0707 09:41:01.427723  1460 net.cpp:141] Setting up drop1
I0707 09:41:01.427729  1460 net.cpp:148] Top shape: 512 32 (16384)
I0707 09:41:01.427734  1460 net.cpp:156] Memory required for data: 678418432
I0707 09:41:01.427738  1460 layer_factory.hpp:77] Creating layer ip2
I0707 09:41:01.427747  1460 net.cpp:91] Creating Layer ip2
I0707 09:41:01.427752  1460 net.cpp:425] ip2 <- ip1
I0707 09:41:01.427758  1460 net.cpp:399] ip2 -> ip2
I0707 09:41:01.427814  1460 net.cpp:141] Setting up ip2
I0707 09:41:01.427820  1460 net.cpp:148] Top shape: 512 2 (1024)
I0707 09:41:01.427825  1460 net.cpp:156] Memory required for data: 678422528
I0707 09:41:01.427831  1460 layer_factory.hpp:77] Creating layer loss
I0707 09:41:01.427839  1460 net.cpp:91] Creating Layer loss
I0707 09:41:01.427842  1460 net.cpp:425] loss <- ip2
I0707 09:41:01.427847  1460 net.cpp:425] loss <- label
I0707 09:41:01.427853  1460 net.cpp:399] loss -> loss
I0707 09:41:01.427865  1460 layer_factory.hpp:77] Creating layer loss
I0707 09:41:01.427952  1460 net.cpp:141] Setting up loss
I0707 09:41:01.427960  1460 net.cpp:148] Top shape: (1)
I0707 09:41:01.427964  1460 net.cpp:151]     with loss weight 1
I0707 09:41:01.427981  1460 net.cpp:156] Memory required for data: 678422532
I0707 09:41:01.427985  1460 net.cpp:217] loss needs backward computation.
I0707 09:41:01.427990  1460 net.cpp:217] ip2 needs backward computation.
I0707 09:41:01.427995  1460 net.cpp:217] drop1 needs backward computation.
I0707 09:41:01.427999  1460 net.cpp:217] relu1 needs backward computation.
I0707 09:41:01.428012  1460 net.cpp:217] ip1 needs backward computation.
I0707 09:41:01.428016  1460 net.cpp:217] pool2 needs backward computation.
I0707 09:41:01.428020  1460 net.cpp:217] conv2 needs backward computation.
I0707 09:41:01.428025  1460 net.cpp:217] pool1 needs backward computation.
I0707 09:41:01.428030  1460 net.cpp:217] conv1 needs backward computation.
I0707 09:41:01.428035  1460 net.cpp:219] training_cells does not need backward computation.
I0707 09:41:01.428038  1460 net.cpp:261] This network produces output loss
I0707 09:41:01.428047  1460 net.cpp:274] Network initialization done.
I0707 09:41:01.428349  1460 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0707 09:41:01.428375  1460 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0707 09:41:01.428386  1460 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer drop1
I0707 09:41:01.428457  1460 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'test\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0707 09:41:01.428762  1460 layer_factory.hpp:77] Creating layer testing_cells
I0707 09:41:01.428812  1460 net.cpp:91] Creating Layer testing_cells
I0707 09:41:01.428822  1460 net.cpp:399] testing_cells -> image
I0707 09:41:01.428856  1460 net.cpp:399] testing_cells -> label
I0707 09:41:49.406829  1460 net.cpp:141] Setting up testing_cells
I0707 09:41:49.406931  1460 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0707 09:41:49.406939  1460 net.cpp:148] Top shape: 512 (512)
I0707 09:41:49.406944  1460 net.cpp:156] Memory required for data: 90937344
I0707 09:41:49.406951  1460 layer_factory.hpp:77] Creating layer label_testing_cells_1_split
I0707 09:41:49.406965  1460 net.cpp:91] Creating Layer label_testing_cells_1_split
I0707 09:41:49.406971  1460 net.cpp:425] label_testing_cells_1_split <- label
I0707 09:41:49.406980  1460 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_0
I0707 09:41:49.406991  1460 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_1
I0707 09:41:49.407021  1460 net.cpp:141] Setting up label_testing_cells_1_split
I0707 09:41:49.407027  1460 net.cpp:148] Top shape: 512 (512)
I0707 09:41:49.407033  1460 net.cpp:148] Top shape: 512 (512)
I0707 09:41:49.407037  1460 net.cpp:156] Memory required for data: 90941440
I0707 09:41:49.407042  1460 layer_factory.hpp:77] Creating layer conv1
I0707 09:41:49.407055  1460 net.cpp:91] Creating Layer conv1
I0707 09:41:49.407060  1460 net.cpp:425] conv1 <- image
I0707 09:41:49.407068  1460 net.cpp:399] conv1 -> conv1
I0707 09:41:49.407269  1460 net.cpp:141] Setting up conv1
I0707 09:41:49.407289  1460 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0707 09:41:49.407294  1460 net.cpp:156] Memory required for data: 650813440
I0707 09:41:49.407305  1460 layer_factory.hpp:77] Creating layer pool1
I0707 09:41:49.407313  1460 net.cpp:91] Creating Layer pool1
I0707 09:41:49.407318  1460 net.cpp:425] pool1 <- conv1
I0707 09:41:49.407325  1460 net.cpp:399] pool1 -> pool1
I0707 09:41:49.407351  1460 net.cpp:141] Setting up pool1
I0707 09:41:49.407358  1460 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0707 09:41:49.407363  1460 net.cpp:156] Memory required for data: 673208320
I0707 09:41:49.407367  1460 layer_factory.hpp:77] Creating layer conv2
I0707 09:41:49.407377  1460 net.cpp:91] Creating Layer conv2
I0707 09:41:49.407382  1460 net.cpp:425] conv2 <- pool1
I0707 09:41:49.407387  1460 net.cpp:399] conv2 -> conv2
I0707 09:41:49.407541  1460 net.cpp:141] Setting up conv2
I0707 09:41:49.407549  1460 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0707 09:41:49.407564  1460 net.cpp:156] Memory required for data: 677724160
I0707 09:41:49.407572  1460 layer_factory.hpp:77] Creating layer pool2
I0707 09:41:49.407579  1460 net.cpp:91] Creating Layer pool2
I0707 09:41:49.407584  1460 net.cpp:425] pool2 <- conv2
I0707 09:41:49.407589  1460 net.cpp:399] pool2 -> pool2
I0707 09:41:49.407613  1460 net.cpp:141] Setting up pool2
I0707 09:41:49.407619  1460 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0707 09:41:49.407624  1460 net.cpp:156] Memory required for data: 678225920
I0707 09:41:49.407629  1460 layer_factory.hpp:77] Creating layer ip1
I0707 09:41:49.407637  1460 net.cpp:91] Creating Layer ip1
I0707 09:41:49.407641  1460 net.cpp:425] ip1 <- pool2
I0707 09:41:49.407647  1460 net.cpp:399] ip1 -> ip1
I0707 09:41:49.407781  1460 net.cpp:141] Setting up ip1
I0707 09:41:49.407789  1460 net.cpp:148] Top shape: 512 32 (16384)
I0707 09:41:49.407804  1460 net.cpp:156] Memory required for data: 678291456
I0707 09:41:49.407812  1460 layer_factory.hpp:77] Creating layer relu1
I0707 09:41:49.407819  1460 net.cpp:91] Creating Layer relu1
I0707 09:41:49.407824  1460 net.cpp:425] relu1 <- ip1
I0707 09:41:49.407829  1460 net.cpp:386] relu1 -> ip1 (in-place)
I0707 09:41:49.407835  1460 net.cpp:141] Setting up relu1
I0707 09:41:49.407840  1460 net.cpp:148] Top shape: 512 32 (16384)
I0707 09:41:49.407845  1460 net.cpp:156] Memory required for data: 678356992
I0707 09:41:49.407850  1460 layer_factory.hpp:77] Creating layer ip2
I0707 09:41:49.407857  1460 net.cpp:91] Creating Layer ip2
I0707 09:41:49.407861  1460 net.cpp:425] ip2 <- ip1
I0707 09:41:49.407867  1460 net.cpp:399] ip2 -> ip2
I0707 09:41:49.407930  1460 net.cpp:141] Setting up ip2
I0707 09:41:49.407938  1460 net.cpp:148] Top shape: 512 2 (1024)
I0707 09:41:49.407953  1460 net.cpp:156] Memory required for data: 678361088
I0707 09:41:49.407958  1460 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0707 09:41:49.407992  1460 net.cpp:91] Creating Layer ip2_ip2_0_split
I0707 09:41:49.407999  1460 net.cpp:425] ip2_ip2_0_split <- ip2
I0707 09:41:49.408004  1460 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0707 09:41:49.408011  1460 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0707 09:41:49.408033  1460 net.cpp:141] Setting up ip2_ip2_0_split
I0707 09:41:49.408041  1460 net.cpp:148] Top shape: 512 2 (1024)
I0707 09:41:49.408046  1460 net.cpp:148] Top shape: 512 2 (1024)
I0707 09:41:49.408051  1460 net.cpp:156] Memory required for data: 678369280
I0707 09:41:49.408056  1460 layer_factory.hpp:77] Creating layer accuracy
I0707 09:41:49.408068  1460 net.cpp:91] Creating Layer accuracy
I0707 09:41:49.408073  1460 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0707 09:41:49.408078  1460 net.cpp:425] accuracy <- label_testing_cells_1_split_0
I0707 09:41:49.408085  1460 net.cpp:399] accuracy -> accuracy
I0707 09:41:49.408093  1460 net.cpp:141] Setting up accuracy
I0707 09:41:49.408098  1460 net.cpp:148] Top shape: (1)
I0707 09:41:49.408103  1460 net.cpp:156] Memory required for data: 678369284
I0707 09:41:49.408107  1460 layer_factory.hpp:77] Creating layer loss
I0707 09:41:49.408113  1460 net.cpp:91] Creating Layer loss
I0707 09:41:49.408118  1460 net.cpp:425] loss <- ip2_ip2_0_split_1
I0707 09:41:49.408123  1460 net.cpp:425] loss <- label_testing_cells_1_split_1
I0707 09:41:49.408129  1460 net.cpp:399] loss -> loss
I0707 09:41:49.408138  1460 layer_factory.hpp:77] Creating layer loss
I0707 09:41:49.408197  1460 net.cpp:141] Setting up loss
I0707 09:41:49.408208  1460 net.cpp:148] Top shape: (1)
I0707 09:41:49.408223  1460 net.cpp:151]     with loss weight 1
I0707 09:41:49.408236  1460 net.cpp:156] Memory required for data: 678369288
I0707 09:41:49.408239  1460 net.cpp:217] loss needs backward computation.
I0707 09:41:49.408244  1460 net.cpp:219] accuracy does not need backward computation.
I0707 09:41:49.408251  1460 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0707 09:41:49.408257  1460 net.cpp:217] ip2 needs backward computation.
I0707 09:41:49.408265  1460 net.cpp:217] relu1 needs backward computation.
I0707 09:41:49.408273  1460 net.cpp:217] ip1 needs backward computation.
I0707 09:41:49.408282  1460 net.cpp:217] pool2 needs backward computation.
I0707 09:41:49.408288  1460 net.cpp:217] conv2 needs backward computation.
I0707 09:41:49.408295  1460 net.cpp:217] pool1 needs backward computation.
I0707 09:41:49.408303  1460 net.cpp:217] conv1 needs backward computation.
I0707 09:41:49.408311  1460 net.cpp:219] label_testing_cells_1_split does not need backward computation.
I0707 09:41:49.408321  1460 net.cpp:219] testing_cells does not need backward computation.
I0707 09:41:49.408329  1460 net.cpp:261] This network produces output accuracy
I0707 09:41:49.408336  1460 net.cpp:261] This network produces output loss
I0707 09:41:49.408352  1460 net.cpp:274] Network initialization done.
I0707 09:41:49.408423  1460 solver.cpp:60] Solver scaffolding done.
I0707 09:41:49.408613  1460 caffe.cpp:209] Resuming from fish_net_memory_map_output_iter_2551.solverstate
I0707 09:41:49.408888  1460 sgd_solver.cpp:318] SGDSolver: restoring history
I0707 09:41:49.408987  1460 caffe.cpp:219] Starting Optimization
I0707 09:41:49.408995  1460 solver.cpp:279] Solving fish_filter
I0707 09:41:49.408999  1460 solver.cpp:280] Learning Rate Policy: inv
I0707 09:45:59.286237  1460 solver.cpp:228] Iteration 2575, loss = 0.38474
I0707 09:45:59.286336  1460 solver.cpp:244]     Train net output #0: loss = 0.38474 (* 1 = 0.38474 loss)
I0707 09:45:59.286348  1460 sgd_solver.cpp:106] Iteration 2575, lr = 0.00084211
I0707 09:49:57.481901  1460 solver.cpp:228] Iteration 2600, loss = 0.376839
I0707 09:49:57.481973  1460 solver.cpp:244]     Train net output #0: loss = 0.376839 (* 1 = 0.376839 loss)
I0707 09:49:57.481986  1460 sgd_solver.cpp:106] Iteration 2600, lr = 0.000840857
I0707 09:54:01.094826  1460 solver.cpp:228] Iteration 2625, loss = 0.393973
I0707 09:54:01.094900  1460 solver.cpp:244]     Train net output #0: loss = 0.393973 (* 1 = 0.393973 loss)
I0707 09:54:01.094912  1460 sgd_solver.cpp:106] Iteration 2625, lr = 0.000839608
I0707 09:57:57.337119  1460 solver.cpp:228] Iteration 2650, loss = 0.432646
I0707 09:57:57.337260  1460 solver.cpp:244]     Train net output #0: loss = 0.432646 (* 1 = 0.432646 loss)
I0707 09:57:57.337273  1460 sgd_solver.cpp:106] Iteration 2650, lr = 0.000838363
I0707 10:01:53.753969  1460 solver.cpp:228] Iteration 2675, loss = 0.41115
I0707 10:01:53.754058  1460 solver.cpp:244]     Train net output #0: loss = 0.41115 (* 1 = 0.41115 loss)
I0707 10:01:53.754070  1460 sgd_solver.cpp:106] Iteration 2675, lr = 0.000837123
I0707 10:05:39.648717  1460 solver.cpp:228] Iteration 2700, loss = 0.358535
I0707 10:05:39.648803  1460 solver.cpp:244]     Train net output #0: loss = 0.358535 (* 1 = 0.358535 loss)
I0707 10:05:39.648815  1460 sgd_solver.cpp:106] Iteration 2700, lr = 0.000835886
I0707 10:09:25.252346  1460 solver.cpp:228] Iteration 2725, loss = 0.398262
I0707 10:09:25.252427  1460 solver.cpp:244]     Train net output #0: loss = 0.398262 (* 1 = 0.398262 loss)
I0707 10:09:25.252439  1460 sgd_solver.cpp:106] Iteration 2725, lr = 0.000834654
I0707 10:13:13.211364  1460 solver.cpp:228] Iteration 2750, loss = 0.41999
I0707 10:13:13.211453  1460 solver.cpp:244]     Train net output #0: loss = 0.41999 (* 1 = 0.41999 loss)
I0707 10:13:13.211465  1460 sgd_solver.cpp:106] Iteration 2750, lr = 0.000833427
I0707 10:17:10.255523  1460 solver.cpp:228] Iteration 2775, loss = 0.395887
I0707 10:17:10.255622  1460 solver.cpp:244]     Train net output #0: loss = 0.395887 (* 1 = 0.395887 loss)
I0707 10:17:10.255640  1460 sgd_solver.cpp:106] Iteration 2775, lr = 0.000832203
I0707 10:21:00.788803  1460 solver.cpp:228] Iteration 2800, loss = 0.427019
I0707 10:21:00.788887  1460 solver.cpp:244]     Train net output #0: loss = 0.427019 (* 1 = 0.427019 loss)
I0707 10:21:00.788902  1460 sgd_solver.cpp:106] Iteration 2800, lr = 0.000830984
I0707 10:25:00.542762  1460 solver.cpp:228] Iteration 2825, loss = 0.368454
I0707 10:25:00.542843  1460 solver.cpp:244]     Train net output #0: loss = 0.368454 (* 1 = 0.368454 loss)
I0707 10:25:00.542860  1460 sgd_solver.cpp:106] Iteration 2825, lr = 0.000829769
I0707 10:28:54.644131  1460 solver.cpp:228] Iteration 2850, loss = 0.385049
I0707 10:28:54.644220  1460 solver.cpp:244]     Train net output #0: loss = 0.385049 (* 1 = 0.385049 loss)
I0707 10:28:54.644237  1460 sgd_solver.cpp:106] Iteration 2850, lr = 0.000828558
I0707 10:32:39.241825  1460 solver.cpp:228] Iteration 2875, loss = 0.443182
I0707 10:32:39.241912  1460 solver.cpp:244]     Train net output #0: loss = 0.443182 (* 1 = 0.443182 loss)
I0707 10:32:39.241925  1460 sgd_solver.cpp:106] Iteration 2875, lr = 0.000827351
I0707 10:36:31.486665  1460 solver.cpp:228] Iteration 2900, loss = 0.362092
I0707 10:36:31.486742  1460 solver.cpp:244]     Train net output #0: loss = 0.362092 (* 1 = 0.362092 loss)
I0707 10:36:31.486753  1460 sgd_solver.cpp:106] Iteration 2900, lr = 0.000826148
I0707 10:40:18.201009  1460 solver.cpp:228] Iteration 2925, loss = 0.476497
I0707 10:40:18.201095  1460 solver.cpp:244]     Train net output #0: loss = 0.476497 (* 1 = 0.476497 loss)
I0707 10:40:18.201108  1460 sgd_solver.cpp:106] Iteration 2925, lr = 0.000824949
I0707 10:44:06.732183  1460 solver.cpp:228] Iteration 2950, loss = 0.426869
I0707 10:44:06.732272  1460 solver.cpp:244]     Train net output #0: loss = 0.426869 (* 1 = 0.426869 loss)
I0707 10:44:06.732285  1460 sgd_solver.cpp:106] Iteration 2950, lr = 0.000823754
I0707 10:47:55.293990  1460 solver.cpp:228] Iteration 2975, loss = 0.403838
I0707 10:47:55.294070  1460 solver.cpp:244]     Train net output #0: loss = 0.403838 (* 1 = 0.403838 loss)
I0707 10:47:55.294082  1460 sgd_solver.cpp:106] Iteration 2975, lr = 0.000822564
I0707 10:52:00.956966  1460 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_3000.caffemodel
I0707 10:52:01.995537  1460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_3000.solverstate
I0707 10:52:01.995916  1460 solver.cpp:337] Iteration 3000, Testing net (#0)
I0707 10:52:01.995944  1460 net.cpp:684] Ignoring source layer training_cells
I0707 10:52:01.996049  1460 net.cpp:684] Ignoring source layer drop1
I0707 11:05:15.033552  1460 solver.cpp:404]     Test net output #0: accuracy = 0.861787
I0707 11:05:15.033658  1460 solver.cpp:404]     Test net output #1: loss = 0.371017 (* 1 = 0.371017 loss)
I0707 11:05:26.815577  1460 solver.cpp:228] Iteration 3000, loss = 0.418579
I0707 11:05:26.815613  1460 solver.cpp:244]     Train net output #0: loss = 0.418579 (* 1 = 0.418579 loss)
I0707 11:05:26.815625  1460 sgd_solver.cpp:106] Iteration 3000, lr = 0.000821377
I0707 11:09:52.318722  1460 solver.cpp:228] Iteration 3025, loss = 0.498159
I0707 11:09:52.318805  1460 solver.cpp:244]     Train net output #0: loss = 0.498159 (* 1 = 0.498159 loss)
I0707 11:09:52.318819  1460 sgd_solver.cpp:106] Iteration 3025, lr = 0.000820194
I0707 11:14:21.275547  1460 solver.cpp:228] Iteration 3050, loss = 0.352425
I0707 11:14:21.275640  1460 solver.cpp:244]     Train net output #0: loss = 0.352425 (* 1 = 0.352425 loss)
I0707 11:14:21.275655  1460 sgd_solver.cpp:106] Iteration 3050, lr = 0.000819015
I0707 11:16:00.594214  1460 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_3060.caffemodel
I0707 11:16:01.627528  1460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_3060.solverstate
I0707 11:16:01.627887  1460 solver.cpp:301] Optimization stopped early.
I0707 11:16:01.627898  1460 caffe.cpp:222] Optimization Done.
