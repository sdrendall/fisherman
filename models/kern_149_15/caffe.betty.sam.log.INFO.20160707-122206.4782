Log file created at: 2016/07/07 12:22:06
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0707 12:22:06.855532  4782 caffe.cpp:185] Using GPUs 0
I0707 12:22:06.954308  4782 caffe.cpp:190] GPU 0: GeForce GTX 760
I0707 12:22:07.119422  4782 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 1000
base_lr: 0.001
display: 25
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.4
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "fish_net_memory_map_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt"
I0707 12:22:07.119632  4782 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0707 12:22:07.120163  4782 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0707 12:22:07.120193  4782 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0707 12:22:07.120321  4782 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'train\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0707 12:22:07.120748  4782 layer_factory.hpp:77] Creating layer training_cells
I0707 12:22:08.039312  4782 net.cpp:91] Creating Layer training_cells
I0707 12:22:08.039356  4782 net.cpp:399] training_cells -> image
I0707 12:22:08.039376  4782 net.cpp:399] training_cells -> label
I0707 12:23:51.824422  4782 net.cpp:141] Setting up training_cells
I0707 12:23:51.824512  4782 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0707 12:23:51.824519  4782 net.cpp:148] Top shape: 512 (512)
I0707 12:23:51.824524  4782 net.cpp:156] Memory required for data: 90937344
I0707 12:23:51.824535  4782 layer_factory.hpp:77] Creating layer conv1
I0707 12:23:51.824558  4782 net.cpp:91] Creating Layer conv1
I0707 12:23:51.824564  4782 net.cpp:425] conv1 <- image
I0707 12:23:51.824578  4782 net.cpp:399] conv1 -> conv1
I0707 12:23:51.825788  4782 net.cpp:141] Setting up conv1
I0707 12:23:51.825803  4782 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0707 12:23:51.825809  4782 net.cpp:156] Memory required for data: 650809344
I0707 12:23:51.825824  4782 layer_factory.hpp:77] Creating layer pool1
I0707 12:23:51.825834  4782 net.cpp:91] Creating Layer pool1
I0707 12:23:51.825839  4782 net.cpp:425] pool1 <- conv1
I0707 12:23:51.825845  4782 net.cpp:399] pool1 -> pool1
I0707 12:23:51.825882  4782 net.cpp:141] Setting up pool1
I0707 12:23:51.825889  4782 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0707 12:23:51.825894  4782 net.cpp:156] Memory required for data: 673204224
I0707 12:23:51.825898  4782 layer_factory.hpp:77] Creating layer conv2
I0707 12:23:51.825907  4782 net.cpp:91] Creating Layer conv2
I0707 12:23:51.825912  4782 net.cpp:425] conv2 <- pool1
I0707 12:23:51.825918  4782 net.cpp:399] conv2 -> conv2
I0707 12:23:51.828666  4782 net.cpp:141] Setting up conv2
I0707 12:23:51.828680  4782 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0707 12:23:51.828686  4782 net.cpp:156] Memory required for data: 677720064
I0707 12:23:51.828696  4782 layer_factory.hpp:77] Creating layer pool2
I0707 12:23:51.828703  4782 net.cpp:91] Creating Layer pool2
I0707 12:23:51.828708  4782 net.cpp:425] pool2 <- conv2
I0707 12:23:51.828714  4782 net.cpp:399] pool2 -> pool2
I0707 12:23:51.828742  4782 net.cpp:141] Setting up pool2
I0707 12:23:51.828750  4782 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0707 12:23:51.828754  4782 net.cpp:156] Memory required for data: 678221824
I0707 12:23:51.828758  4782 layer_factory.hpp:77] Creating layer ip1
I0707 12:23:51.828768  4782 net.cpp:91] Creating Layer ip1
I0707 12:23:51.828773  4782 net.cpp:425] ip1 <- pool2
I0707 12:23:51.828778  4782 net.cpp:399] ip1 -> ip1
I0707 12:23:51.828892  4782 net.cpp:141] Setting up ip1
I0707 12:23:51.828899  4782 net.cpp:148] Top shape: 512 32 (16384)
I0707 12:23:51.828904  4782 net.cpp:156] Memory required for data: 678287360
I0707 12:23:51.828912  4782 layer_factory.hpp:77] Creating layer relu1
I0707 12:23:51.828919  4782 net.cpp:91] Creating Layer relu1
I0707 12:23:51.828924  4782 net.cpp:425] relu1 <- ip1
I0707 12:23:51.828930  4782 net.cpp:386] relu1 -> ip1 (in-place)
I0707 12:23:51.828938  4782 net.cpp:141] Setting up relu1
I0707 12:23:51.828944  4782 net.cpp:148] Top shape: 512 32 (16384)
I0707 12:23:51.828951  4782 net.cpp:156] Memory required for data: 678352896
I0707 12:23:51.828959  4782 layer_factory.hpp:77] Creating layer drop1
I0707 12:23:51.828974  4782 net.cpp:91] Creating Layer drop1
I0707 12:23:51.828980  4782 net.cpp:425] drop1 <- ip1
I0707 12:23:51.828990  4782 net.cpp:386] drop1 -> ip1 (in-place)
I0707 12:23:51.829020  4782 net.cpp:141] Setting up drop1
I0707 12:23:51.829032  4782 net.cpp:148] Top shape: 512 32 (16384)
I0707 12:23:51.829041  4782 net.cpp:156] Memory required for data: 678418432
I0707 12:23:51.829048  4782 layer_factory.hpp:77] Creating layer ip2
I0707 12:23:51.829061  4782 net.cpp:91] Creating Layer ip2
I0707 12:23:51.829069  4782 net.cpp:425] ip2 <- ip1
I0707 12:23:51.829079  4782 net.cpp:399] ip2 -> ip2
I0707 12:23:51.829174  4782 net.cpp:141] Setting up ip2
I0707 12:23:51.829190  4782 net.cpp:148] Top shape: 512 2 (1024)
I0707 12:23:51.829197  4782 net.cpp:156] Memory required for data: 678422528
I0707 12:23:51.829208  4782 layer_factory.hpp:77] Creating layer loss
I0707 12:23:51.829221  4782 net.cpp:91] Creating Layer loss
I0707 12:23:51.829229  4782 net.cpp:425] loss <- ip2
I0707 12:23:51.829238  4782 net.cpp:425] loss <- label
I0707 12:23:51.829251  4782 net.cpp:399] loss -> loss
I0707 12:23:51.829268  4782 layer_factory.hpp:77] Creating layer loss
I0707 12:23:51.829381  4782 net.cpp:141] Setting up loss
I0707 12:23:51.829393  4782 net.cpp:148] Top shape: (1)
I0707 12:23:51.829401  4782 net.cpp:151]     with loss weight 1
I0707 12:23:51.829421  4782 net.cpp:156] Memory required for data: 678422532
I0707 12:23:51.829427  4782 net.cpp:217] loss needs backward computation.
I0707 12:23:51.829434  4782 net.cpp:217] ip2 needs backward computation.
I0707 12:23:51.829440  4782 net.cpp:217] drop1 needs backward computation.
I0707 12:23:51.829447  4782 net.cpp:217] relu1 needs backward computation.
I0707 12:23:51.829452  4782 net.cpp:217] ip1 needs backward computation.
I0707 12:23:51.829459  4782 net.cpp:217] pool2 needs backward computation.
I0707 12:23:51.829465  4782 net.cpp:217] conv2 needs backward computation.
I0707 12:23:51.829471  4782 net.cpp:217] pool1 needs backward computation.
I0707 12:23:51.829478  4782 net.cpp:217] conv1 needs backward computation.
I0707 12:23:51.829484  4782 net.cpp:219] training_cells does not need backward computation.
I0707 12:23:51.829491  4782 net.cpp:261] This network produces output loss
I0707 12:23:51.829504  4782 net.cpp:274] Network initialization done.
I0707 12:23:51.829938  4782 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0707 12:23:51.829970  4782 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0707 12:23:51.829989  4782 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer drop1
I0707 12:23:51.830102  4782 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'test\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0707 12:23:51.830482  4782 layer_factory.hpp:77] Creating layer testing_cells
I0707 12:23:51.830533  4782 net.cpp:91] Creating Layer testing_cells
I0707 12:23:51.830541  4782 net.cpp:399] testing_cells -> image
I0707 12:23:51.830576  4782 net.cpp:399] testing_cells -> label
I0707 12:24:42.337370  4782 net.cpp:141] Setting up testing_cells
I0707 12:24:42.337461  4782 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0707 12:24:42.337471  4782 net.cpp:148] Top shape: 512 (512)
I0707 12:24:42.337476  4782 net.cpp:156] Memory required for data: 90937344
I0707 12:24:42.337484  4782 layer_factory.hpp:77] Creating layer label_testing_cells_1_split
I0707 12:24:42.337503  4782 net.cpp:91] Creating Layer label_testing_cells_1_split
I0707 12:24:42.337509  4782 net.cpp:425] label_testing_cells_1_split <- label
I0707 12:24:42.337517  4782 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_0
I0707 12:24:42.337528  4782 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_1
I0707 12:24:42.337556  4782 net.cpp:141] Setting up label_testing_cells_1_split
I0707 12:24:42.337563  4782 net.cpp:148] Top shape: 512 (512)
I0707 12:24:42.337568  4782 net.cpp:148] Top shape: 512 (512)
I0707 12:24:42.337573  4782 net.cpp:156] Memory required for data: 90941440
I0707 12:24:42.337576  4782 layer_factory.hpp:77] Creating layer conv1
I0707 12:24:42.337589  4782 net.cpp:91] Creating Layer conv1
I0707 12:24:42.337594  4782 net.cpp:425] conv1 <- image
I0707 12:24:42.337601  4782 net.cpp:399] conv1 -> conv1
I0707 12:24:42.337795  4782 net.cpp:141] Setting up conv1
I0707 12:24:42.337803  4782 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0707 12:24:42.337808  4782 net.cpp:156] Memory required for data: 650813440
I0707 12:24:42.337818  4782 layer_factory.hpp:77] Creating layer pool1
I0707 12:24:42.337827  4782 net.cpp:91] Creating Layer pool1
I0707 12:24:42.337831  4782 net.cpp:425] pool1 <- conv1
I0707 12:24:42.337837  4782 net.cpp:399] pool1 -> pool1
I0707 12:24:42.337862  4782 net.cpp:141] Setting up pool1
I0707 12:24:42.337868  4782 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0707 12:24:42.337873  4782 net.cpp:156] Memory required for data: 673208320
I0707 12:24:42.337877  4782 layer_factory.hpp:77] Creating layer conv2
I0707 12:24:42.337887  4782 net.cpp:91] Creating Layer conv2
I0707 12:24:42.337891  4782 net.cpp:425] conv2 <- pool1
I0707 12:24:42.337898  4782 net.cpp:399] conv2 -> conv2
I0707 12:24:42.338043  4782 net.cpp:141] Setting up conv2
I0707 12:24:42.338050  4782 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0707 12:24:42.338055  4782 net.cpp:156] Memory required for data: 677724160
I0707 12:24:42.338063  4782 layer_factory.hpp:77] Creating layer pool2
I0707 12:24:42.338069  4782 net.cpp:91] Creating Layer pool2
I0707 12:24:42.338074  4782 net.cpp:425] pool2 <- conv2
I0707 12:24:42.338079  4782 net.cpp:399] pool2 -> pool2
I0707 12:24:42.338102  4782 net.cpp:141] Setting up pool2
I0707 12:24:42.338109  4782 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0707 12:24:42.338114  4782 net.cpp:156] Memory required for data: 678225920
I0707 12:24:42.338117  4782 layer_factory.hpp:77] Creating layer ip1
I0707 12:24:42.338125  4782 net.cpp:91] Creating Layer ip1
I0707 12:24:42.338129  4782 net.cpp:425] ip1 <- pool2
I0707 12:24:42.338135  4782 net.cpp:399] ip1 -> ip1
I0707 12:24:42.338281  4782 net.cpp:141] Setting up ip1
I0707 12:24:42.338289  4782 net.cpp:148] Top shape: 512 32 (16384)
I0707 12:24:42.338292  4782 net.cpp:156] Memory required for data: 678291456
I0707 12:24:42.338300  4782 layer_factory.hpp:77] Creating layer relu1
I0707 12:24:42.338307  4782 net.cpp:91] Creating Layer relu1
I0707 12:24:42.338311  4782 net.cpp:425] relu1 <- ip1
I0707 12:24:42.338317  4782 net.cpp:386] relu1 -> ip1 (in-place)
I0707 12:24:42.338323  4782 net.cpp:141] Setting up relu1
I0707 12:24:42.338328  4782 net.cpp:148] Top shape: 512 32 (16384)
I0707 12:24:42.338332  4782 net.cpp:156] Memory required for data: 678356992
I0707 12:24:42.338336  4782 layer_factory.hpp:77] Creating layer ip2
I0707 12:24:42.338345  4782 net.cpp:91] Creating Layer ip2
I0707 12:24:42.338348  4782 net.cpp:425] ip2 <- ip1
I0707 12:24:42.338354  4782 net.cpp:399] ip2 -> ip2
I0707 12:24:42.338407  4782 net.cpp:141] Setting up ip2
I0707 12:24:42.338413  4782 net.cpp:148] Top shape: 512 2 (1024)
I0707 12:24:42.338418  4782 net.cpp:156] Memory required for data: 678361088
I0707 12:24:42.338423  4782 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0707 12:24:42.338455  4782 net.cpp:91] Creating Layer ip2_ip2_0_split
I0707 12:24:42.338460  4782 net.cpp:425] ip2_ip2_0_split <- ip2
I0707 12:24:42.338466  4782 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0707 12:24:42.338472  4782 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0707 12:24:42.338495  4782 net.cpp:141] Setting up ip2_ip2_0_split
I0707 12:24:42.338500  4782 net.cpp:148] Top shape: 512 2 (1024)
I0707 12:24:42.338505  4782 net.cpp:148] Top shape: 512 2 (1024)
I0707 12:24:42.338510  4782 net.cpp:156] Memory required for data: 678369280
I0707 12:24:42.338515  4782 layer_factory.hpp:77] Creating layer accuracy
I0707 12:24:42.338523  4782 net.cpp:91] Creating Layer accuracy
I0707 12:24:42.338527  4782 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0707 12:24:42.338532  4782 net.cpp:425] accuracy <- label_testing_cells_1_split_0
I0707 12:24:42.338538  4782 net.cpp:399] accuracy -> accuracy
I0707 12:24:42.338546  4782 net.cpp:141] Setting up accuracy
I0707 12:24:42.338551  4782 net.cpp:148] Top shape: (1)
I0707 12:24:42.338556  4782 net.cpp:156] Memory required for data: 678369284
I0707 12:24:42.338559  4782 layer_factory.hpp:77] Creating layer loss
I0707 12:24:42.338565  4782 net.cpp:91] Creating Layer loss
I0707 12:24:42.338570  4782 net.cpp:425] loss <- ip2_ip2_0_split_1
I0707 12:24:42.338574  4782 net.cpp:425] loss <- label_testing_cells_1_split_1
I0707 12:24:42.338580  4782 net.cpp:399] loss -> loss
I0707 12:24:42.338587  4782 layer_factory.hpp:77] Creating layer loss
I0707 12:24:42.338634  4782 net.cpp:141] Setting up loss
I0707 12:24:42.338641  4782 net.cpp:148] Top shape: (1)
I0707 12:24:42.338656  4782 net.cpp:151]     with loss weight 1
I0707 12:24:42.338666  4782 net.cpp:156] Memory required for data: 678369288
I0707 12:24:42.338670  4782 net.cpp:217] loss needs backward computation.
I0707 12:24:42.338675  4782 net.cpp:219] accuracy does not need backward computation.
I0707 12:24:42.338680  4782 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0707 12:24:42.338693  4782 net.cpp:217] ip2 needs backward computation.
I0707 12:24:42.338698  4782 net.cpp:217] relu1 needs backward computation.
I0707 12:24:42.338702  4782 net.cpp:217] ip1 needs backward computation.
I0707 12:24:42.338706  4782 net.cpp:217] pool2 needs backward computation.
I0707 12:24:42.338711  4782 net.cpp:217] conv2 needs backward computation.
I0707 12:24:42.338726  4782 net.cpp:217] pool1 needs backward computation.
I0707 12:24:42.338729  4782 net.cpp:217] conv1 needs backward computation.
I0707 12:24:42.338734  4782 net.cpp:219] label_testing_cells_1_split does not need backward computation.
I0707 12:24:42.338739  4782 net.cpp:219] testing_cells does not need backward computation.
I0707 12:24:42.338743  4782 net.cpp:261] This network produces output accuracy
I0707 12:24:42.338757  4782 net.cpp:261] This network produces output loss
I0707 12:24:42.338767  4782 net.cpp:274] Network initialization done.
I0707 12:24:42.338829  4782 solver.cpp:60] Solver scaffolding done.
I0707 12:24:42.339002  4782 caffe.cpp:209] Resuming from fish_net_memory_map_output_iter_3412.solverstate
I0707 12:24:42.339278  4782 sgd_solver.cpp:318] SGDSolver: restoring history
I0707 12:24:42.339367  4782 caffe.cpp:219] Starting Optimization
I0707 12:24:42.339375  4782 solver.cpp:279] Solving fish_filter
I0707 12:24:42.339378  4782 solver.cpp:280] Learning Rate Policy: inv
I0707 12:26:39.280848  4782 solver.cpp:228] Iteration 3425, loss = 0.406564
I0707 12:26:39.280963  4782 solver.cpp:244]     Train net output #0: loss = 0.406564 (* 1 = 0.406564 loss)
I0707 12:26:39.280975  4782 sgd_solver.cpp:106] Iteration 3425, lr = 0.000801797
I0707 12:30:33.846295  4782 solver.cpp:228] Iteration 3450, loss = 0.3878
I0707 12:30:33.846379  4782 solver.cpp:244]     Train net output #0: loss = 0.3878 (* 1 = 0.3878 loss)
I0707 12:30:33.846395  4782 sgd_solver.cpp:106] Iteration 3450, lr = 0.000800679
I0707 12:34:17.735643  4782 solver.cpp:228] Iteration 3475, loss = 0.330135
I0707 12:34:17.735733  4782 solver.cpp:244]     Train net output #0: loss = 0.330135 (* 1 = 0.330135 loss)
I0707 12:34:17.735749  4782 sgd_solver.cpp:106] Iteration 3475, lr = 0.000799564
I0707 12:37:48.520778  4782 solver.cpp:228] Iteration 3500, loss = 0.490711
I0707 12:37:48.520920  4782 solver.cpp:244]     Train net output #0: loss = 0.490711 (* 1 = 0.490711 loss)
I0707 12:37:48.520931  4782 sgd_solver.cpp:106] Iteration 3500, lr = 0.000798454
I0707 12:41:29.307296  4782 solver.cpp:228] Iteration 3525, loss = 0.369451
I0707 12:41:29.307430  4782 solver.cpp:244]     Train net output #0: loss = 0.369451 (* 1 = 0.369451 loss)
I0707 12:41:29.307448  4782 sgd_solver.cpp:106] Iteration 3525, lr = 0.000797346
I0707 12:45:00.731387  4782 solver.cpp:228] Iteration 3550, loss = 0.455273
I0707 12:45:00.731485  4782 solver.cpp:244]     Train net output #0: loss = 0.455273 (* 1 = 0.455273 loss)
I0707 12:45:00.731497  4782 sgd_solver.cpp:106] Iteration 3550, lr = 0.000796243
I0707 12:48:28.020558  4782 solver.cpp:228] Iteration 3575, loss = 0.349433
I0707 12:48:28.020653  4782 solver.cpp:244]     Train net output #0: loss = 0.349433 (* 1 = 0.349433 loss)
I0707 12:48:28.020668  4782 sgd_solver.cpp:106] Iteration 3575, lr = 0.000795143
I0707 12:51:45.993163  4782 solver.cpp:228] Iteration 3600, loss = 0.378069
I0707 12:51:45.993271  4782 solver.cpp:244]     Train net output #0: loss = 0.378069 (* 1 = 0.378069 loss)
I0707 12:51:45.993285  4782 sgd_solver.cpp:106] Iteration 3600, lr = 0.000794046
I0707 12:55:04.780194  4782 solver.cpp:228] Iteration 3625, loss = 0.38091
I0707 12:55:04.780287  4782 solver.cpp:244]     Train net output #0: loss = 0.38091 (* 1 = 0.38091 loss)
I0707 12:55:04.780298  4782 sgd_solver.cpp:106] Iteration 3625, lr = 0.000792953
I0707 12:58:31.427244  4782 solver.cpp:228] Iteration 3650, loss = 0.342891
I0707 12:58:31.427332  4782 solver.cpp:244]     Train net output #0: loss = 0.342891 (* 1 = 0.342891 loss)
I0707 12:58:31.427345  4782 sgd_solver.cpp:106] Iteration 3650, lr = 0.000791864
I0707 13:01:50.479697  4782 solver.cpp:228] Iteration 3675, loss = 0.416559
I0707 13:01:50.479786  4782 solver.cpp:244]     Train net output #0: loss = 0.416559 (* 1 = 0.416559 loss)
I0707 13:01:50.479802  4782 sgd_solver.cpp:106] Iteration 3675, lr = 0.000790778
I0707 13:05:15.754091  4782 solver.cpp:228] Iteration 3700, loss = 0.323641
I0707 13:05:15.754189  4782 solver.cpp:244]     Train net output #0: loss = 0.323641 (* 1 = 0.323641 loss)
I0707 13:05:15.754200  4782 sgd_solver.cpp:106] Iteration 3700, lr = 0.000789695
I0707 13:08:41.781675  4782 solver.cpp:228] Iteration 3725, loss = 0.332303
I0707 13:08:41.781771  4782 solver.cpp:244]     Train net output #0: loss = 0.332303 (* 1 = 0.332303 loss)
I0707 13:08:41.781783  4782 sgd_solver.cpp:106] Iteration 3725, lr = 0.000788616
I0707 13:12:12.807862  4782 solver.cpp:228] Iteration 3750, loss = 0.396086
I0707 13:12:12.807976  4782 solver.cpp:244]     Train net output #0: loss = 0.396086 (* 1 = 0.396086 loss)
I0707 13:12:12.807991  4782 sgd_solver.cpp:106] Iteration 3750, lr = 0.000787541
I0707 13:15:50.644335  4782 solver.cpp:228] Iteration 3775, loss = 0.338277
I0707 13:15:50.644704  4782 solver.cpp:244]     Train net output #0: loss = 0.338277 (* 1 = 0.338277 loss)
I0707 13:15:50.644721  4782 sgd_solver.cpp:106] Iteration 3775, lr = 0.000786468
I0707 13:19:16.828351  4782 solver.cpp:228] Iteration 3800, loss = 0.36186
I0707 13:19:16.828438  4782 solver.cpp:244]     Train net output #0: loss = 0.36186 (* 1 = 0.36186 loss)
I0707 13:19:16.828449  4782 sgd_solver.cpp:106] Iteration 3800, lr = 0.0007854
I0707 13:22:42.054980  4782 solver.cpp:228] Iteration 3825, loss = 0.3428
I0707 13:22:42.055078  4782 solver.cpp:244]     Train net output #0: loss = 0.3428 (* 1 = 0.3428 loss)
I0707 13:22:42.055090  4782 sgd_solver.cpp:106] Iteration 3825, lr = 0.000784334
I0707 13:26:07.194994  4782 solver.cpp:228] Iteration 3850, loss = 0.368277
I0707 13:26:07.195116  4782 solver.cpp:244]     Train net output #0: loss = 0.368277 (* 1 = 0.368277 loss)
I0707 13:26:07.195129  4782 sgd_solver.cpp:106] Iteration 3850, lr = 0.000783272
I0707 13:29:25.392724  4782 solver.cpp:228] Iteration 3875, loss = 0.328074
I0707 13:29:25.392841  4782 solver.cpp:244]     Train net output #0: loss = 0.328074 (* 1 = 0.328074 loss)
I0707 13:29:25.392868  4782 sgd_solver.cpp:106] Iteration 3875, lr = 0.000782213
I0707 13:32:51.056547  4782 solver.cpp:228] Iteration 3900, loss = 0.366785
I0707 13:32:51.056680  4782 solver.cpp:244]     Train net output #0: loss = 0.366785 (* 1 = 0.366785 loss)
I0707 13:32:51.056694  4782 sgd_solver.cpp:106] Iteration 3900, lr = 0.000781158
I0707 13:36:12.047751  4782 solver.cpp:228] Iteration 3925, loss = 0.390287
I0707 13:36:12.047843  4782 solver.cpp:244]     Train net output #0: loss = 0.390287 (* 1 = 0.390287 loss)
I0707 13:36:12.047855  4782 sgd_solver.cpp:106] Iteration 3925, lr = 0.000780106
I0707 13:39:39.444592  4782 solver.cpp:228] Iteration 3950, loss = 0.311855
I0707 13:39:39.444682  4782 solver.cpp:244]     Train net output #0: loss = 0.311855 (* 1 = 0.311855 loss)
I0707 13:39:39.444694  4782 sgd_solver.cpp:106] Iteration 3950, lr = 0.000779057
I0707 13:42:59.755591  4782 solver.cpp:228] Iteration 3975, loss = 0.317719
I0707 13:42:59.755671  4782 solver.cpp:244]     Train net output #0: loss = 0.317719 (* 1 = 0.317719 loss)
I0707 13:42:59.755683  4782 sgd_solver.cpp:106] Iteration 3975, lr = 0.000778012
I0707 13:46:18.292446  4782 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_4000.caffemodel
I0707 13:46:19.206883  4782 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_4000.solverstate
I0707 13:46:19.207222  4782 solver.cpp:337] Iteration 4000, Testing net (#0)
I0707 13:46:19.207237  4782 net.cpp:684] Ignoring source layer training_cells
I0707 13:46:19.207330  4782 net.cpp:684] Ignoring source layer drop1
I0707 13:55:08.788280  4782 solver.cpp:404]     Test net output #0: accuracy = 0.872793
I0707 13:55:08.788368  4782 solver.cpp:404]     Test net output #1: loss = 0.349416 (* 1 = 0.349416 loss)
I0707 13:55:17.057725  4782 solver.cpp:228] Iteration 4000, loss = 0.363325
I0707 13:55:17.057777  4782 solver.cpp:244]     Train net output #0: loss = 0.363325 (* 1 = 0.363325 loss)
I0707 13:55:17.057787  4782 sgd_solver.cpp:106] Iteration 4000, lr = 0.00077697
I0707 13:58:48.362082  4782 solver.cpp:228] Iteration 4025, loss = 0.383478
I0707 13:58:48.362172  4782 solver.cpp:244]     Train net output #0: loss = 0.383478 (* 1 = 0.383478 loss)
I0707 13:58:48.362185  4782 sgd_solver.cpp:106] Iteration 4025, lr = 0.000775931
I0707 14:02:10.807292  4782 solver.cpp:228] Iteration 4050, loss = 0.346326
I0707 14:02:10.807390  4782 solver.cpp:244]     Train net output #0: loss = 0.346326 (* 1 = 0.346326 loss)
I0707 14:02:10.807405  4782 sgd_solver.cpp:106] Iteration 4050, lr = 0.000774895
I0707 14:05:35.799343  4782 solver.cpp:228] Iteration 4075, loss = 0.452465
I0707 14:05:35.799432  4782 solver.cpp:244]     Train net output #0: loss = 0.452465 (* 1 = 0.452465 loss)
I0707 14:05:35.799444  4782 sgd_solver.cpp:106] Iteration 4075, lr = 0.000773862
I0707 14:09:09.949216  4782 solver.cpp:228] Iteration 4100, loss = 0.381246
I0707 14:09:09.949318  4782 solver.cpp:244]     Train net output #0: loss = 0.381246 (* 1 = 0.381246 loss)
I0707 14:09:09.949332  4782 sgd_solver.cpp:106] Iteration 4100, lr = 0.000772833
I0707 14:10:20.358441  4782 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_4110.caffemodel
I0707 14:10:21.267521  4782 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_4110.solverstate
I0707 14:10:21.267870  4782 solver.cpp:301] Optimization stopped early.
I0707 14:10:21.267894  4782 caffe.cpp:222] Optimization Done.
