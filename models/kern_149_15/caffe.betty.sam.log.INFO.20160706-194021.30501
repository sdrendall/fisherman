Log file created at: 2016/07/06 19:40:21
Running on machine: betty
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0706 19:40:21.749308 30501 caffe.cpp:185] Using GPUs 0
I0706 19:40:21.821724 30501 caffe.cpp:190] GPU 0: GeForce GTX 760
I0706 19:40:22.024827 30501 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 250
base_lr: 0.001
display: 25
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.4
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "fish_net_memory_map_output"
solver_mode: GPU
device_id: 0
net: "/home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt"
I0706 19:40:22.025035 30501 solver.cpp:91] Creating training net from net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0706 19:40:22.025540 30501 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer testing_cells
I0706 19:40:22.025569 30501 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0706 19:40:22.025671 30501 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TRAIN
}
layer {
  name: "training_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TRAIN
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'train\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0706 19:40:22.026161 30501 layer_factory.hpp:77] Creating layer training_cells
I0706 19:40:22.851984 30501 net.cpp:91] Creating Layer training_cells
I0706 19:40:22.852026 30501 net.cpp:399] training_cells -> image
I0706 19:40:22.852053 30501 net.cpp:399] training_cells -> label
I0706 19:42:03.965368 30501 net.cpp:141] Setting up training_cells
I0706 19:42:03.965448 30501 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0706 19:42:03.965456 30501 net.cpp:148] Top shape: 512 (512)
I0706 19:42:03.965461 30501 net.cpp:156] Memory required for data: 90937344
I0706 19:42:03.965472 30501 layer_factory.hpp:77] Creating layer conv1
I0706 19:42:03.965493 30501 net.cpp:91] Creating Layer conv1
I0706 19:42:03.965500 30501 net.cpp:425] conv1 <- image
I0706 19:42:03.965513 30501 net.cpp:399] conv1 -> conv1
I0706 19:42:03.968654 30501 net.cpp:141] Setting up conv1
I0706 19:42:03.968680 30501 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0706 19:42:03.968686 30501 net.cpp:156] Memory required for data: 650809344
I0706 19:42:03.968701 30501 layer_factory.hpp:77] Creating layer pool1
I0706 19:42:03.968711 30501 net.cpp:91] Creating Layer pool1
I0706 19:42:03.968716 30501 net.cpp:425] pool1 <- conv1
I0706 19:42:03.968724 30501 net.cpp:399] pool1 -> pool1
I0706 19:42:03.968761 30501 net.cpp:141] Setting up pool1
I0706 19:42:03.968770 30501 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0706 19:42:03.968775 30501 net.cpp:156] Memory required for data: 673204224
I0706 19:42:03.968778 30501 layer_factory.hpp:77] Creating layer conv2
I0706 19:42:03.968788 30501 net.cpp:91] Creating Layer conv2
I0706 19:42:03.968793 30501 net.cpp:425] conv2 <- pool1
I0706 19:42:03.968799 30501 net.cpp:399] conv2 -> conv2
I0706 19:42:03.969632 30501 net.cpp:141] Setting up conv2
I0706 19:42:03.969646 30501 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0706 19:42:03.969651 30501 net.cpp:156] Memory required for data: 677720064
I0706 19:42:03.969660 30501 layer_factory.hpp:77] Creating layer pool2
I0706 19:42:03.969678 30501 net.cpp:91] Creating Layer pool2
I0706 19:42:03.969684 30501 net.cpp:425] pool2 <- conv2
I0706 19:42:03.969691 30501 net.cpp:399] pool2 -> pool2
I0706 19:42:03.969717 30501 net.cpp:141] Setting up pool2
I0706 19:42:03.969725 30501 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0706 19:42:03.969729 30501 net.cpp:156] Memory required for data: 678221824
I0706 19:42:03.969734 30501 layer_factory.hpp:77] Creating layer ip1
I0706 19:42:03.969743 30501 net.cpp:91] Creating Layer ip1
I0706 19:42:03.969748 30501 net.cpp:425] ip1 <- pool2
I0706 19:42:03.969754 30501 net.cpp:399] ip1 -> ip1
I0706 19:42:03.969866 30501 net.cpp:141] Setting up ip1
I0706 19:42:03.969874 30501 net.cpp:148] Top shape: 512 32 (16384)
I0706 19:42:03.969879 30501 net.cpp:156] Memory required for data: 678287360
I0706 19:42:03.969887 30501 layer_factory.hpp:77] Creating layer relu1
I0706 19:42:03.969894 30501 net.cpp:91] Creating Layer relu1
I0706 19:42:03.969899 30501 net.cpp:425] relu1 <- ip1
I0706 19:42:03.969905 30501 net.cpp:386] relu1 -> ip1 (in-place)
I0706 19:42:03.969915 30501 net.cpp:141] Setting up relu1
I0706 19:42:03.969921 30501 net.cpp:148] Top shape: 512 32 (16384)
I0706 19:42:03.969926 30501 net.cpp:156] Memory required for data: 678352896
I0706 19:42:03.969931 30501 layer_factory.hpp:77] Creating layer drop1
I0706 19:42:03.969944 30501 net.cpp:91] Creating Layer drop1
I0706 19:42:03.969950 30501 net.cpp:425] drop1 <- ip1
I0706 19:42:03.969956 30501 net.cpp:386] drop1 -> ip1 (in-place)
I0706 19:42:03.969975 30501 net.cpp:141] Setting up drop1
I0706 19:42:03.969981 30501 net.cpp:148] Top shape: 512 32 (16384)
I0706 19:42:03.969986 30501 net.cpp:156] Memory required for data: 678418432
I0706 19:42:03.969990 30501 layer_factory.hpp:77] Creating layer ip2
I0706 19:42:03.970000 30501 net.cpp:91] Creating Layer ip2
I0706 19:42:03.970003 30501 net.cpp:425] ip2 <- ip1
I0706 19:42:03.970011 30501 net.cpp:399] ip2 -> ip2
I0706 19:42:03.970068 30501 net.cpp:141] Setting up ip2
I0706 19:42:03.970077 30501 net.cpp:148] Top shape: 512 2 (1024)
I0706 19:42:03.970082 30501 net.cpp:156] Memory required for data: 678422528
I0706 19:42:03.970088 30501 layer_factory.hpp:77] Creating layer loss
I0706 19:42:03.970095 30501 net.cpp:91] Creating Layer loss
I0706 19:42:03.970100 30501 net.cpp:425] loss <- ip2
I0706 19:42:03.970105 30501 net.cpp:425] loss <- label
I0706 19:42:03.970113 30501 net.cpp:399] loss -> loss
I0706 19:42:03.970125 30501 layer_factory.hpp:77] Creating layer loss
I0706 19:42:03.970216 30501 net.cpp:141] Setting up loss
I0706 19:42:03.970224 30501 net.cpp:148] Top shape: (1)
I0706 19:42:03.970229 30501 net.cpp:151]     with loss weight 1
I0706 19:42:03.970245 30501 net.cpp:156] Memory required for data: 678422532
I0706 19:42:03.970250 30501 net.cpp:217] loss needs backward computation.
I0706 19:42:03.970257 30501 net.cpp:217] ip2 needs backward computation.
I0706 19:42:03.970260 30501 net.cpp:217] drop1 needs backward computation.
I0706 19:42:03.970265 30501 net.cpp:217] relu1 needs backward computation.
I0706 19:42:03.970269 30501 net.cpp:217] ip1 needs backward computation.
I0706 19:42:03.970274 30501 net.cpp:217] pool2 needs backward computation.
I0706 19:42:03.970279 30501 net.cpp:217] conv2 needs backward computation.
I0706 19:42:03.970284 30501 net.cpp:217] pool1 needs backward computation.
I0706 19:42:03.970288 30501 net.cpp:217] conv1 needs backward computation.
I0706 19:42:03.970293 30501 net.cpp:219] training_cells does not need backward computation.
I0706 19:42:03.970299 30501 net.cpp:261] This network produces output loss
I0706 19:42:03.970309 30501 net.cpp:274] Network initialization done.
I0706 19:42:03.970607 30501 solver.cpp:181] Creating test net (#0) specified by net file: /home/sam/code/fisherman/caffe/fish_net/kern_149/fish_net_memory_map_trainer.prototxt
I0706 19:42:03.970633 30501 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer training_cells
I0706 19:42:03.970644 30501 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer drop1
I0706 19:42:03.970715 30501 net.cpp:49] Initializing net from parameters: 
name: "fish_filter"
state {
  phase: TEST
}
layer {
  name: "testing_cells"
  type: "Python"
  top: "image"
  top: "label"
  include {
    phase: TEST
  }
  python_param {
    module: "fisherman.caffe_layers"
    layer: "DataMapLayer"
    param_str: "{\'tops\': [\'image\', \'label\'], \'data_dir\': \'/home/sam/code/fisherman/data/dense_labelling_k149/\', \'split\': \'test\', \'samples_per_class\': 256, \'kernel\': 149}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 15
    kernel_size: 15
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 5
    stride: 5
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 5
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0706 19:42:03.971031 30501 layer_factory.hpp:77] Creating layer testing_cells
I0706 19:42:03.971076 30501 net.cpp:91] Creating Layer testing_cells
I0706 19:42:03.971086 30501 net.cpp:399] testing_cells -> image
I0706 19:42:03.971120 30501 net.cpp:399] testing_cells -> label
I0706 19:42:52.087226 30501 net.cpp:141] Setting up testing_cells
I0706 19:42:52.087338 30501 net.cpp:148] Top shape: 512 2 149 149 (22733824)
I0706 19:42:52.087347 30501 net.cpp:148] Top shape: 512 (512)
I0706 19:42:52.087352 30501 net.cpp:156] Memory required for data: 90937344
I0706 19:42:52.087360 30501 layer_factory.hpp:77] Creating layer label_testing_cells_1_split
I0706 19:42:52.087375 30501 net.cpp:91] Creating Layer label_testing_cells_1_split
I0706 19:42:52.087381 30501 net.cpp:425] label_testing_cells_1_split <- label
I0706 19:42:52.087391 30501 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_0
I0706 19:42:52.087404 30501 net.cpp:399] label_testing_cells_1_split -> label_testing_cells_1_split_1
I0706 19:42:52.087432 30501 net.cpp:141] Setting up label_testing_cells_1_split
I0706 19:42:52.087450 30501 net.cpp:148] Top shape: 512 (512)
I0706 19:42:52.087455 30501 net.cpp:148] Top shape: 512 (512)
I0706 19:42:52.087460 30501 net.cpp:156] Memory required for data: 90941440
I0706 19:42:52.087465 30501 layer_factory.hpp:77] Creating layer conv1
I0706 19:42:52.087478 30501 net.cpp:91] Creating Layer conv1
I0706 19:42:52.087483 30501 net.cpp:425] conv1 <- image
I0706 19:42:52.087491 30501 net.cpp:399] conv1 -> conv1
I0706 19:42:52.087692 30501 net.cpp:141] Setting up conv1
I0706 19:42:52.087712 30501 net.cpp:148] Top shape: 512 15 135 135 (139968000)
I0706 19:42:52.087716 30501 net.cpp:156] Memory required for data: 650813440
I0706 19:42:52.087728 30501 layer_factory.hpp:77] Creating layer pool1
I0706 19:42:52.087736 30501 net.cpp:91] Creating Layer pool1
I0706 19:42:52.087741 30501 net.cpp:425] pool1 <- conv1
I0706 19:42:52.087748 30501 net.cpp:399] pool1 -> pool1
I0706 19:42:52.087774 30501 net.cpp:141] Setting up pool1
I0706 19:42:52.087790 30501 net.cpp:148] Top shape: 512 15 27 27 (5598720)
I0706 19:42:52.087795 30501 net.cpp:156] Memory required for data: 673208320
I0706 19:42:52.087810 30501 layer_factory.hpp:77] Creating layer conv2
I0706 19:42:52.087818 30501 net.cpp:91] Creating Layer conv2
I0706 19:42:52.087823 30501 net.cpp:425] conv2 <- pool1
I0706 19:42:52.087831 30501 net.cpp:399] conv2 -> conv2
I0706 19:42:52.087987 30501 net.cpp:141] Setting up conv2
I0706 19:42:52.088006 30501 net.cpp:148] Top shape: 512 5 21 21 (1128960)
I0706 19:42:52.088011 30501 net.cpp:156] Memory required for data: 677724160
I0706 19:42:52.088029 30501 layer_factory.hpp:77] Creating layer pool2
I0706 19:42:52.088037 30501 net.cpp:91] Creating Layer pool2
I0706 19:42:52.088052 30501 net.cpp:425] pool2 <- conv2
I0706 19:42:52.088057 30501 net.cpp:399] pool2 -> pool2
I0706 19:42:52.088080 30501 net.cpp:141] Setting up pool2
I0706 19:42:52.088088 30501 net.cpp:148] Top shape: 512 5 7 7 (125440)
I0706 19:42:52.088091 30501 net.cpp:156] Memory required for data: 678225920
I0706 19:42:52.088096 30501 layer_factory.hpp:77] Creating layer ip1
I0706 19:42:52.088104 30501 net.cpp:91] Creating Layer ip1
I0706 19:42:52.088109 30501 net.cpp:425] ip1 <- pool2
I0706 19:42:52.088115 30501 net.cpp:399] ip1 -> ip1
I0706 19:42:52.088228 30501 net.cpp:141] Setting up ip1
I0706 19:42:52.088248 30501 net.cpp:148] Top shape: 512 32 (16384)
I0706 19:42:52.088251 30501 net.cpp:156] Memory required for data: 678291456
I0706 19:42:52.088260 30501 layer_factory.hpp:77] Creating layer relu1
I0706 19:42:52.088266 30501 net.cpp:91] Creating Layer relu1
I0706 19:42:52.088271 30501 net.cpp:425] relu1 <- ip1
I0706 19:42:52.088277 30501 net.cpp:386] relu1 -> ip1 (in-place)
I0706 19:42:52.088284 30501 net.cpp:141] Setting up relu1
I0706 19:42:52.088289 30501 net.cpp:148] Top shape: 512 32 (16384)
I0706 19:42:52.088294 30501 net.cpp:156] Memory required for data: 678356992
I0706 19:42:52.088299 30501 layer_factory.hpp:77] Creating layer ip2
I0706 19:42:52.088306 30501 net.cpp:91] Creating Layer ip2
I0706 19:42:52.088311 30501 net.cpp:425] ip2 <- ip1
I0706 19:42:52.088317 30501 net.cpp:399] ip2 -> ip2
I0706 19:42:52.088392 30501 net.cpp:141] Setting up ip2
I0706 19:42:52.088398 30501 net.cpp:148] Top shape: 512 2 (1024)
I0706 19:42:52.088403 30501 net.cpp:156] Memory required for data: 678361088
I0706 19:42:52.088410 30501 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0706 19:42:52.088462 30501 net.cpp:91] Creating Layer ip2_ip2_0_split
I0706 19:42:52.088469 30501 net.cpp:425] ip2_ip2_0_split <- ip2
I0706 19:42:52.088491 30501 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0706 19:42:52.088498 30501 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0706 19:42:52.088522 30501 net.cpp:141] Setting up ip2_ip2_0_split
I0706 19:42:52.088529 30501 net.cpp:148] Top shape: 512 2 (1024)
I0706 19:42:52.088534 30501 net.cpp:148] Top shape: 512 2 (1024)
I0706 19:42:52.088539 30501 net.cpp:156] Memory required for data: 678369280
I0706 19:42:52.088544 30501 layer_factory.hpp:77] Creating layer accuracy
I0706 19:42:52.088557 30501 net.cpp:91] Creating Layer accuracy
I0706 19:42:52.088562 30501 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0706 19:42:52.088577 30501 net.cpp:425] accuracy <- label_testing_cells_1_split_0
I0706 19:42:52.088593 30501 net.cpp:399] accuracy -> accuracy
I0706 19:42:52.088603 30501 net.cpp:141] Setting up accuracy
I0706 19:42:52.088609 30501 net.cpp:148] Top shape: (1)
I0706 19:42:52.088613 30501 net.cpp:156] Memory required for data: 678369284
I0706 19:42:52.088618 30501 layer_factory.hpp:77] Creating layer loss
I0706 19:42:52.088634 30501 net.cpp:91] Creating Layer loss
I0706 19:42:52.088639 30501 net.cpp:425] loss <- ip2_ip2_0_split_1
I0706 19:42:52.088654 30501 net.cpp:425] loss <- label_testing_cells_1_split_1
I0706 19:42:52.088660 30501 net.cpp:399] loss -> loss
I0706 19:42:52.088670 30501 layer_factory.hpp:77] Creating layer loss
I0706 19:42:52.088758 30501 net.cpp:141] Setting up loss
I0706 19:42:52.088775 30501 net.cpp:148] Top shape: (1)
I0706 19:42:52.088780 30501 net.cpp:151]     with loss weight 1
I0706 19:42:52.088790 30501 net.cpp:156] Memory required for data: 678369288
I0706 19:42:52.088794 30501 net.cpp:217] loss needs backward computation.
I0706 19:42:52.088799 30501 net.cpp:219] accuracy does not need backward computation.
I0706 19:42:52.088805 30501 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0706 19:42:52.088809 30501 net.cpp:217] ip2 needs backward computation.
I0706 19:42:52.088814 30501 net.cpp:217] relu1 needs backward computation.
I0706 19:42:52.088819 30501 net.cpp:217] ip1 needs backward computation.
I0706 19:42:52.088824 30501 net.cpp:217] pool2 needs backward computation.
I0706 19:42:52.088829 30501 net.cpp:217] conv2 needs backward computation.
I0706 19:42:52.088834 30501 net.cpp:217] pool1 needs backward computation.
I0706 19:42:52.088837 30501 net.cpp:217] conv1 needs backward computation.
I0706 19:42:52.088842 30501 net.cpp:219] label_testing_cells_1_split does not need backward computation.
I0706 19:42:52.088848 30501 net.cpp:219] testing_cells does not need backward computation.
I0706 19:42:52.088852 30501 net.cpp:261] This network produces output accuracy
I0706 19:42:52.088857 30501 net.cpp:261] This network produces output loss
I0706 19:42:52.088867 30501 net.cpp:274] Network initialization done.
I0706 19:42:52.088917 30501 solver.cpp:60] Solver scaffolding done.
I0706 19:42:52.089093 30501 caffe.cpp:219] Starting Optimization
I0706 19:42:52.089100 30501 solver.cpp:279] Solving fish_filter
I0706 19:42:52.089114 30501 solver.cpp:280] Learning Rate Policy: inv
I0706 19:42:52.092125 30501 solver.cpp:337] Iteration 0, Testing net (#0)
I0706 19:42:52.092141 30501 net.cpp:684] Ignoring source layer training_cells
I0706 19:42:52.092429 30501 net.cpp:684] Ignoring source layer drop1
I0706 19:54:19.850658 30501 solver.cpp:404]     Test net output #0: accuracy = 0.456904
I0706 19:54:19.850764 30501 solver.cpp:404]     Test net output #1: loss = 0.869549 (* 1 = 0.869549 loss)
I0706 19:54:29.610270 30501 solver.cpp:228] Iteration 0, loss = 1.09425
I0706 19:54:29.610316 30501 solver.cpp:244]     Train net output #0: loss = 1.09425 (* 1 = 1.09425 loss)
I0706 19:54:29.610332 30501 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0706 19:58:25.131971 30501 solver.cpp:228] Iteration 25, loss = 0.731286
I0706 19:58:25.132062 30501 solver.cpp:244]     Train net output #0: loss = 0.731286 (* 1 = 0.731286 loss)
I0706 19:58:25.132077 30501 sgd_solver.cpp:106] Iteration 25, lr = 0.000998129
I0706 20:02:25.052242 30501 solver.cpp:228] Iteration 50, loss = 0.72788
I0706 20:02:25.052340 30501 solver.cpp:244]     Train net output #0: loss = 0.72788 (* 1 = 0.72788 loss)
I0706 20:02:25.052351 30501 sgd_solver.cpp:106] Iteration 50, lr = 0.000996266
I0706 20:06:26.163056 30501 solver.cpp:228] Iteration 75, loss = 0.713839
I0706 20:06:26.163135 30501 solver.cpp:244]     Train net output #0: loss = 0.713839 (* 1 = 0.713839 loss)
I0706 20:06:26.163146 30501 sgd_solver.cpp:106] Iteration 75, lr = 0.000994412
I0706 20:10:28.879802 30501 solver.cpp:228] Iteration 100, loss = 0.65698
I0706 20:10:28.879904 30501 solver.cpp:244]     Train net output #0: loss = 0.65698 (* 1 = 0.65698 loss)
I0706 20:10:28.879916 30501 sgd_solver.cpp:106] Iteration 100, lr = 0.000992565
I0706 20:14:38.185825 30501 solver.cpp:228] Iteration 125, loss = 0.64511
I0706 20:14:38.185919 30501 solver.cpp:244]     Train net output #0: loss = 0.64511 (* 1 = 0.64511 loss)
I0706 20:14:38.185931 30501 sgd_solver.cpp:106] Iteration 125, lr = 0.000990726
I0706 20:18:38.251427 30501 solver.cpp:228] Iteration 150, loss = 0.654071
I0706 20:18:38.251509 30501 solver.cpp:244]     Train net output #0: loss = 0.654071 (* 1 = 0.654071 loss)
I0706 20:18:38.251521 30501 sgd_solver.cpp:106] Iteration 150, lr = 0.000988896
I0706 20:22:44.258599 30501 solver.cpp:228] Iteration 175, loss = 0.643392
I0706 20:22:44.258689 30501 solver.cpp:244]     Train net output #0: loss = 0.643392 (* 1 = 0.643392 loss)
I0706 20:22:44.258702 30501 sgd_solver.cpp:106] Iteration 175, lr = 0.000987073
I0706 20:26:53.697024 30501 solver.cpp:228] Iteration 200, loss = 0.653091
I0706 20:26:53.697106 30501 solver.cpp:244]     Train net output #0: loss = 0.653091 (* 1 = 0.653091 loss)
I0706 20:26:53.697118 30501 sgd_solver.cpp:106] Iteration 200, lr = 0.000985258
I0706 20:31:01.178599 30501 solver.cpp:228] Iteration 225, loss = 0.673153
I0706 20:31:01.178685 30501 solver.cpp:244]     Train net output #0: loss = 0.673153 (* 1 = 0.673153 loss)
I0706 20:31:01.178697 30501 sgd_solver.cpp:106] Iteration 225, lr = 0.000983451
I0706 20:34:57.274147 30501 solver.cpp:337] Iteration 250, Testing net (#0)
I0706 20:34:57.274214 30501 net.cpp:684] Ignoring source layer training_cells
I0706 20:34:57.274224 30501 net.cpp:684] Ignoring source layer drop1
I0706 20:46:17.336122 30501 solver.cpp:404]     Test net output #0: accuracy = 0.639619
I0706 20:46:17.336207 30501 solver.cpp:404]     Test net output #1: loss = 0.620508 (* 1 = 0.620508 loss)
I0706 20:46:29.009047 30501 solver.cpp:228] Iteration 250, loss = 0.612539
I0706 20:46:29.009097 30501 solver.cpp:244]     Train net output #0: loss = 0.612539 (* 1 = 0.612539 loss)
I0706 20:46:29.009109 30501 sgd_solver.cpp:106] Iteration 250, lr = 0.000981651
I0706 20:50:47.286183 30501 solver.cpp:228] Iteration 275, loss = 0.612441
I0706 20:50:47.286275 30501 solver.cpp:244]     Train net output #0: loss = 0.612441 (* 1 = 0.612441 loss)
I0706 20:50:47.286288 30501 sgd_solver.cpp:106] Iteration 275, lr = 0.000979859
I0706 20:55:03.974733 30501 solver.cpp:228] Iteration 300, loss = 0.644416
I0706 20:55:03.974820 30501 solver.cpp:244]     Train net output #0: loss = 0.644416 (* 1 = 0.644416 loss)
I0706 20:55:03.974833 30501 sgd_solver.cpp:106] Iteration 300, lr = 0.000978075
I0706 20:59:23.351542 30501 solver.cpp:228] Iteration 325, loss = 0.627622
I0706 20:59:23.351639 30501 solver.cpp:244]     Train net output #0: loss = 0.627622 (* 1 = 0.627622 loss)
I0706 20:59:23.351651 30501 sgd_solver.cpp:106] Iteration 325, lr = 0.000976298
I0706 21:03:46.100723 30501 solver.cpp:228] Iteration 350, loss = 0.625483
I0706 21:03:46.100811 30501 solver.cpp:244]     Train net output #0: loss = 0.625483 (* 1 = 0.625483 loss)
I0706 21:03:46.100823 30501 sgd_solver.cpp:106] Iteration 350, lr = 0.000974529
I0706 21:08:03.349083 30501 solver.cpp:228] Iteration 375, loss = 0.58759
I0706 21:08:03.349184 30501 solver.cpp:244]     Train net output #0: loss = 0.58759 (* 1 = 0.58759 loss)
I0706 21:08:03.349196 30501 sgd_solver.cpp:106] Iteration 375, lr = 0.000972767
I0706 21:12:21.212851 30501 solver.cpp:228] Iteration 400, loss = 0.629304
I0706 21:12:21.212951 30501 solver.cpp:244]     Train net output #0: loss = 0.629304 (* 1 = 0.629304 loss)
I0706 21:12:21.212963 30501 sgd_solver.cpp:106] Iteration 400, lr = 0.000971013
I0706 21:16:48.183074 30501 solver.cpp:228] Iteration 425, loss = 0.614046
I0706 21:16:48.183137 30501 solver.cpp:244]     Train net output #0: loss = 0.614046 (* 1 = 0.614046 loss)
I0706 21:16:48.183148 30501 sgd_solver.cpp:106] Iteration 425, lr = 0.000969266
I0706 21:21:15.149869 30501 solver.cpp:228] Iteration 450, loss = 0.609703
I0706 21:21:15.149966 30501 solver.cpp:244]     Train net output #0: loss = 0.609703 (* 1 = 0.609703 loss)
I0706 21:21:15.149977 30501 sgd_solver.cpp:106] Iteration 450, lr = 0.000967526
I0706 21:25:39.967777 30501 solver.cpp:228] Iteration 475, loss = 0.574119
I0706 21:25:39.967869 30501 solver.cpp:244]     Train net output #0: loss = 0.574119 (* 1 = 0.574119 loss)
I0706 21:25:39.967880 30501 sgd_solver.cpp:106] Iteration 475, lr = 0.000965794
I0706 21:29:50.450515 30501 solver.cpp:337] Iteration 500, Testing net (#0)
I0706 21:29:50.450595 30501 net.cpp:684] Ignoring source layer training_cells
I0706 21:29:50.450606 30501 net.cpp:684] Ignoring source layer drop1
I0706 21:41:22.133616 30501 solver.cpp:404]     Test net output #0: accuracy = 0.676582
I0706 21:41:22.133697 30501 solver.cpp:404]     Test net output #1: loss = 0.580912 (* 1 = 0.580912 loss)
I0706 21:41:32.667455 30501 solver.cpp:228] Iteration 500, loss = 0.624442
I0706 21:41:32.667490 30501 solver.cpp:244]     Train net output #0: loss = 0.624442 (* 1 = 0.624442 loss)
I0706 21:41:32.667501 30501 sgd_solver.cpp:106] Iteration 500, lr = 0.000964069
I0706 21:45:54.353216 30501 solver.cpp:228] Iteration 525, loss = 0.575221
I0706 21:45:54.353304 30501 solver.cpp:244]     Train net output #0: loss = 0.575221 (* 1 = 0.575221 loss)
I0706 21:45:54.353317 30501 sgd_solver.cpp:106] Iteration 525, lr = 0.000962351
I0706 21:50:27.441706 30501 solver.cpp:228] Iteration 550, loss = 0.593077
I0706 21:50:27.441781 30501 solver.cpp:244]     Train net output #0: loss = 0.593077 (* 1 = 0.593077 loss)
I0706 21:50:27.441793 30501 sgd_solver.cpp:106] Iteration 550, lr = 0.00096064
I0706 21:55:09.868290 30501 solver.cpp:228] Iteration 575, loss = 0.592021
I0706 21:55:09.868386 30501 solver.cpp:244]     Train net output #0: loss = 0.592021 (* 1 = 0.592021 loss)
I0706 21:55:09.868397 30501 sgd_solver.cpp:106] Iteration 575, lr = 0.000958936
I0706 21:59:35.458461 30501 solver.cpp:228] Iteration 600, loss = 0.553165
I0706 21:59:35.458540 30501 solver.cpp:244]     Train net output #0: loss = 0.553165 (* 1 = 0.553165 loss)
I0706 21:59:35.458552 30501 sgd_solver.cpp:106] Iteration 600, lr = 0.00095724
I0706 22:03:42.165786 30501 solver.cpp:228] Iteration 625, loss = 0.575632
I0706 22:03:42.165869 30501 solver.cpp:244]     Train net output #0: loss = 0.575632 (* 1 = 0.575632 loss)
I0706 22:03:42.165881 30501 sgd_solver.cpp:106] Iteration 625, lr = 0.00095555
I0706 22:07:45.044972 30501 solver.cpp:228] Iteration 650, loss = 0.578319
I0706 22:07:45.045056 30501 solver.cpp:244]     Train net output #0: loss = 0.578319 (* 1 = 0.578319 loss)
I0706 22:07:45.045068 30501 sgd_solver.cpp:106] Iteration 650, lr = 0.000953867
I0706 22:11:47.149930 30501 solver.cpp:228] Iteration 675, loss = 0.567702
I0706 22:11:47.150013 30501 solver.cpp:244]     Train net output #0: loss = 0.567702 (* 1 = 0.567702 loss)
I0706 22:11:47.150025 30501 sgd_solver.cpp:106] Iteration 675, lr = 0.000952191
I0706 22:16:22.940423 30501 solver.cpp:228] Iteration 700, loss = 0.54973
I0706 22:16:22.940516 30501 solver.cpp:244]     Train net output #0: loss = 0.54973 (* 1 = 0.54973 loss)
I0706 22:16:22.940529 30501 sgd_solver.cpp:106] Iteration 700, lr = 0.000950522
I0706 22:21:31.119438 30501 solver.cpp:228] Iteration 725, loss = 0.532318
I0706 22:21:31.119524 30501 solver.cpp:244]     Train net output #0: loss = 0.532318 (* 1 = 0.532318 loss)
I0706 22:21:31.119535 30501 sgd_solver.cpp:106] Iteration 725, lr = 0.00094886
I0706 22:26:17.650045 30501 solver.cpp:337] Iteration 750, Testing net (#0)
I0706 22:26:17.650120 30501 net.cpp:684] Ignoring source layer training_cells
I0706 22:26:17.650128 30501 net.cpp:684] Ignoring source layer drop1
I0706 22:39:50.404350 30501 solver.cpp:404]     Test net output #0: accuracy = 0.72958
I0706 22:39:50.404455 30501 solver.cpp:404]     Test net output #1: loss = 0.542712 (* 1 = 0.542712 loss)
I0706 22:40:13.328692 30501 solver.cpp:228] Iteration 750, loss = 0.554764
I0706 22:40:13.328727 30501 solver.cpp:244]     Train net output #0: loss = 0.554764 (* 1 = 0.554764 loss)
I0706 22:40:13.328737 30501 sgd_solver.cpp:106] Iteration 750, lr = 0.000947204
I0706 22:47:39.668313 30501 solver.cpp:228] Iteration 775, loss = 0.542847
I0706 22:47:39.668393 30501 solver.cpp:244]     Train net output #0: loss = 0.542847 (* 1 = 0.542847 loss)
I0706 22:47:39.668411 30501 sgd_solver.cpp:106] Iteration 775, lr = 0.000945556
I0706 22:52:43.077399 30501 solver.cpp:228] Iteration 800, loss = 0.520918
I0706 22:52:43.077484 30501 solver.cpp:244]     Train net output #0: loss = 0.520918 (* 1 = 0.520918 loss)
I0706 22:52:43.077498 30501 sgd_solver.cpp:106] Iteration 800, lr = 0.000943913
I0706 22:57:17.393628 30501 solver.cpp:228] Iteration 825, loss = 0.547749
I0706 22:57:17.393728 30501 solver.cpp:244]     Train net output #0: loss = 0.547749 (* 1 = 0.547749 loss)
I0706 22:57:17.393739 30501 sgd_solver.cpp:106] Iteration 825, lr = 0.000942278
I0706 23:01:43.472012 30501 solver.cpp:228] Iteration 850, loss = 0.549216
I0706 23:01:43.472120 30501 solver.cpp:244]     Train net output #0: loss = 0.549216 (* 1 = 0.549216 loss)
I0706 23:01:43.472134 30501 sgd_solver.cpp:106] Iteration 850, lr = 0.000940649
I0706 23:07:08.777715 30501 solver.cpp:228] Iteration 875, loss = 0.508165
I0706 23:07:08.777801 30501 solver.cpp:244]     Train net output #0: loss = 0.508165 (* 1 = 0.508165 loss)
I0706 23:07:08.777813 30501 sgd_solver.cpp:106] Iteration 875, lr = 0.000939027
I0706 23:11:48.350322 30501 solver.cpp:228] Iteration 900, loss = 0.479796
I0706 23:11:48.350417 30501 solver.cpp:244]     Train net output #0: loss = 0.479796 (* 1 = 0.479796 loss)
I0706 23:11:48.350435 30501 sgd_solver.cpp:106] Iteration 900, lr = 0.000937411
I0706 23:16:23.203343 30501 solver.cpp:228] Iteration 925, loss = 0.549755
I0706 23:16:23.203449 30501 solver.cpp:244]     Train net output #0: loss = 0.549755 (* 1 = 0.549755 loss)
I0706 23:16:23.203470 30501 sgd_solver.cpp:106] Iteration 925, lr = 0.000935802
I0706 23:21:11.015226 30501 solver.cpp:228] Iteration 950, loss = 0.543349
I0706 23:21:11.015328 30501 solver.cpp:244]     Train net output #0: loss = 0.543349 (* 1 = 0.543349 loss)
I0706 23:21:11.015342 30501 sgd_solver.cpp:106] Iteration 950, lr = 0.000934199
I0706 23:26:37.719250 30501 solver.cpp:228] Iteration 975, loss = 0.55946
I0706 23:26:37.719341 30501 solver.cpp:244]     Train net output #0: loss = 0.55946 (* 1 = 0.55946 loss)
I0706 23:26:37.719354 30501 sgd_solver.cpp:106] Iteration 975, lr = 0.000932603
I0706 23:30:47.609680 30501 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_1000.caffemodel
I0706 23:30:48.612738 30501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_1000.solverstate
I0706 23:30:48.613191 30501 solver.cpp:337] Iteration 1000, Testing net (#0)
I0706 23:30:48.613205 30501 net.cpp:684] Ignoring source layer training_cells
I0706 23:30:48.613212 30501 net.cpp:684] Ignoring source layer drop1
I0706 23:46:55.101416 30501 solver.cpp:404]     Test net output #0: accuracy = 0.77626
I0706 23:46:55.101500 30501 solver.cpp:404]     Test net output #1: loss = 0.51121 (* 1 = 0.51121 loss)
I0706 23:47:12.320024 30501 solver.cpp:228] Iteration 1000, loss = 0.533116
I0706 23:47:12.320060 30501 solver.cpp:244]     Train net output #0: loss = 0.533116 (* 1 = 0.533116 loss)
I0706 23:47:12.320071 30501 sgd_solver.cpp:106] Iteration 1000, lr = 0.000931013
I0706 23:51:58.340101 30501 solver.cpp:228] Iteration 1025, loss = 0.581107
I0706 23:51:58.340190 30501 solver.cpp:244]     Train net output #0: loss = 0.581107 (* 1 = 0.581107 loss)
I0706 23:51:58.340203 30501 sgd_solver.cpp:106] Iteration 1025, lr = 0.000929429
I0706 23:57:20.391949 30501 solver.cpp:228] Iteration 1050, loss = 0.527986
I0706 23:57:20.392050 30501 solver.cpp:244]     Train net output #0: loss = 0.527986 (* 1 = 0.527986 loss)
I0706 23:57:20.392062 30501 sgd_solver.cpp:106] Iteration 1050, lr = 0.000927851
I0707 00:01:57.809818 30501 solver.cpp:228] Iteration 1075, loss = 0.53139
I0707 00:01:57.809903 30501 solver.cpp:244]     Train net output #0: loss = 0.53139 (* 1 = 0.53139 loss)
I0707 00:01:57.809918 30501 sgd_solver.cpp:106] Iteration 1075, lr = 0.00092628
I0707 00:06:23.578799 30501 solver.cpp:228] Iteration 1100, loss = 0.567735
I0707 00:06:23.578884 30501 solver.cpp:244]     Train net output #0: loss = 0.567735 (* 1 = 0.567735 loss)
I0707 00:06:23.578896 30501 sgd_solver.cpp:106] Iteration 1100, lr = 0.000924715
I0707 00:10:59.745733 30501 solver.cpp:228] Iteration 1125, loss = 0.510467
I0707 00:10:59.745820 30501 solver.cpp:244]     Train net output #0: loss = 0.510467 (* 1 = 0.510467 loss)
I0707 00:10:59.745832 30501 sgd_solver.cpp:106] Iteration 1125, lr = 0.000923156
I0707 00:16:20.295869 30501 solver.cpp:228] Iteration 1150, loss = 0.525979
I0707 00:16:20.295958 30501 solver.cpp:244]     Train net output #0: loss = 0.525979 (* 1 = 0.525979 loss)
I0707 00:16:20.295969 30501 sgd_solver.cpp:106] Iteration 1150, lr = 0.000921603
I0707 00:21:20.630329 30501 solver.cpp:228] Iteration 1175, loss = 0.503229
I0707 00:21:20.630414 30501 solver.cpp:244]     Train net output #0: loss = 0.503229 (* 1 = 0.503229 loss)
I0707 00:21:20.630426 30501 sgd_solver.cpp:106] Iteration 1175, lr = 0.000920056
I0707 00:26:00.140635 30501 solver.cpp:228] Iteration 1200, loss = 0.497772
I0707 00:26:00.140723 30501 solver.cpp:244]     Train net output #0: loss = 0.497772 (* 1 = 0.497772 loss)
I0707 00:26:00.140735 30501 sgd_solver.cpp:106] Iteration 1200, lr = 0.000918516
I0707 00:30:21.225289 30501 solver.cpp:228] Iteration 1225, loss = 0.526832
I0707 00:30:21.225389 30501 solver.cpp:244]     Train net output #0: loss = 0.526832 (* 1 = 0.526832 loss)
I0707 00:30:21.225400 30501 sgd_solver.cpp:106] Iteration 1225, lr = 0.000916981
I0707 00:34:55.664986 30501 solver.cpp:337] Iteration 1250, Testing net (#0)
I0707 00:34:55.665053 30501 net.cpp:684] Ignoring source layer training_cells
I0707 00:34:55.665065 30501 net.cpp:684] Ignoring source layer drop1
I0707 00:48:10.594888 30501 solver.cpp:404]     Test net output #0: accuracy = 0.806338
I0707 00:48:10.594981 30501 solver.cpp:404]     Test net output #1: loss = 0.483711 (* 1 = 0.483711 loss)
I0707 00:48:22.016649 30501 solver.cpp:228] Iteration 1250, loss = 0.486086
I0707 00:48:22.016686 30501 solver.cpp:244]     Train net output #0: loss = 0.486086 (* 1 = 0.486086 loss)
I0707 00:48:22.016697 30501 sgd_solver.cpp:106] Iteration 1250, lr = 0.000915452
I0707 00:54:58.708266 30501 solver.cpp:228] Iteration 1275, loss = 0.505128
I0707 00:54:58.708354 30501 solver.cpp:244]     Train net output #0: loss = 0.505128 (* 1 = 0.505128 loss)
I0707 00:54:58.708372 30501 sgd_solver.cpp:106] Iteration 1275, lr = 0.000913929
I0707 01:03:08.601043 30501 solver.cpp:228] Iteration 1300, loss = 0.470813
I0707 01:03:08.601132 30501 solver.cpp:244]     Train net output #0: loss = 0.470813 (* 1 = 0.470813 loss)
I0707 01:03:08.601145 30501 sgd_solver.cpp:106] Iteration 1300, lr = 0.000912412
I0707 01:10:41.507558 30501 solver.cpp:228] Iteration 1325, loss = 0.5839
I0707 01:10:41.507666 30501 solver.cpp:244]     Train net output #0: loss = 0.5839 (* 1 = 0.5839 loss)
I0707 01:10:41.507680 30501 sgd_solver.cpp:106] Iteration 1325, lr = 0.000910901
I0707 01:15:18.060871 30501 solver.cpp:228] Iteration 1350, loss = 0.487446
I0707 01:15:18.060966 30501 solver.cpp:244]     Train net output #0: loss = 0.487446 (* 1 = 0.487446 loss)
I0707 01:15:18.060977 30501 sgd_solver.cpp:106] Iteration 1350, lr = 0.000909396
I0707 01:19:57.122512 30501 solver.cpp:228] Iteration 1375, loss = 0.49215
I0707 01:19:57.122599 30501 solver.cpp:244]     Train net output #0: loss = 0.49215 (* 1 = 0.49215 loss)
I0707 01:19:57.122612 30501 sgd_solver.cpp:106] Iteration 1375, lr = 0.000907897
I0707 01:25:52.756656 30501 solver.cpp:228] Iteration 1400, loss = 0.450678
I0707 01:25:52.756757 30501 solver.cpp:244]     Train net output #0: loss = 0.450678 (* 1 = 0.450678 loss)
I0707 01:25:52.756769 30501 sgd_solver.cpp:106] Iteration 1400, lr = 0.000906403
I0707 01:33:09.250155 30501 solver.cpp:228] Iteration 1425, loss = 0.463564
I0707 01:33:09.250238 30501 solver.cpp:244]     Train net output #0: loss = 0.463564 (* 1 = 0.463564 loss)
I0707 01:33:09.250252 30501 sgd_solver.cpp:106] Iteration 1425, lr = 0.000904915
I0707 01:39:23.356147 30501 solver.cpp:228] Iteration 1450, loss = 0.502735
I0707 01:39:23.356240 30501 solver.cpp:244]     Train net output #0: loss = 0.502735 (* 1 = 0.502735 loss)
I0707 01:39:23.356252 30501 sgd_solver.cpp:106] Iteration 1450, lr = 0.000903433
I0707 01:45:17.981678 30501 solver.cpp:228] Iteration 1475, loss = 0.529735
I0707 01:45:17.981776 30501 solver.cpp:244]     Train net output #0: loss = 0.529735 (* 1 = 0.529735 loss)
I0707 01:45:17.981789 30501 sgd_solver.cpp:106] Iteration 1475, lr = 0.000901956
I0707 01:51:40.865263 30501 solver.cpp:337] Iteration 1500, Testing net (#0)
I0707 01:51:40.865353 30501 net.cpp:684] Ignoring source layer training_cells
I0707 01:51:40.865363 30501 net.cpp:684] Ignoring source layer drop1
I0707 02:09:03.309471 30501 solver.cpp:404]     Test net output #0: accuracy = 0.824717
I0707 02:09:03.309556 30501 solver.cpp:404]     Test net output #1: loss = 0.458491 (* 1 = 0.458491 loss)
I0707 02:09:48.999956 30501 solver.cpp:228] Iteration 1500, loss = 0.492888
I0707 02:09:49.000048 30501 solver.cpp:244]     Train net output #0: loss = 0.492888 (* 1 = 0.492888 loss)
I0707 02:09:49.000062 30501 sgd_solver.cpp:106] Iteration 1500, lr = 0.000900485
I0707 02:17:13.396308 30501 solver.cpp:228] Iteration 1525, loss = 0.489263
I0707 02:17:13.396392 30501 solver.cpp:244]     Train net output #0: loss = 0.489263 (* 1 = 0.489263 loss)
I0707 02:17:13.396404 30501 sgd_solver.cpp:106] Iteration 1525, lr = 0.00089902
I0707 02:23:22.146168 30501 solver.cpp:228] Iteration 1550, loss = 0.507127
I0707 02:23:22.146260 30501 solver.cpp:244]     Train net output #0: loss = 0.507127 (* 1 = 0.507127 loss)
I0707 02:23:22.146272 30501 sgd_solver.cpp:106] Iteration 1550, lr = 0.00089756
I0707 02:29:49.097681 30501 solver.cpp:228] Iteration 1575, loss = 0.451752
I0707 02:29:49.097774 30501 solver.cpp:244]     Train net output #0: loss = 0.451752 (* 1 = 0.451752 loss)
I0707 02:29:49.097789 30501 sgd_solver.cpp:106] Iteration 1575, lr = 0.000896106
I0707 02:38:44.285747 30501 solver.cpp:228] Iteration 1600, loss = 0.490886
I0707 02:38:44.285840 30501 solver.cpp:244]     Train net output #0: loss = 0.490886 (* 1 = 0.490886 loss)
I0707 02:38:44.285853 30501 sgd_solver.cpp:106] Iteration 1600, lr = 0.000894657
I0707 02:44:41.387311 30501 solver.cpp:228] Iteration 1625, loss = 0.478632
I0707 02:44:41.387398 30501 solver.cpp:244]     Train net output #0: loss = 0.478632 (* 1 = 0.478632 loss)
I0707 02:44:41.387411 30501 sgd_solver.cpp:106] Iteration 1625, lr = 0.000893214
I0707 02:49:45.133249 30501 solver.cpp:228] Iteration 1650, loss = 0.450964
I0707 02:49:45.133343 30501 solver.cpp:244]     Train net output #0: loss = 0.450964 (* 1 = 0.450964 loss)
I0707 02:49:45.133355 30501 sgd_solver.cpp:106] Iteration 1650, lr = 0.000891776
I0707 02:55:03.792363 30501 solver.cpp:228] Iteration 1675, loss = 0.455378
I0707 02:55:03.792454 30501 solver.cpp:244]     Train net output #0: loss = 0.455378 (* 1 = 0.455378 loss)
I0707 02:55:03.792466 30501 sgd_solver.cpp:106] Iteration 1675, lr = 0.000890343
I0707 03:03:33.093888 30501 solver.cpp:228] Iteration 1700, loss = 0.561712
I0707 03:03:33.093979 30501 solver.cpp:244]     Train net output #0: loss = 0.561712 (* 1 = 0.561712 loss)
I0707 03:03:33.093992 30501 sgd_solver.cpp:106] Iteration 1700, lr = 0.000888916
I0707 03:10:58.530084 30501 solver.cpp:228] Iteration 1725, loss = 0.446779
I0707 03:10:58.530169 30501 solver.cpp:244]     Train net output #0: loss = 0.446779 (* 1 = 0.446779 loss)
I0707 03:10:58.530181 30501 sgd_solver.cpp:106] Iteration 1725, lr = 0.000887494
I0707 03:18:02.770279 30501 solver.cpp:337] Iteration 1750, Testing net (#0)
I0707 03:18:02.770368 30501 net.cpp:684] Ignoring source layer training_cells
I0707 03:18:02.770377 30501 net.cpp:684] Ignoring source layer drop1
I0707 03:38:51.225082 30501 solver.cpp:404]     Test net output #0: accuracy = 0.837627
I0707 03:38:51.225183 30501 solver.cpp:404]     Test net output #1: loss = 0.436813 (* 1 = 0.436813 loss)
I0707 03:39:38.906535 30501 solver.cpp:228] Iteration 1750, loss = 0.520202
I0707 03:39:38.906627 30501 solver.cpp:244]     Train net output #0: loss = 0.520202 (* 1 = 0.520202 loss)
I0707 03:39:38.906639 30501 sgd_solver.cpp:106] Iteration 1750, lr = 0.000886077
I0707 03:56:44.624212 30501 solver.cpp:228] Iteration 1775, loss = 0.430666
I0707 03:56:44.624291 30501 solver.cpp:244]     Train net output #0: loss = 0.430666 (* 1 = 0.430666 loss)
I0707 03:56:44.624308 30501 sgd_solver.cpp:106] Iteration 1775, lr = 0.000884666
I0707 04:02:12.293611 30501 solver.cpp:228] Iteration 1800, loss = 0.442659
I0707 04:02:12.293702 30501 solver.cpp:244]     Train net output #0: loss = 0.442659 (* 1 = 0.442659 loss)
I0707 04:02:12.293715 30501 sgd_solver.cpp:106] Iteration 1800, lr = 0.00088326
I0707 04:07:38.872375 30501 solver.cpp:228] Iteration 1825, loss = 0.463688
I0707 04:07:38.872480 30501 solver.cpp:244]     Train net output #0: loss = 0.463688 (* 1 = 0.463688 loss)
I0707 04:07:38.872503 30501 sgd_solver.cpp:106] Iteration 1825, lr = 0.000881859
I0707 04:12:13.409154 30501 solver.cpp:228] Iteration 1850, loss = 0.474387
I0707 04:12:13.409237 30501 solver.cpp:244]     Train net output #0: loss = 0.474387 (* 1 = 0.474387 loss)
I0707 04:12:13.409250 30501 sgd_solver.cpp:106] Iteration 1850, lr = 0.000880463
I0707 04:16:47.164358 30501 solver.cpp:228] Iteration 1875, loss = 0.468765
I0707 04:16:47.164443 30501 solver.cpp:244]     Train net output #0: loss = 0.468765 (* 1 = 0.468765 loss)
I0707 04:16:47.164454 30501 sgd_solver.cpp:106] Iteration 1875, lr = 0.000879073
I0707 04:22:30.864403 30501 solver.cpp:228] Iteration 1900, loss = 0.487321
I0707 04:22:30.864516 30501 solver.cpp:244]     Train net output #0: loss = 0.487321 (* 1 = 0.487321 loss)
I0707 04:22:30.864529 30501 sgd_solver.cpp:106] Iteration 1900, lr = 0.000877687
I0707 04:29:05.485553 30501 solver.cpp:228] Iteration 1925, loss = 0.477168
I0707 04:29:05.485647 30501 solver.cpp:244]     Train net output #0: loss = 0.477168 (* 1 = 0.477168 loss)
I0707 04:29:05.485661 30501 sgd_solver.cpp:106] Iteration 1925, lr = 0.000876307
I0707 04:36:34.611935 30501 solver.cpp:228] Iteration 1950, loss = 0.439093
I0707 04:36:34.612025 30501 solver.cpp:244]     Train net output #0: loss = 0.439093 (* 1 = 0.439093 loss)
I0707 04:36:34.612041 30501 sgd_solver.cpp:106] Iteration 1950, lr = 0.000874932
I0707 04:45:02.774710 30501 solver.cpp:228] Iteration 1975, loss = 0.44482
I0707 04:45:02.774796 30501 solver.cpp:244]     Train net output #0: loss = 0.44482 (* 1 = 0.44482 loss)
I0707 04:45:02.774808 30501 sgd_solver.cpp:106] Iteration 1975, lr = 0.000873561
I0707 04:53:46.171973 30501 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_2000.caffemodel
I0707 04:53:47.221456 30501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_2000.solverstate
I0707 04:53:47.221853 30501 solver.cpp:337] Iteration 2000, Testing net (#0)
I0707 04:53:47.221880 30501 net.cpp:684] Ignoring source layer training_cells
I0707 04:53:47.221890 30501 net.cpp:684] Ignoring source layer drop1
I0707 05:21:51.484197 30501 solver.cpp:404]     Test net output #0: accuracy = 0.840732
I0707 05:21:51.484290 30501 solver.cpp:404]     Test net output #1: loss = 0.427699 (* 1 = 0.427699 loss)
I0707 05:22:08.792879 30501 solver.cpp:228] Iteration 2000, loss = 0.439401
I0707 05:22:08.792932 30501 solver.cpp:244]     Train net output #0: loss = 0.439401 (* 1 = 0.439401 loss)
I0707 05:22:08.792943 30501 sgd_solver.cpp:106] Iteration 2000, lr = 0.000872196
I0707 05:38:21.606003 30501 solver.cpp:228] Iteration 2025, loss = 0.435102
I0707 05:38:21.606111 30501 solver.cpp:244]     Train net output #0: loss = 0.435102 (* 1 = 0.435102 loss)
I0707 05:38:21.606123 30501 sgd_solver.cpp:106] Iteration 2025, lr = 0.000870836
I0707 05:54:26.758321 30501 solver.cpp:228] Iteration 2050, loss = 0.391486
I0707 05:54:26.758409 30501 solver.cpp:244]     Train net output #0: loss = 0.391486 (* 1 = 0.391486 loss)
I0707 05:54:26.758420 30501 sgd_solver.cpp:106] Iteration 2050, lr = 0.00086948
I0707 06:08:58.371556 30501 solver.cpp:228] Iteration 2075, loss = 0.455137
I0707 06:08:58.371651 30501 solver.cpp:244]     Train net output #0: loss = 0.455137 (* 1 = 0.455137 loss)
I0707 06:08:58.371664 30501 sgd_solver.cpp:106] Iteration 2075, lr = 0.00086813
I0707 06:16:34.103324 30501 solver.cpp:228] Iteration 2100, loss = 0.439953
I0707 06:16:34.103437 30501 solver.cpp:244]     Train net output #0: loss = 0.439953 (* 1 = 0.439953 loss)
I0707 06:16:34.103453 30501 sgd_solver.cpp:106] Iteration 2100, lr = 0.000866784
I0707 06:25:03.025424 30501 solver.cpp:228] Iteration 2125, loss = 0.404837
I0707 06:25:03.025521 30501 solver.cpp:244]     Train net output #0: loss = 0.404837 (* 1 = 0.404837 loss)
I0707 06:25:03.025534 30501 sgd_solver.cpp:106] Iteration 2125, lr = 0.000865443
I0707 06:34:30.784576 30501 solver.cpp:228] Iteration 2150, loss = 0.434783
I0707 06:34:30.784663 30501 solver.cpp:244]     Train net output #0: loss = 0.434783 (* 1 = 0.434783 loss)
I0707 06:34:30.784675 30501 sgd_solver.cpp:106] Iteration 2150, lr = 0.000864108
I0707 06:41:59.643173 30501 solver.cpp:228] Iteration 2175, loss = 0.388106
I0707 06:41:59.643265 30501 solver.cpp:244]     Train net output #0: loss = 0.388106 (* 1 = 0.388106 loss)
I0707 06:41:59.643278 30501 sgd_solver.cpp:106] Iteration 2175, lr = 0.000862776
I0707 06:50:50.571943 30501 solver.cpp:228] Iteration 2200, loss = 0.485281
I0707 06:50:50.572029 30501 solver.cpp:244]     Train net output #0: loss = 0.485281 (* 1 = 0.485281 loss)
I0707 06:50:50.572042 30501 sgd_solver.cpp:106] Iteration 2200, lr = 0.00086145
I0707 06:58:19.209255 30501 solver.cpp:228] Iteration 2225, loss = 0.460542
I0707 06:58:19.209347 30501 solver.cpp:244]     Train net output #0: loss = 0.460542 (* 1 = 0.460542 loss)
I0707 06:58:19.209360 30501 sgd_solver.cpp:106] Iteration 2225, lr = 0.000860129
I0707 07:05:17.453461 30501 solver.cpp:337] Iteration 2250, Testing net (#0)
I0707 07:05:17.453547 30501 net.cpp:684] Ignoring source layer training_cells
I0707 07:05:17.453555 30501 net.cpp:684] Ignoring source layer drop1
I0707 07:24:56.832461 30501 solver.cpp:404]     Test net output #0: accuracy = 0.852656
I0707 07:24:56.832566 30501 solver.cpp:404]     Test net output #1: loss = 0.404403 (* 1 = 0.404403 loss)
I0707 07:25:14.568249 30501 solver.cpp:228] Iteration 2250, loss = 0.45429
I0707 07:25:14.568307 30501 solver.cpp:244]     Train net output #0: loss = 0.45429 (* 1 = 0.45429 loss)
I0707 07:25:14.568318 30501 sgd_solver.cpp:106] Iteration 2250, lr = 0.000858812
I0707 07:35:17.175892 30501 solver.cpp:228] Iteration 2275, loss = 0.475203
I0707 07:35:17.175988 30501 solver.cpp:244]     Train net output #0: loss = 0.475203 (* 1 = 0.475203 loss)
I0707 07:35:17.176000 30501 sgd_solver.cpp:106] Iteration 2275, lr = 0.0008575
I0707 07:45:06.508414 30501 solver.cpp:228] Iteration 2300, loss = 0.431633
I0707 07:45:06.508502 30501 solver.cpp:244]     Train net output #0: loss = 0.431633 (* 1 = 0.431633 loss)
I0707 07:45:06.508514 30501 sgd_solver.cpp:106] Iteration 2300, lr = 0.000856192
I0707 07:53:37.606482 30501 solver.cpp:228] Iteration 2325, loss = 0.427298
I0707 07:53:37.606570 30501 solver.cpp:244]     Train net output #0: loss = 0.427298 (* 1 = 0.427298 loss)
I0707 07:53:37.606585 30501 sgd_solver.cpp:106] Iteration 2325, lr = 0.000854889
I0707 08:00:52.021015 30501 solver.cpp:228] Iteration 2350, loss = 0.460202
I0707 08:00:52.021105 30501 solver.cpp:244]     Train net output #0: loss = 0.460202 (* 1 = 0.460202 loss)
I0707 08:00:52.021116 30501 sgd_solver.cpp:106] Iteration 2350, lr = 0.000853591
I0707 08:08:03.044356 30501 solver.cpp:228] Iteration 2375, loss = 0.423738
I0707 08:08:03.044464 30501 solver.cpp:244]     Train net output #0: loss = 0.423738 (* 1 = 0.423738 loss)
I0707 08:08:03.044477 30501 sgd_solver.cpp:106] Iteration 2375, lr = 0.000852297
I0707 08:15:14.653863 30501 solver.cpp:228] Iteration 2400, loss = 0.433676
I0707 08:15:14.653954 30501 solver.cpp:244]     Train net output #0: loss = 0.433676 (* 1 = 0.433676 loss)
I0707 08:15:14.653970 30501 sgd_solver.cpp:106] Iteration 2400, lr = 0.000851008
I0707 08:24:28.018501 30501 solver.cpp:228] Iteration 2425, loss = 0.426048
I0707 08:24:28.018592 30501 solver.cpp:244]     Train net output #0: loss = 0.426048 (* 1 = 0.426048 loss)
I0707 08:24:28.018609 30501 sgd_solver.cpp:106] Iteration 2425, lr = 0.000849724
I0707 08:32:30.347609 30501 solver.cpp:228] Iteration 2450, loss = 0.498381
I0707 08:32:30.347695 30501 solver.cpp:244]     Train net output #0: loss = 0.498381 (* 1 = 0.498381 loss)
I0707 08:32:30.347707 30501 sgd_solver.cpp:106] Iteration 2450, lr = 0.000848444
I0707 08:39:38.602208 30501 solver.cpp:228] Iteration 2475, loss = 0.446688
I0707 08:39:38.602295 30501 solver.cpp:244]     Train net output #0: loss = 0.446688 (* 1 = 0.446688 loss)
I0707 08:39:38.602308 30501 sgd_solver.cpp:106] Iteration 2475, lr = 0.000847168
I0707 08:52:36.262460 30501 solver.cpp:337] Iteration 2500, Testing net (#0)
I0707 08:52:36.262529 30501 net.cpp:684] Ignoring source layer training_cells
I0707 08:52:36.262538 30501 net.cpp:684] Ignoring source layer drop1
I0707 09:17:08.310315 30501 solver.cpp:404]     Test net output #0: accuracy = 0.856602
I0707 09:17:08.310413 30501 solver.cpp:404]     Test net output #1: loss = 0.393273 (* 1 = 0.393273 loss)
I0707 09:17:20.786021 30501 solver.cpp:228] Iteration 2500, loss = 0.375593
I0707 09:17:20.786073 30501 solver.cpp:244]     Train net output #0: loss = 0.375593 (* 1 = 0.375593 loss)
I0707 09:17:20.786084 30501 sgd_solver.cpp:106] Iteration 2500, lr = 0.000845897
I0707 09:24:13.105535 30501 solver.cpp:228] Iteration 2525, loss = 0.384957
I0707 09:24:13.105618 30501 solver.cpp:244]     Train net output #0: loss = 0.384957 (* 1 = 0.384957 loss)
I0707 09:24:13.105630 30501 sgd_solver.cpp:106] Iteration 2525, lr = 0.00084463
I0707 09:37:25.886505 30501 solver.cpp:228] Iteration 2550, loss = 0.427795
I0707 09:37:25.886616 30501 solver.cpp:244]     Train net output #0: loss = 0.427795 (* 1 = 0.427795 loss)
I0707 09:37:25.886631 30501 sgd_solver.cpp:106] Iteration 2550, lr = 0.000843368
I0707 09:37:25.906795 30501 solver.cpp:454] Snapshotting to binary proto file fish_net_memory_map_output_iter_2551.caffemodel
I0707 09:37:26.894944 30501 sgd_solver.cpp:273] Snapshotting solver state to binary proto file fish_net_memory_map_output_iter_2551.solverstate
I0707 09:37:26.895244 30501 solver.cpp:301] Optimization stopped early.
I0707 09:37:26.895258 30501 caffe.cpp:222] Optimization Done.
